{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a1d334c-4ffa-4b85-aac1-be2d193364d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain_community in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.3.4)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (2.0.36)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (3.9.5)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (0.6.7)\n",
      "Requirement already satisfied: httpx-sse<0.5.0,>=0.4.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (0.4.0)\n",
      "Requirement already satisfied: langchain<0.4.0,>=0.3.6 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (0.3.7)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.14 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (0.3.15)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.125 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (0.1.139)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (1.26.4)\n",
      "Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (2.6.1)\n",
      "Requirement already satisfied: requests<3,>=2 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (2.31.0)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain_community) (9.0.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain_community) (1.9.4)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (3.23.0)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain_community) (0.9.0)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.6->langchain_community) (0.3.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain<0.4.0,>=0.3.6->langchain_community) (2.9.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.14->langchain_community) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.14->langchain_community) (24.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langchain-core<0.4.0,>=0.3.14->langchain_community) (4.11.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (3.10.10)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain_community) (1.0.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests<3,>=2->langchain_community) (2024.2.2)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from SQLAlchemy<3,>=1.4->langchain_community) (3.1.1)\n",
      "Requirement already satisfied: anyio in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (4.3.0)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.0.5)\n",
      "Requirement already satisfied: sniffio in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (1.3.1)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.125->langchain_community) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.14->langchain_community) (2.4)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain_community) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.7.4->langchain<0.4.0,>=0.3.6->langchain_community) (2.23.4)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain_community) (1.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: azure-ai-ml in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (1.21.1)\n",
      "Requirement already satisfied: pyyaml>=5.1.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (6.0.1)\n",
      "Requirement already satisfied: msrest>=0.6.18 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (0.7.1)\n",
      "Requirement already satisfied: azure-core>=1.23.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (1.32.0)\n",
      "Requirement already satisfied: azure-mgmt-core>=1.3.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (1.5.0)\n",
      "Requirement already satisfied: marshmallow>=3.5 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (3.23.0)\n",
      "Requirement already satisfied: jsonschema>=4.0.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (4.22.0)\n",
      "Requirement already satisfied: tqdm in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (4.66.4)\n",
      "Requirement already satisfied: strictyaml in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (1.7.3)\n",
      "Requirement already satisfied: colorama in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (0.4.6)\n",
      "Requirement already satisfied: pyjwt in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (2.9.0)\n",
      "Requirement already satisfied: azure-storage-blob>=12.10.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (12.23.1)\n",
      "Requirement already satisfied: azure-storage-file-share in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (12.19.0)\n",
      "Requirement already satisfied: azure-storage-file-datalake>=12.2.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (12.17.0)\n",
      "Requirement already satisfied: pydash>=6.0.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (8.0.3)\n",
      "Requirement already satisfied: isodate in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (0.7.2)\n",
      "Requirement already satisfied: azure-common>=1.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (1.1.28)\n",
      "Requirement already satisfied: typing-extensions in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (4.11.0)\n",
      "Requirement already satisfied: opencensus-ext-azure in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (1.1.13)\n",
      "Requirement already satisfied: opencensus-ext-logging in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-ai-ml) (0.1.1)\n",
      "Requirement already satisfied: requests>=2.21.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-core>=1.23.0->azure-ai-ml) (2.31.0)\n",
      "Requirement already satisfied: six>=1.11.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-core>=1.23.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: cryptography>=2.1.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-storage-blob>=12.10.0->azure-ai-ml) (43.0.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (23.2.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.35.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from jsonschema>=4.0.0->azure-ai-ml) (0.18.1)\n",
      "Requirement already satisfied: packaging>=17.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from marshmallow>=3.5->azure-ai-ml) (24.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from msrest>=0.6.18->azure-ai-ml) (2024.2.2)\n",
      "Requirement already satisfied: requests-oauthlib>=0.5.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from msrest>=0.6.18->azure-ai-ml) (2.0.0)\n",
      "Requirement already satisfied: azure-identity<2.0.0,>=1.5.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from opencensus-ext-azure->azure-ai-ml) (1.19.0)\n",
      "Requirement already satisfied: opencensus<1.0.0,>=0.11.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from opencensus-ext-azure->azure-ai-ml) (0.11.4)\n",
      "Requirement already satisfied: psutil>=5.6.3 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from opencensus-ext-azure->azure-ai-ml) (5.9.8)\n",
      "Requirement already satisfied: python-dateutil>=2.6.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from strictyaml->azure-ai-ml) (2.9.0.post0)\n",
      "Requirement already satisfied: msal>=1.30.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (1.31.0)\n",
      "Requirement already satisfied: msal-extensions>=1.2.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (1.2.0)\n",
      "Requirement already satisfied: cffi>=1.12 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (1.16.0)\n",
      "Requirement already satisfied: opencensus-context>=0.1.3 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.1.3)\n",
      "Requirement already satisfied: google-api-core<3.0.0,>=1.0.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (2.22.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests>=2.21.0->azure-core>=1.23.0->azure-ai-ml) (2.2.1)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from requests-oauthlib>=0.5.0->msrest>=0.6.18->azure-ai-ml) (3.2.2)\n",
      "Requirement already satisfied: pycparser in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from cffi>=1.12->cryptography>=2.1.4->azure-storage-blob>=12.10.0->azure-ai-ml) (2.22)\n",
      "Requirement already satisfied: googleapis-common-protos<2.0.dev0,>=1.56.2 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (1.65.0)\n",
      "Requirement already satisfied: protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0.dev0,>=3.19.5 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (4.25.3)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (1.25.0)\n",
      "Requirement already satisfied: google-auth<3.0.dev0,>=2.14.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (2.35.0)\n",
      "Requirement already satisfied: portalocker<3,>=1.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from msal-extensions>=1.2.0->azure-identity<2.0.0,>=1.5.0->opencensus-ext-azure->azure-ai-ml) (2.10.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (4.9)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.dev0,>=2.14.1->google-api-core<3.0.0,>=1.0.0->opencensus<1.0.0,>=0.11.4->opencensus-ext-azure->azure-ai-ml) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Requirement already satisfied: json5 in /Users/venkatmalladi/opt/anaconda3/envs/llm/lib/python3.12/site-packages (0.9.25)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install langchain_community\n",
    "%pip install azure-ai-ml\n",
    "%pip install -qU langchain-openai\n",
    "%pip install json5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2f430c0b-3287-46a2-9871-50f08f536f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms.azureml_endpoint import (\n",
    "    AzureMLOnlineEndpoint,\n",
    "    AzureMLEndpointApiType,\n",
    "    DollyContentFormatter,\n",
    "    ContentFormatterBase\n",
    ")\n",
    "from typing import Dict\n",
    "import urllib.request\n",
    "import json\n",
    "import os\n",
    "import ssl\n",
    "import time\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "from langchain_community.retrievers import AzureAISearchRetriever\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.messages import HumanMessage\n",
    "from langchain_community.chat_models.azureml_endpoint import (\n",
    "    AzureMLEndpointApiType,\n",
    "    CustomOpenAIChatContentFormatter,\n",
    ")\n",
    "from langchain_community.chat_models.azureml_endpoint import AzureMLChatOnlineEndpoint\n",
    "\n",
    "import json5\n",
    "import re\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f74a3d36-8870-4cdc-a039-a31c6d8afb5d",
   "metadata": {},
   "source": [
    "#### 1st Agent: Fine-tuned LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a1b983e-9bf7-4b47-b427-918fbfe6e796",
   "metadata": {},
   "outputs": [],
   "source": [
    "def allowSelfSignedHttps(allowed):\n",
    "    # bypass the server certificate verification on client side\n",
    "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
    "        ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "allowSelfSignedHttps(True)\n",
    "\n",
    "class CustomFormatter(ContentFormatterBase):\n",
    "    content_type = \"application/json\"\n",
    "    accepts = \"application/json\"\n",
    "\n",
    "    def format_request_payload(self, prompt: str, *args, **kwargs) -> bytes:\n",
    "        model_kwargs = kwargs.get('model_kwargs', {})\n",
    "        input_str = json.dumps(\n",
    "            {\"input_data\": [{\"role\": \"user\", \"content\": prompt}], \"params\": {\"temperature\": 0.1, \"max_new_tokens\": 1000, \"do_sample\": True}}\n",
    "        )\n",
    "        return str.encode(input_str)\n",
    "\n",
    "    def format_response_payload(self, output: bytes, *args, **kwargs) -> Dict:\n",
    "        response_json =  json.loads(output)\n",
    "\n",
    "        try:\n",
    "            # Step 1: Prepare JSON string\n",
    "            response_content = response_json['result'].replace(\"'\", \"\\\"\")\n",
    "\n",
    "            # Step 2: Escape double quotes within the `content` fields\n",
    "            response_content = re.sub(r'(?<=content\": \")', lambda m: m.group(0).replace('\"', '\\\\\"'), response_content)\n",
    "            response_content = response_content.replace('Here\"s', 'Here\\'s')\n",
    "            \n",
    "            # Step 3: Split the JSON string based on \"} {\" separator to handle multiple entries\n",
    "            json_entries = response_content.split(\"} {\")\n",
    "            \n",
    "            # Wrap entries in braces to create valid JSON for each entry\n",
    "            json_entries = [\"{\" + entry + \"}\" if not entry.startswith(\"{\") else entry for entry in json_entries]\n",
    "            json_entries = [entry + \"}\" if not entry.endswith(\"}\") else entry for entry in json_entries]\n",
    "            \n",
    "            # Step 4: Parse each JSON entry and retrieve assistant content\n",
    "            assistant_content = None\n",
    "            for json_string in json_entries:\n",
    "                # Regular expression to find \"role\": \"assistant\" and capture \"content\" value\n",
    "                match = re.search(r'\"role\"\\s*:\\s*\"assistant\",\\s*\"content\"\\s*:\\s*\"(.*?)\"\\s*}', json_string, re.DOTALL)\n",
    "\n",
    "                # Check if role is \"assistant\" and retrieve content\n",
    "                if match:\n",
    "                    assistant_content = match.group(1)  # Get the content text\n",
    "        \n",
    "            return {\"text\": assistant_content}\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"JSON decode error: {e}\")\n",
    "\n",
    "        # Return a default response if there was an error\n",
    "            return {\"text\": \"No valid response\"}\n",
    "\n",
    "# Example usage of the CustomFormatter\n",
    "content_formatter = CustomFormatter()\n",
    "\n",
    "\n",
    "solver_llm = AzureMLOnlineEndpoint(\n",
    "    endpoint_url=\"\",\n",
    "    endpoint_api_type=AzureMLEndpointApiType.dedicated,\n",
    "    endpoint_api_key=\"\",\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1000, \"do_sample\": True},\n",
    "    content_formatter=content_formatter,\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da9d27f-7a2a-42e6-910b-fb18b4f6d2a9",
   "metadata": {},
   "source": [
    "#### Second Agent: RAG + Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c494d42-a270-4d81-acd6-f906d7328241",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure AI Search setup\n",
    "AZURE_AI_SEARCH_SERVICE_NAME = \"\"\n",
    "AZURE_AI_SEARCH_API_KEY = \"\"\n",
    "\n",
    "retriever = AzureAISearchRetriever(\n",
    "    service_name=AZURE_AI_SEARCH_SERVICE_NAME,\n",
    "    index_name=\"vector-rag\",\n",
    "    api_key=AZURE_AI_SEARCH_API_KEY,\n",
    "    content_key=\"chunk\",\n",
    "    top_k=1\n",
    ")\n",
    "\n",
    "# Azure OpenAI setup\n",
    "coder_llm = AzureMLChatOnlineEndpoint(\n",
    "    endpoint_url=\"\",\n",
    "    endpoint_api_type=AzureMLEndpointApiType.dedicated,\n",
    "    endpoint_api_key=\"\",\n",
    "    content_formatter=CustomOpenAIChatContentFormatter(),\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1000},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43d8ae1-3c5f-4abf-afc8-305b4ded983a",
   "metadata": {},
   "source": [
    "#### Reasoning Agent: Phi-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbfa1633-d1e8-4b5d-8360-7914750fa2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "reasoning_agent = AzureMLChatOnlineEndpoint(\n",
    "    endpoint_url=\"\",\n",
    "    endpoint_api_type=AzureMLEndpointApiType.dedicated,\n",
    "    endpoint_api_key=\"\",\n",
    "    content_formatter=CustomOpenAIChatContentFormatter(),\n",
    "    model_kwargs={\"temperature\": 0.1, \"max_new_tokens\": 1000},\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34c87926-c12f-45ab-93ae-36cbae39b63f",
   "metadata": {},
   "source": [
    "#### Muti-Agent Logic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "21c2c28d-adfa-4b6a-a4f9-eb01390b714d",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_part1 = \"Question 1: Samtools can be used to select reads above certain mapping quality. samtools view -h -b -q 30 aligned.bam -o above.mapQ30.bam But, how to select a read below certain mapping quality - all aligned reads below mapQ 30?</strong>I know it can be done using awk. But, the pipeline gets lengthy and time consuming when first need to convert bam to sam - separate header - use awk for mapQ below 30 - add header - sam file - convert to bam.Really, its taking a lots of time.\"\n",
    "query_part2 = \"Question 2: Provide your logic and reasoning.\"\n",
    "combined_query = f\"{query_part1}\\n{query_part2}\"\n",
    "# Invoke the model and print the result\n",
    "query = f\"Question: {combined_query}.\\nPlease provide a solution in raw text.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f4ba3e52-249f-4edc-96b9-808e3cc39d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_docs(docs):\n",
    "    \"\"\"Format retrieved documents into a single string.\"\"\"\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs if doc.page_content) if docs else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab4e6bb-1c8b-43d0-8024-87ed54843cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_with_retries(llm, prompt, max_retries=5, backoff_factor=2):\n",
    "    \"\"\"\n",
    "    Helper function to invoke LLM with retry logic in case of timeout or HTTP errors.\n",
    "    \"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return llm.invoke(prompt, timeout=300000)  # Attempt to invoke LLM\n",
    "        except TimeoutError:\n",
    "            # Handle timeout errors with exponential backoff\n",
    "            sleep_time = backoff_factor * (2 ** attempt)\n",
    "            time.sleep(sleep_time)\n",
    "            print(f\"Timeout occurred. Retrying after {sleep_time} seconds... Attempt {attempt + 1}/{max_retries}\")\n",
    "        except HTTPError as e:\n",
    "            if e.code == 424:  # Handle HTTP 424: Failed Dependency\n",
    "                print(f\"HTTP Error 424: Failed Dependency. Skipping after {attempt + 1}/{max_retries} attempts.\")\n",
    "                return None\n",
    "            else:\n",
    "                print(f\"HTTP Error {e.code}: {e.reason}. Retrying...\")\n",
    "                sleep_time = backoff_factor * (2 ** attempt)\n",
    "                time.sleep(sleep_time)\n",
    "    print(f\"Max retries reached for {llm}.\")\n",
    "    return None  # Return None if all retries fail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "0f9de2b8-8085-4551-a5ab-cc083a6ff8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_response(query, retries=3, backoff_factor=1):\n",
    "    for attempt in range(retries):\n",
    "\n",
    "        # Step 1: Retrieve documents\n",
    "        docs = retriever.invoke(query)\n",
    "        if not docs:\n",
    "            print(\"No documents found for the query.\")\n",
    "\n",
    "        # Step 2: Format documents\n",
    "        context = format_docs(docs)\n",
    "        if not context:\n",
    "            print(\"Formatted context is empty.\")\n",
    "\n",
    "        # Step 3: Generate prompt for coder LLM\n",
    "        prompt = f\"This is relevant code documentation: \\n{context}\\n\\nQuestion: {query}.\\nPlease respond with code.\"\n",
    "        response_coder = coder_llm.invoke(prompt)\n",
    "\n",
    "        # Process coder response\n",
    "        response_code = response_coder.content if hasattr(response_coder, 'content') else \"\"\n",
    "        if not response_code:\n",
    "            print(\"Unexpected response from coder LLM.\")\n",
    "\n",
    "        # Step 4: Generate solver response with retry on timeout\n",
    "        solver_text = invoke_with_retries(solver_llm, prompt, max_retries=6)\n",
    "        if solver_text is None:\n",
    "            print(\"Failed to get response from solver LLM after retries.\")\n",
    "            solver_text = \"\"\n",
    "\n",
    "        # Step 5: Generate reasoning prompt\n",
    "        prompt_reason = (\n",
    "            f\"You are an AI reasoning agent collaborating with other specialized assistants. \"\n",
    "            f\"There are two previous responses you should review: \"\n",
    "            f\"A solving agent specializing in bioinformatics tools has provided the following response: {solver_text}. \"\n",
    "            f\"A coder agent specializing in genomics analysis workflows written in Nextflow has provided the following response: {response_code}. \"\n",
    "            f\"Your task is to answer the user query: {query}. \"\n",
    "            f\"After you generate your response, print the quality rating of your output as Quality Rating: on a new line. using the scale 1-10 using format x/10.\"\n",
    "        )\n",
    "\n",
    "        combined_reasoning = reasoning_agent.invoke(prompt_reason)\n",
    "\n",
    "        # Step 6: Check reasoning response\n",
    "        if hasattr(combined_reasoning, 'content'):\n",
    "            final_response = combined_reasoning.content\n",
    "\n",
    "            match = re.search(r'Quality Rating: (\\d+)/10', final_response)\n",
    "\n",
    "            if match:\n",
    "                quality_rating = int(match.group(1))\n",
    "                if quality_rating > 7:\n",
    "                    print(\"Achieved satisfactory rating.\")\n",
    "                    return final_response  # Return response if rating is above 7\n",
    "                else:\n",
    "                    print(\"Rating not satisfactory, continuing to next attempt.\")\n",
    "            else:\n",
    "                print(\"Quality Rating not found in the response.\")\n",
    "        else:\n",
    "            print(\"Reasoning agent did not return valid content.\")\n",
    "\n",
    "        # Backoff before the next attempt\n",
    "        sleep_time = backoff_factor * (2 ** attempt)  # Exponential backoff\n",
    "        print(f\"Sleeping for {sleep_time} seconds before next attempt.\")\n",
    "        time.sleep(sleep_time)\n",
    "\n",
    "    print(\"Max attempts reached. Returning last response.\")\n",
    "    return final_response if 'final_response' in locals() else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "2b0c72a6-004f-4886-a32f-b3d4767af04b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Student ask: 'How would I provide quality metric on fastq files?' Provide the steps you would advise the student to take.\n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Achieved satisfactory rating.\n",
      "Question: Student ask: 'What code or workflow do I need to write to provide quality metric on fastq files?' Provide the steps you would advise the student to take.\n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Trying Solver again\n",
      "Trying Solver again\n",
      "Trying Solver again\n",
      "Max retries reached for \u001b[1mAzureMLOnlineEndpoint\u001b[0m\n",
      "Params: {'deployment_name': '', 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100, 'do_sample': True}}.\n",
      "Failed to get response from solver LLM after retries.\n",
      "Achieved satisfactory rating.\n",
      "Question: Student ask: 'How do I align RNA-seq data against human reference genome?' Provide the steps you would advise the student to take.\n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Achieved satisfactory rating.\n",
      "Question: Student ask: 'What code or workflow do I need to write to align RNA-seq data against human reference genome?' Provide the steps you would advise the student to take.\n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Achieved satisfactory rating.\n",
      "Question: Student ask: 'How can I assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?' Provide the steps you would advise the student to take.\n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Trying Solver again\n",
      "Trying Solver again\n",
      "Trying Solver again\n",
      "Max retries reached for \u001b[1mAzureMLOnlineEndpoint\u001b[0m\n",
      "Params: {'deployment_name': '', 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100, 'do_sample': True}}.\n",
      "Failed to get response from solver LLM after retries.\n",
      "Achieved satisfactory rating.\n",
      "Question: Student ask: 'What code or workflow do I need to write to assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?' Provide the steps you would advise the student to take.\n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Trying Solver again\n",
      "Trying Solver again\n",
      "Trying Solver again\n",
      "Max retries reached for \u001b[1mAzureMLOnlineEndpoint\u001b[0m\n",
      "Params: {'deployment_name': '', 'model_kwargs': {'temperature': 0.1, 'max_new_tokens': 100, 'do_sample': True}}.\n",
      "Failed to get response from solver LLM after retries.\n",
      "Achieved satisfactory rating.\n"
     ]
    }
   ],
   "source": [
    "query_part1= [\"Student ask: 'How would I provide quality metric on fastq files?' Provide the steps you would advise the student to take.\",\n",
    "              \"Student ask: 'What code or workflow do I need to write to provide quality metric on fastq files?' Provide the steps you would advise the student to take.\",\n",
    "              \"Student ask: 'How do I align RNA-seq data against human reference genome?' Provide the steps you would advise the student to take.\",\n",
    "              \"Student ask: 'What code or workflow do I need to write to align RNA-seq data against human reference genome?' Provide the steps you would advise the student to take.\",\n",
    "              \"Student ask: 'How can I assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?' Provide the steps you would advise the student to take.\",\n",
    "              \"Student ask: 'What code or workflow do I need to write to assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?' Provide the steps you would advise the student to take.\"\n",
    "              ]\n",
    "\n",
    "query_part2= \"Please explain your logic and reasoning behind your answer.\"\n",
    "query_part3 = \"What additional information do you need to answer the question?\"\n",
    "\n",
    "results = {\"easy_q\" : \"\", \"easy_code\" : \"\", \"med_q\": \"\" , \"med_code\": \"\", \"hard_q\": \"\", \"hard_code\": \"\"}\n",
    "\n",
    "for idx, question in enumerate (query_part1):\n",
    "\n",
    "  combined_query = f\"{question}\\n{query_part2}\\n{query_part3}\"\n",
    "  query = f\"Question: {combined_query}\\nPlease provide a solution in raw text.\"\n",
    "  print (query)\n",
    "  final_response = generate_final_response(query)\n",
    "\n",
    "  if idx == 0:\n",
    "    results[\"easy_q\"] = final_response\n",
    "\n",
    "  elif idx == 1:\n",
    "    results[\"easy_code\"] = final_response\n",
    "\n",
    "  elif idx == 2:\n",
    "    results[\"med_q\"] = final_response\n",
    "\n",
    "  elif idx == 3:\n",
    "    results[\"med_code\"] = final_response\n",
    "\n",
    "  elif idx == 4:\n",
    "    results[\"hard_q\"] = final_response\n",
    "\n",
    "  elif idx == 5:\n",
    "    results[\"hard_code\"] = final_response\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "8fbea42e-afdd-442f-8c06-0826f9916363",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Student ask: 'What code or workflow do I need to write to assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?' \n",
      "Please explain your logic and reasoning behind your answer.\n",
      "What additional information do you need to answer the question?\n",
      "Please provide a solution in raw text.\n",
      "Achieved satisfactory rating.\n",
      "Response:\n",
      "\n",
      "To assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus, you can follow the workflow provided by the coder agent specializing in genomics analysis workflows written in Nextflow. This workflow uses tools like BWA, SAMtools, GATK, and custom scripts.\n",
      "\n",
      "Here's an example of a workflow using command-line tools:\n",
      "\n",
      "1. Quality control and trimming:\n",
      "   - Use tools like FastQC and Trimmomatic to assess the quality of raw sequencing reads and trim low-quality bases and adapters.\n",
      "\n",
      "2. Genome assembly:\n",
      "   - Use a de novo assembler like SPAdes or Velvet to assemble the trimmed reads into contigs.\n",
      "   - Use tools like MEGAHIT or SPAdes-legacy for long-read assemblies.\n",
      "\n",
      "3. Genome annotation:\n",
      "   - Use Prokka or RAST to annotate the assembled contigs with gene predictions and functional information.\n",
      "\n",
      "4. Variant calling:\n",
      "   - Align the cleaned reads to the reference genome using BWA or minimap2.\n",
      "   - Use SAMtools and GATK to perform variant calling and filter the results based on quality scores and allele frequency.\n",
      "\n",
      "5. Variant annotation and characterization:\n",
      "   - Use tools like SnpEff or VEP to annotate the identified variants with functional information and predict their effects on protein sequences.\n",
      "   - Use tools like SnpEff or SIFT to predict the potential impact of the variants on protein function.\n",
      "   - Use tools like Variant Effect Predictor (VEP) or SnpEff to annotate the identified variants with functional information and predict their effects on protein sequences.\n",
      "   - Use tools like SnpEff or SIFT to predict the potential impact of the variants on protein function.\n",
      "\n",
      "6. Phylogenetic analysis:\n",
      "   - Use tools like MEGA or PhyML to construct phylogenetic trees based on the identified variants and compare them with known SARS-CoV-2 sequences.\n",
      "\n",
      "7. Visualization:\n",
      "   - Use tools like IGV or UCSC Genome Browser to visualize the assembled genome, annotated variants, and phylogenetic trees.\n",
      "\n",
      "Additional information needed to answer the question:\n",
      "- Raw sequencing data (e.g., FASTQ files)\n",
      "- Reference genome sequence of SARS-CoV-2\n",
      "- Software and tool versions\n",
      "- Computational resources (e.g., CPU, memory, disk space)\n",
      "- Bioinformatics skills and experience\n",
      "\n",
      "Quality Rating: 9/10.\n",
      "\n",
      "The provided workflow is comprehensive and covers all the necessary steps for assembling, annotating, and analyzing SARS-CoV-2 genomes from sequencing data. It also includes variant calling, annotation, characterization, phylogenetic analysis, and visualization. The workflow is well-structured and uses widely-used tools, making it suitable for researchers with bioinformatics skills and experience. However, the workflow could be improved by providing more details on the specific parameters and options for each tool, as well as troubleshooting tips for common issues that may arise during the analysis.\n"
     ]
    }
   ],
   "source": [
    "query_part1= [\n",
    "              \"Student ask: 'What code or workflow do I need to write to assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus?' \"\n",
    "              ]\n",
    "\n",
    "query_part2= \"Please explain your logic and reasoning behind your answer.\"\n",
    "query_part3 = \"What additional information do you need to answer the question?\"\n",
    "\n",
    "\n",
    "\n",
    "for idx, question in enumerate (query_part1):\n",
    "\n",
    "  combined_query = f\"{question}\\n{query_part2}\\n{query_part3}\"\n",
    "  query = f\"Question: {combined_query}\\nPlease provide a solution in raw text.\"\n",
    "  print (query)\n",
    "  final_response = generate_final_response(query)\n",
    "  print(final_response)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "3a443868-bf58-46e3-9839-b3b4ab08a0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "easy_q: Quality Rating: 9/10\n",
      "\n",
      "To provide quality metrics on fastq files, I would advise the student to follow these steps:\n",
      "\n",
      "1. Choose a quality metric: The student should decide on the quality metric they want to use, such as Phred score, Q-score, or GC content. The Phred score is a popular choice as it represents the probability of an incorrect base call on a logarithmic scale.\n",
      "\n",
      "2. Install necessary software: The student should install a tool that can calculate the chosen quality metric for fastq files. FastQC is a widely used quality control tool for high-throughput sequence data.\n",
      "\n",
      "3. Run FastQC on the fastq files: The student should use the FastQC command-line tool to generate a quality report for their fastq files. The command would look like this:\n",
      "\n",
      "```bash\n",
      "fastqc *.fastq.gz\n",
      "```\n",
      "\n",
      "This command will generate a report in HTML format, which the student can open in a web browser to view the quality metrics.\n",
      "\n",
      "4. Analyze the FastQC report: The FastQC report will contain several quality metrics, including the mean quality score, the percentage of reads with a quality score below a certain threshold, and the distribution of quality scores across the reads. The student should use these metrics to assess the quality of their fastq files.\n",
      "\n",
      "5. Visualize the quality metrics: The student can create visualizations of the quality metrics using tools like ggplot2 in R or matplotlib in Python. This will help them better understand the quality of their fastq files and identify any potential issues.\n",
      "\n",
      "Additional information needed:\n",
      "\n",
      "- The specific quality metric the student wants to use (e.g., Phred score, Q-score, or GC content).\n",
      "- The format of the fastq files (e.g., gzipped or uncompressed).\n",
      "- The software and programming language the student is comfortable using to analyze the quality metrics.\n",
      "\n",
      "By following these steps, the student will be able to provide quality metrics on their fastq files and identify any potential issues that may affect their downstream analysis.\n",
      "easy_code: To provide quality metrics on fastq files, you can follow these steps:\n",
      "\n",
      "1. Install necessary software and libraries:\n",
      "   - Install FastQC (https://www.bioinformatics.babraham.ac.uk/projects/fastqc/)\n",
      "   - Install MultiQC (https://github.com/ewenzz/multiqc)\n",
      "   - Install Flexbar (https://github.com/Flexbar/flexbar)\n",
      "\n",
      "2. Prepare your fastq files:\n",
      "   - Organize your fastq files in a directory structure that makes it easy to navigate and access the files.\n",
      "\n",
      "3. Run FastQC on your fastq files:\n",
      "   - Open a terminal or command prompt.\n",
      "   - Navigate to the directory containing your fastq files.\n",
      "   - Run the following command to generate a FastQC report for each fastq file:\n",
      "     ```\n",
      "     fastqc *.fastq.gz\n",
      "     ```\n",
      "   - Wait for the report to be generated. FastQC will create a report file for each fastq file with the extension `.html`.\n",
      "\n",
      "4. Generate MultiQC report:\n",
      "   - Run the following command to generate a MultiQC report:\n",
      "     ```\n",
      "     multiqc -p /path/to/fastq_files\n",
      "     ```\n",
      "   - Wait for the report to be generated. MultiQC will create a report file with the extension `.html`.\n",
      "\n",
      "5. Visualize the data:\n",
      "   - Run the following command to visualize the quality metrics using Flexbar:\n",
      "     ```\n",
      "     flexbar -i multiqc_report.html\n",
      "     ```\n",
      "\n",
      "6. Interpret the results:\n",
      "   - Analyze the quality metrics and visualizations generated by FastQC and MultiQC. Look for patterns and trends in the data that may indicate problems with the sequencing process or data quality.\n",
      "\n",
      "7. Take corrective action:\n",
      "   - Based on your analysis of the quality metrics, take corrective action to address any issues with your fastq files. This may involve re-sequencing the samples, trimming low-quality reads, or adjusting the sequencing parameters.\n",
      "\n",
      "Additional information needed:\n",
      "- The specific quality metrics you want to generate (e.g., base quality, sequence quality, adapter content, etc.)\n",
      "- The software and libraries you have access to (e.g., FastQC, MultiQC, Flexbar, etc.)\n",
      "- The sequencing platform and chemistry used (e.g., Illumina, PacBio, etc.)\n",
      "- The sample preparation and sequencing protocols used\n",
      "- The desired output format (e.g., HTML, PDF, etc.)\n",
      "\n",
      "Quality Rating: 8/10\n",
      "\n",
      "The provided workflow covers the essential steps for generating quality metrics on fastq files. However, the quality rating is not perfect (9/10) because it does not include steps for re-sequencing, trimming, or adjusting sequencing parameters, which are crucial for addressing issues identified during the quality assessment.\n",
      "med_q: Quality Rating: 9/10\n",
      "\n",
      "To align RNA-seq data against a human reference genome, you can follow these steps:\n",
      "\n",
      "1. Obtain the RNA-seq data: Ensure you have the raw sequencing reads in FASTQ format.\n",
      "\n",
      "2. Choose a reference genome: Select a high-quality human reference genome, such as GRCh38 or hg38.\n",
      "\n",
      "3. Perform quality control on the raw sequencing reads using tools like FastQC.\n",
      "\n",
      "4. Trim and filter reads: Use tools like Trimmomatic or Cutadapt to remove adapter sequences and low-quality bases from the reads.\n",
      "\n",
      "5. Align reads to the reference genome: Use a read aligner like STAR, HISAT2, or TopHat2 to map the reads to the reference genome.\n",
      "\n",
      "6. Convert SAM/BAM to BED: Convert the alignment file (SAM/BAM) to a BED file using tools like SAMtools or bedtools.\n",
      "\n",
      "7. Quantify gene expression: Use tools like featureCounts or HTSeq to quantify gene expression levels from the BED file.\n",
      "\n",
      "8. Normalize and analyze data: Normalize the gene expression data using methods like TPM or FPKM, and perform downstream analyses such as differential expression analysis using tools like DESeq2 or edgeR.\n",
      "\n",
      "Additional information needed to answer the question:\n",
      "\n",
      "- The specific RNA-seq data format (FASTQ, BAM, etc.)\n",
      "- The sequencing platform used (Illumina, PacBio, etc.)\n",
      "- The desired level of analysis (e.g., gene expression, splice junctions, etc.)\n",
      "- The specific software and tools available in the analysis environment\n",
      "\n",
      "The logic behind this answer is to provide a step-by-step guide for aligning RNA-seq data against a human reference genome, starting from obtaining the raw sequencing reads to performing downstream analyses. The steps are organized in a logical order, ensuring that each step builds upon the previous one. The additional information needed to answer the question is essential for tailoring the analysis to the specific data and tools available.\n",
      "med_code: Quality Rating: 9/10\n",
      "\n",
      "To align RNA-seq data against the human reference genome, you can follow these steps using the popular bioinformatics tool `STAR` (Spliced Transcripts Alignment to a Reference Genome):\n",
      "\n",
      "1. Install the necessary software:\n",
      "   - Install `STAR` from the official website (https://github.com/alexdobin/STAR/releases) or using a package manager like `conda` or `mamba`.\n",
      "   - Install other required tools like `samtools`, `htslib`, and `R`.\n",
      "\n",
      "2. Prepare your RNA-seq data:\n",
      "   - Quality control and trimming: Use tools like `FastQC` and `Trimmomatic` to assess and trim the raw sequencing reads.\n",
      "   - Sort and index the reads: Use `STAR` to sort and index the trimmed reads.\n",
      "\n",
      "3. Align the reads to the human reference genome:\n",
      "   - Download the human reference genome (GRCh38) from the UCSC Genome Browser (ftp://hgdownload.cse.ucsc.edu/goldenPath/hg38/bigZips/hg38.fa.gz).\n",
      "   - Run `STAR` with the following command:\n",
      "     ```\n",
      "     STAR --runMode alignReads --genomeDir /path/to/STAR/index/ --readFilesIn /path/to/trimmed_reads.fastq --readFilesCommand zcat --outFileNamePrefix /path/to/output/ --outSAMtype BAM SortedByCoordinate\n",
      "     ```\n",
      "   - This command will generate a sorted BAM file containing the aligned reads.\n",
      "\n",
      "4. Post-processing:\n",
      "   - Convert the BAM file to a sorted, indexed BAM file using `samtools`:\n",
      "     ```\n",
      "     samtools sort /path/to/output/Aligned.out.bam -o /path/to/output/Aligned.sortedByCoord.out.bam\n",
      "     samtools index /path/to/output/Aligned.sortedByCoord.out.bam\n",
      "     ```\n",
      "   - Quantify gene expression using `featureCounts` or `HTSeq-count`:\n",
      "     ```\n",
      "     featureCounts -a /path/to/gencode.v37.annotation.gtf -o /path/to/output/gene_counts.txt /path/to/output/Aligned.sortedByCoord.out.bam\n",
      "     ```\n",
      "   - Normalize the gene expression data using tools like `DESeq2` or `edgeR`.\n",
      "\n",
      "5. Visualize the results:\n",
      "   - Generate a heatmap of gene expression using tools like `ggplot2` in R or `matplotlib` in Python.\n",
      "   - Perform differential gene expression analysis using tools like `DESeq2` or `edgeR`.\n",
      "\n",
      "Additional information needed to answer the question:\n",
      "- The location of the RNA-seq data files (fastq format).\n",
      "- The path to the STAR index directory.\n",
      "- The path to the human reference genome (GRCh38).\n",
      "- The path to the gene annotation file (e.g., gencode.v37.annotation.gtf).\n",
      "- The desired output directory for the aligned reads and other files.\n",
      "\n",
      "By following these steps and using the appropriate software, you can align RNA-seq data against the human reference genome and perform downstream analyses.\n",
      "hard_q: Quality Rating: 9/10\n",
      "\n",
      "To assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus, you can follow these steps:\n",
      "\n",
      "1. Obtain sequencing data: Obtain raw sequencing data from a reliable source, such as the GISAID database or other public repositories.\n",
      "\n",
      "2. Quality control: Perform quality control on the raw sequencing data using tools like FastQC and Trimmomatic to remove low-quality reads and adapter sequences.\n",
      "\n",
      "3. Read assembly: Assemble the high-quality reads using a de novo assembler like SPAdes or Velvet. This will generate contigs and scaffolds representing the SARS-CoV-2 genome.\n",
      "\n",
      "4. Reference-based assembly: Alternatively, you can use a reference-based assembler like SPAdes or MEGAHIT to align the reads to a reference genome and generate a consensus sequence.\n",
      "\n",
      "5. Annotation: Annotate the assembled genome using tools like Prokka or RAST to identify genes, protein-coding regions, and other functional elements.\n",
      "\n",
      "6. Variant calling: Identify variants in the SARS-CoV-2 genome using tools like FreeBayes, GATK, or SURVIVOR. This will generate a VCF file containing the identified variants.\n",
      "\n",
      "7. Variant analysis: Analyze the identified variants to characterize different variants of the virus. This can be done using tools like SnpEff, Variant Effect Predictor (VEP), or ANNOVAR to predict the functional effects of the variants.\n",
      "\n",
      "8. Phylogenetic analysis: Construct a phylogenetic tree to visualize the relationships between different SARS-CoV-2 variants. This can be done using tools like MEGA, RAxML, or FastTree.\n",
      "\n",
      "9. Report generation: Generate a report summarizing the findings, including the identified variants, their functional effects, and the phylogenetic relationships between different variants.\n",
      "\n",
      "Additional information needed:\n",
      "\n",
      "- The sequencing data format (e.g., FASTQ, BAM)\n",
      "- The reference genome used for assembly and annotation (e.g., NC_045512.2)\n",
      "- The desired output formats (e.g., HTML, PDF, or Jupyter notebook)\n",
      "- The computational resources available (e.g., CPU cores, memory, and disk space)\n",
      "- The programming languages and software packages installed on the system (e.g., Python, R, and bioinformatics tools)\n",
      "\n",
      "To implement this workflow, you can use a combination of command-line tools and programming languages like Python or R. Here's an example workflow using Python and the above tools:\n",
      "\n",
      "```python\n",
      "# Import required libraries\n",
      "import subprocess\n",
      "import os\n",
      "\n",
      "# Set working directory\n",
      "os.chdir(\"/path/to/working/directory\")\n",
      "\n",
      "# Quality control\n",
      "subprocess.run([\"fastqc\", \"input_data.fastq.gz\"])\n",
      "subprocess.run([\"trimmomatic\", \"PE\", \"-phred33\", \"input_data.fastq.gz\", \"output_data_1.fastq.gz\", \"output_data_2.fastq.gz\", \"ILLUMINACLIP:TruSeq3-PE.fa:2:30:10\", \"LEADING:3\", \"TRAILING:3\", \"SLIDINGWINDOW:4:15\", \"MINLEN:36\"])\n",
      "\n",
      "# Read assembly\n",
      "subprocess.run([\"spades.py\", \"-1\", \"output_data_1.fastq.gz\", \"-2\", \"output_data_2.fastq.gz\", \"-o\", \"assembly\"])\n",
      "\n",
      "# Annotation\n",
      "subprocess.run([\"prokka\", \"assembly/scaffolds.fasta\", \"-p\", \"genetic\", \"-m\", \"10\", \"-s\", \"100\", \"-v\", \"0\"])\n",
      "\n",
      "# Variant calling\n",
      "subprocess.run([\"freebayes\", \"-f\", \"reference_genome.fasta\", \"assembly/scaffolds.fasta\", \"-v\", \"100\", \"-o\", \"variants\"])\n",
      "\n",
      "# Variant analysis\n",
      "subprocess.run([\"snpEff\", \"variants/variants.vcf\", \"-g\n",
      "hard_code: Quality Rating: 9/10\n",
      "\n",
      "To assemble, annotate, and analyze SARS-CoV-2 genomes from sequencing data to identify and characterize different variants of the virus, the student should follow these steps:\n",
      "\n",
      "1. Pre-processing:\n",
      "   - Quality control and trimming of raw sequencing reads using tools like FastQC, Trimmomatic, or Cutadapt.\n",
      "   - Filtering low-quality reads and removing adapters.\n",
      "\n",
      "2. Assembly:\n",
      "   - Use a de novo assembly tool like SPAdes, MEGAHIT, or Velvet to assemble the cleaned reads into contigs.\n",
      "   - Assemble the contigs into scaffolds using tools like SSPACE or PERL.\n",
      "\n",
      "3. Annotation:\n",
      "   - Use a genome annotation tool like Prokka, RAST, or MAKER to annotate the assembled genome with gene predictions, functional annotations, and metabolic pathways.\n",
      "   - Use a virus-specific annotation tool like VirSorter or VirFinder to identify viral sequences and predict their functions.\n",
      "\n",
      "4. Variant calling:\n",
      "   - Use a variant calling tool like FreeBayes, GATK, or SURVIVOR to identify single nucleotide polymorphisms (SNPs) and insertions/deletions (indels) in the SARS-CoV-2 genome.\n",
      "   - Filter the variants based on quality scores, read depth, and other criteria.\n",
      "\n",
      "5. Variant analysis:\n",
      "   - Use a variant analysis tool like SnpEff, VEP, or SnpSift to predict the effects of the identified variants on protein function and pathogenicity.\n",
      "   - Use a variant visualization tool like IGV, Integrative Genomics Viewer, or Tableau to visualize the variants and their genomic context.\n",
      "\n",
      "6. Phylogenetic analysis:\n",
      "   - Use a phylogenetic analysis tool like MEGA, RAxML, or BEAST to construct a phylogenetic tree of the identified variants and compare them to reference sequences.\n",
      "   - Use a variant clustering tool like Roary or SURPI to group the variants into lineages and track their evolution over time.\n",
      "\n",
      "7. Reporting:\n",
      "   - Use a reporting tool like Jupyter Notebook, R Markdown, or Quarto to create a report summarizing the results of the analysis and highlighting the key findings.\n",
      "\n",
      "Additional information needed to answer the question:\n",
      "- The sequencing data format (e.g., FASTQ, BAM, CRAM)\n",
      "- The sequencing platform (e.g., Illumina, Nanopore)\n",
      "- The reference genome sequence (e.g., NC_045512.2, GISAID accession number)\n",
      "- The desired output format (e.g., HTML, PDF, CSV)\n",
      "- The computational resources available (e.g., CPU cores, RAM, disk space)\n",
      "- The software and tools to be used (e.g., Python, R, Docker)\n",
      "- The level of expertise of the user (e.g., beginner, intermediate, advanced)\n",
      "\n",
      "The student should be advised to have a basic understanding of bioinformatics tools and workflows, as well as access to the necessary computational resources and software. They should also be familiar with the sequencing data format and the reference genome sequence. The student should be able to use Python or R for data analysis and visualization, and have access to Docker for running bioinformatics tools. The desired output format and level of expertise will determine the complexity of the workflow and the level of detail in the report.\n"
     ]
    }
   ],
   "source": [
    "# Open a file to save the output\n",
    "with open(\"multiagent_output.txt\", \"w\") as file:\n",
    "    for key, value in results.items():\n",
    "        # Print each key-value pair\n",
    "        print(f\"{key}: {value}\")\n",
    "        \n",
    "        # Write each key-value pair to the file\n",
    "        file.write(f\"{key}: {value}\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
