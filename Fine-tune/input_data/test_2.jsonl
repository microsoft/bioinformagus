{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_177", "messages": [{"role": "user", "content": "salmon-1.10.2"}, {"role": "assistant", "content": "salmon v1.10.2\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_150", "messages": [{"role": "user", "content": "salmon-0.11.2"}, {"role": "assistant", "content": "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.11.2\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_229", "messages": [{"role": "user", "content": "vg-1.28.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_38", "messages": [{"role": "user", "content": "bedtools-2.29.0"}, {"role": "assistant", "content": "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.29.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_244", "messages": [{"role": "user", "content": "vg-1.43.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_199", "messages": [{"role": "user", "content": "samtools-1.11"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.11 (using htslib 1.11)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_19", "messages": [{"role": "user", "content": "bcftools-1.15.1"}, {"role": "assistant", "content": "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.15.1 (using htslib 1.16)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    38 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_42", "messages": [{"role": "user", "content": "bedtools-2.31.0"}, {"role": "assistant", "content": "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.31.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General Parameters ]\n     --cram-ref    Reference used by a CRAM input\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_200", "messages": [{"role": "user", "content": "samtools-1.12"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.12 (using htslib 1.12)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_97", "messages": [{"role": "user", "content": "bwa-0.7.13"}, {"role": "assistant", "content": "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.13-r1126\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_55", "messages": [{"role": "user", "content": "blast-2.14.0"}, {"role": "assistant", "content": "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_101", "messages": [{"role": "user", "content": "bwa-v0.7.13"}, {"role": "assistant", "content": "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.13-r1126\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_237", "messages": [{"role": "user", "content": "vg-1.36.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_185", "messages": [{"role": "user", "content": "samtools-1.8"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.8 (using htslib 1.8)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_145", "messages": [{"role": "user", "content": "plink-1.90b6.18"}, {"role": "assistant", "content": "PLINK v1.90b6.18 64-bit (16 Jun 2020)          www.cog-genomics.org/plink/1.9/\n(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * <angle brackets> denote a required parameter, where the text between the\n    angle brackets describes its nature.\n  * [\\'square brackets + single-quotes\\'] denotes an optional modifier.  Use the\n    EXACT text in the quotes.\n  * [{bar|separated|braced|bracketed|values}] denotes a collection of mutually\n    exclusive optional modifiers (again, the exact text must be used).  When\n    there are no outer square brackets, one of the choices must be selected.\n  * [\\'quoted_text=\\'<description of value>] denotes an optional modifier that\n    must begin with the quoted text, and be followed by a value with no\n    whitespace in between.  \\'|\\'may also be used here to indicate mutually\n    exclusive options.\n  * [square brackets without quotes or braces] denote an optional parameter,\n    where the text between the brackets describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]\n  plink --help [flag name(s)...]\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile [prefix] : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed <filename> : Specify full name of .bed file.\n  --bim <filename> : Specify full name of .bim file.\n  --fam <filename> : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file [prefix]  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped <filename> : Specify full name of .ped file.\n  --map <filename> : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile [prefix] : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped <fname>   : Specify full name of .tped file.\n  --tfam <fname>   : Specify full name of .tfam file.\n\n  --lfile [prefix] : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen <fname>   : Specify full name of .lgen file.\n  --reference <fn> : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf <filename> : Specify full name of .vcf or .vcf.gz file.\n  --bcf <filename> : Specify full name of BCF2 file.\n\n  --data [prefix]  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen <filename> : Specify full name of .gen or .gen.gz file.\n  --bgen <f> [\\'snpid-chr\\'] : Specify full name of .bgen file.\n  --sample <fname> : Specify full name of .sample file.\n\n  --23file <fname> [FID] [IID] [sex] [pheno] [pat. ID] [mat. ID] :\n    Specify 23andMe input file.\n\n  --grm-gz [prfx]  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin [prfx] : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy <sample ct> <SNP ct> [missing geno freq] [missing pheno freq]\n          [{acgt | 1234 | 12}] [\\'scalar-pheno\\']\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate <simulation parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n  --simulate-qt <sim. parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.<extension>\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out <prefix>   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode <output format> [{01 | 12}] [{tab | tabx | spacex | bgz | gen-gz}]\n           [\\'include-alt\\'] [\\'omit-nonmale-y\\']\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-<chr #>.recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-<chr #>{.ped,.info}\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan [\\'verbose\\']\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster [\\'omit-unassigned\\']\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to <output prefix>.set, while --set-table writes a\n    variant-by-set membership table to <output prefix>.set.table.\n\n  --merge <.ped filename> <.map filename>\n  --merge <text fileset prefix>\n  --bmerge <.bed filename> <.bim filename> <.fam filename>\n  --bmerge <binary fileset prefix>\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to <output prefix>.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list <filename>\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars [\\'require-same-ref\\'] [\\'ids-only\\'] [\\'suppress-first\\']\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq [{counts | case-control}] [\\'gz\\']\n  --freqx [\\'gz\\']\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing [\\'gz\\']\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n    Unlike most other commands, this doesn\\'t treat het. haploids as missing.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy [\\'midp\\'] [\\'gz\\']\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel [\\'summaries-only\\']\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het [\\'small-sample\\'] [\\'gz\\']\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex [female max F] [male min F]\n  --check-sex ycount [female max F] [male min F] [female max Y obs]\n                     [male min Y obs]\n  --check-sex y-only [female max Y obs] [male min Y obs]\n  --impute-sex [female max F] [male min F]\n  --impute-sex ycount [female max F] [male min F] [female max Y obs]\n                      [male min Y obs]\n  --impute-sex y-only [female max Y obs] [male min Y obs]\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst [\\'case-control\\']\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep <window size>[\\'kb\\'] <step size (variant ct)> <VIF threshold>\n  --indep-pairwise <window size>[\\'kb\\'] <step size (variant ct)> <r^2 threshold>\n  --indep-pairphase <window size>[\\'kb\\'] <step size (variant ct)> <r^2 thresh>\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \"--indep-pairwise 500 kb 5 0.5\"\n    and \"--indep-pairwise 500kb 5 0.5\" have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n      [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n      [\\'yes-really\\']\n  --r2 [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n       [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n       [\\'yes-really\\']\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld <variant ID> <variant ID> [\\'hwe-midp\\']\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags <filename>\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks [\\'no-pheno-req\\'] [\\'no-small-max-span\\']\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \"--recode beagle\" to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance [{square | square0 | triangle}] [{gz | bin | bin4}] [\\'ibs\\']\n             [\\'1-ibs\\'] [\\'allele-ct\\'] [\\'flat-missing\\']\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to <output prefix>.dist, and a list of the\n    corresponding sample IDs to <output prefix>.dist.id.  The first row of the\n    .dist file contains a single <genome 1-genome 2> distance, the second row\n    has the <genome 1-genome 3> and <genome 2-genome 3> distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to <output prefix>.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to <output prefix>.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to <output prefix>.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \"--distance 1-ibs flat-missing\n    square\" and \"--distance ibs flat-missing square\", respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel [{square | square0 | triangle}] [{gz | bin | bin4}]\n             [{cov | ibc2 | ibc3}]\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to <output prefix>.rel, and corresponding IDs to <output prefix>.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz [\\'no-gz\\'] [{cov | ibc2 | ibc3}]\n  --make-grm-bin [{cov | ibc2 | ibc3}]\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff [val]\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    <output prefix>.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test [permutation count]\n  --groupdist [iters] [d]\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance [iters] [d]\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to <number of people>^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel [iters] [d]\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome [\\'gz\\'] [\\'rel-check\\'] [\\'full\\'] [\\'unbounded\\'] [\\'nudge\\']\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg [{group | group-verbose}] [\\'consensus-match\\'] [\\'extend\\']\n            [\\'subtract-1-from-lengths\\']\n  --homozyg-snp <min var count>\n  --homozyg-kb <min length>\n  --homozyg-density <max inverse density (kb/var)>\n  --homozyg-gap <max internal gap kb length>\n  --homozyg-het <max hets>\n  --homozyg-window-snp <scanning window size>\n  --homozyg-window-het <max hets in scanning window hit>\n  --homozyg-window-missing <max missing calls in scanning window hit>\n  --homozyg-window-threshold <min scanning window hit rate>\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group[-verbose]\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group[-verbose]\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as <end bp position> -\n        <start bp position> + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster [\\'cc\\'] [{group-avg | old-tiebreaks}] [\\'missing\\'] [\\'only2\\']\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca [count] [\\'header\\'] [\\'tabs\\'] [\\'var-wts\\']\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour <n1> <n2>\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [{fisher | fisher-midp}]\n          [\\'counts\\'] [\\'set-test\\']\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'qt-means\\'] [\\'lin\\']\n          [\\'set-test\\']\n  --model [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n          [{fisher | fisher-midp | trend-only}] [\\'set-test\\']\n          [{dom | rec | gen | trend}]\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=\\'<value> causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n    (alias: --cmh)\n  --bd [\\'perm\\'| \\'perm-bd\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe [covariate index]\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \"--gxe 3\" to base them on the third covariate instead.\n\n  --linear [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n           [{genotypic | hethom | dominant | recessive | no-snp}]\n           [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n           [\\'standard-beta\\'] [\\'intercept\\']\n  --logistic [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n             [{genotypic | hethom | dominant | recessive | no-snp}]\n             [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n             [\\'intercept\\']\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=\\'<value> starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage <allele dosage file> [\\'noheader\\'] [\\'skip0=\\'<i>] [\\'skip1=\\'<j>]\n           [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --dosage <list file> list [{sepheader | noheader}] [\\'skip0=\\'<i>]\n           [\\'skip1=\\'<j>] [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modifier.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format=\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso <h2 estimate> [min lambda] [\\'report-zeroes\\']\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'midp\\']\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  (Het. haploids are treated as missing.)\n    The \\'midp\\'modifier causes Lancaster\\'s mid-p adjustment to be applied.\n\n  --make-perm-pheno <ct>\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt [{exact | exact-midp | poo}] [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n        [{parentdt1 | parentdt2 | pat | mat}] [\\'set-test\\']\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=\\'requests a family-based adaptive or max(T) permutation\n      test.  By default, the permutation test statistic is the basic TDT\n      p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined test\n      p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-parents [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-between [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-total [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=\\'<value> just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate <PLINK report> [\\'attrib=\\'<file>] [\\'ranges=\\'<file>]\n             [\\'filter=\\'<file>] [\\'snps=\\'<file>] [{NA | prune}] [\\'block\\']\n             [\\'subset=\\'<file>] [\\'minimal\\'] [\\'distance\\']\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=\\'<file> specifies a (possibly gzipped) attribute file.\n    * \\'ranges=\\'<file> specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=\\'<file> causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=\\'<file> causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=\\'<file> causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump <PLINK report filename(s)...>\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report <PLINK report> <gene range file>\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis <PLINK report filenames...>\n  --meta-analysis <PLINK report filenames...> + [{logscale | qt}]\n                  [{no-map | no-allele}] [\\'study\\'] [\\'report-all\\']\n                  [\\'weighted-z\\']\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis [{boost | joint-effects | no-ueki}] [\\'case-only\\']\n                   [{set-by-set | set-by-all}] [\\'nop\\']\n  --epistasis [{set-by-set | set-by-all}]\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge <common file prefix> <ct>\n    When a --[fast-]epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus <variant ID> <variant ID>\n    Two-locus joint genotype count report.\n\n  --score <filename> [i] [j] [k] [\\'header\\'] [{sum | no-sum}]\n          [{no-mean-imputation | center}] [\\'include-cnt\\'] [\\'double-dosage\\']\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --R <R script file> [\\'debug\\']\n    Connect to a Rserve (preferably version 1.7 or later) background process,\n    and execute the Rplink function defined in the input file.  (Unless the\n    \\'debug\\'modifier is present; in that case, the R commands that PLINK would\n    have tried to execute are logged to a file.)\n\n  --write-var-ranges <block ct>\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink/1.9/order .)\n  --script <fname> : Include command-line options from file.\n  --rerun [log]    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype <char> : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid [ID]     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim [d]       : Parse sample IDs as <FID><d><IID> (default delim \\'_\\').\n  --vcf-idspace-to <c> : Convert spaces in sample IDs to the given character.\n  --biallelic-only [\\'strict\\'] [\\'list\\'] : Skip VCF variants with 2+ ALT alleles.\n  --vcf-min-qual <val>           : Skip VCF variants with low/missing QUAL.\n  --vcf-filter [exception(s)...] : Skip variants which have FILTER failures.\n  --vcf-require-gt               : Skip variants with no GT field.\n  --vcf-min-gq <val>             : No-call a genotype when GQ is below the\n                                   given threshold.\n  --vcf-min-gp <val>             : No-call a genotype when 0-1 scaled GP is\n                                   below the given threshold.\n  --vcf-half-call <m>  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr <chr nm> : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name <col nm> : Import named phenotype from the .sample file.\n  --hard-call-threshold <val>  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code [string list] : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases <num>   : Set --simulate case count (default 1000).\n  --simulate-ncontrols <n>  : Set --simulate control count (default 1000).\n  --simulate-prevalence <p> : Set --simulate disease prevalence (default 0.01).\n  --simulate-n <num>        : Set --simulate-qt sample count (default 1000).\n  --simulate-label <prefix> : Set --simulate[-qt] FID/IID name prefix.\n  --simulate-missing <freq> : Set --simulate[-qt] missing genotype frequency.\n  --allow-extra-chr [\\'0\\']   : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set <autosome ct> [\\'no-x\\'] [\\'no-y\\'] [\\'no-xy\\'] [\\'no-mt\\'] :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num <value>    : Alias for \"--chr-set <value> no-y no-xy no-mt\".\n  --cm-map <fname pattern> [chr] : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --allow-no-samples : Allow the input fileset to contain no samples.\n  --allow-no-vars    : Allow the input fileset to contain no variants.\n  --pheno <fname>  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno <n>     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name <c> : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype <v> : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.  This also\n                            forces phenotypes to be interpreted as case/ctrl.\n  --make-pheno <fn> <val> : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno <Lt> [Hbt] : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar <filename> [\\'keep-pheno-on-missing-cov\\'] : Specify covariate file.\n  --covar-name <...>       : Specify covariate(s) in --covar file by name.\n                             Separate multiple names with spaces or commas, and\n                             use dashes to designate ranges.\n  --covar-number <...>     : Specify covariate(s) in --covar file by index.\n  --no-const-covar         : Exclude constant covariates.\n  --allow-no-covars        : Allow no covariates to be loaded from --covar\n                             file.\n  --within <f> [\\'keep-NA\\'] : Specify initial cluster assignments.\n  --mwithin <n>            : Load cluster assignments from column n+2.\n  --family                 : Create a cluster for each family ID.\n  --loop-assoc <f> [\\'keep-NA\\']  : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set <filename>              : Load sets from a .set file.\n  --set-names <name(s)...>      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset <filename>           : Load only sets named in the given text file.\n  --set-collapse-all <set name> : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all <s> : --set-collapse-all + inversion.\n  --make-set <filename>         : Define sets from a list of named bp ranges.\n  --make-set-border <kbs>       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep <filename>       : Exclude all samples not named in the file.\n  --remove <filename>     : Exclude all samples named in the file.\n  --keep-fam <filename>   : Exclude all families not named in the file.\n  --remove-fam <filename> : Exclude all families named in the file.\n  --extract [\\'range\\'] <f> : Exclude all variants not named in the file.\n  --exclude [\\'range\\'] <f> : Exclude all variants named in the file.\n  --keep-clusters <filename>          : These can be used individually or in\n  --keep-cluster-names <name(s)...>     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters <filename>        : Exclude all clusters named in the file.\n  --remove-cluster-names <name(s)...> : Exclude the named clusters.\n  --gene <sets...> : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib <f> [att lst] : Given a file assigning attributes to variants, and a\n  --attrib-indiv <f> [a]   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \"negative match conditions\" instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr <chrs...>  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \"--chr 1-4, 22, xy\".\n  --not-chr <...>  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only [\\'just-acgt\\'] : Exclude non-SNP variants.  By default, SNP = both\n                              allele codes are single-character; \\'just-acgt\\'\n                              restricts codes to {A,C,G,T,a,c,g,t,<missing>}.\n  --from <var ID>  : Use ID(s) to specify a variant range to load.  When used\n  --to   <var ID>    together, both variants must be on the same chromosome.\n  --snp  <var ID>  : Specify a single variant to load.\n  --exclude-snp <> : Specify a single variant to exclude.\n  --window  <kbs>  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp <pos>  : Use physical position(s) to define a variant range to\n  --to-bp   <pos>    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb <pos>    values.  You must also specify a single chromosome (using\n  --to-kb   <pos>    e.g. --chr) when using these flags.\n  --from-mb <pos>\n  --to-mb   <pos>\n  --snps <var IDs...>  : Use IDs to specify variant range(s) to load or\n  --exclude-snps <...>   exclude.  E.g. \"--snps rs1111-rs2222, rs3333, rs4444\".\n  --thin <p>       : Randomly remove variants, retaining each with prob. p.\n  --thin-count <n> : Randomly remove variants until n of them remain.\n  --bp-space <bps> : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv <p>         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count <n>   : Randomly remove samples until n of them remain.\n  --filter <f> <val(s)...> : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter <n>            : Match against (n+2)th column instead.\n  --geno [val]     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind [val]     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing <f1> <f2> : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf [freq]        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf <freq>    : Exclude variants with MAF greater than the threshold.\n  --mac <ct>          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac <ct>      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq <fn> : Estimate MAFs and heterozygote frequencies from the given\n                     --freq[x] report, instead of the input fileset.\n  --hwe <p> [\\'midp\\'] [\\'include-nonctrl\\'] : Exclude variants with Hardy-Weinberg\n                                           equilibrium exact test p-values\n                                           below a threshold.\n  --me <t> <v> [\\'var-first\\'] : Filter out trios and variants with Mendel error\n                               rates exceeding the given thresholds.\n  --me-exclude-one [ratio]   : Make --me exclude only one sample per trio.\n  --qual-scores <f> [qcol] [IDcol] [skip] : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold <min qual score>       : Set --qual-scores range floor.\n  --qual-max-threshold <max qual score>   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders [\\'require-2-missing\\'] [\\'first\\'] :\n    Clear parental IDs for those with 1+ missing parent(s).\n  --recode-allele <fn> : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr <MT code> : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype <ch> : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype <s> : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster <f> : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x <bp1> <bp2> [\\'no-fail\\']\n  --split-x <build> [\\'no-fail\\'] :\n    Changes chromosome code of all chrX variants with bp position <= bp1 or >=\n    bp2 to XY.  The following build codes are supported as shorthand:\n    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n    By default, PLINK errors out when no variants would be affected by\n    --split-x; the \\'no-fail\\'modifier (useful in scripts) overrides this.\n  --merge-x [\\'no-fail\\'] : Merge XY chromosome back with X.\n  --set-me-missing      : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2     : Cause --make-bed to replace all missing calls with\n                          homozygous A2 calls.\n  --set-missing-var-ids <t>   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len <n> : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code <string> : Change unnamed variant code (default \\'.\\').\n  --update-chr  <f> [chrcol] [IDcol]  [skip] : Update variant chromosome codes.\n  --update-cm   <f> [cmcol]  [IDcol]  [skip] : Update centimorgan positions.\n  --update-map  <f> [bpcol]  [IDcol]  [skip] : Update variant bp positions.\n  --update-name <f> [newcol] [oldcol] [skip] : Update variant IDs.\n  --update-alleles <fname> : Update variant allele codes.\n  --allele1234 [\\'multichar\\'] : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                               With \\'multichar\\', converts all A/C/G/Ts in\n                               allele names to 1/2/3/4s.\n  --alleleACGT [\\'multichar\\'] : Reverse of --allele1234.\n  --update-ids <f>     : Update sample IDs.\n  --update-parents <f> : Update parental IDs.\n  --update-sex <f> [n] : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip <filename>    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset <fn>   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window <ct+1> : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb <x> : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold <x> : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{,-fid,-iid}.\n  --a1-allele <f> [a1col] [IDcol] [skip] : Force alleles in the file to A1.\n  --a2-allele <filename> [a2col] [IDcol] [skip] :\n    Force alleles in the file to A2.  (\"--a2-allele <VCF filename> 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort <m> [f] : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype [\\'no-parents\\'] [{no-sex | female-2}] :\n    Include more sample info in new .cov file.\n  --dummy-coding [N] [\\'no-round\\'] : Split categorical variables (n categories,\n                                    2 < n <= N, default N is 49) into n-1\n                                    binary dummy variables when writing\n                                    covariate file.\n  --merge-mode <n>   : Adjust --[b]merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window <ct+1> : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb <x> : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm <x> : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 <x> : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp <var ID>  : Set first variant in all --r/--r2 pairs.\n  --ld-snps <vID...> : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list <f>  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb <kbs>     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 <val>     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr <code>   : Set chrX model for --indep[-pairwise], --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb <kbs>      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf <cutoff>  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci <x>  : Set --blocks \"strong LD\" CI thresholds (defaults\n  --blocks-strong-highci <x>   0.70 and 0.98).\n  --blocks-recomb-highci <x> : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac <x>   : Force haploblock <strong LD pairs>:<total\n                               informative pairs> ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=<x>        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists <dist file> [id file] : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap <val>    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min <cutoff>     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max <cutoff>     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match <> : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size <ct>   : Set minimum size of pools in \"--homozyg group\" report.\n  --read-genome <fn> : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc <p-val>    : Specify minimum PPC test p-value within a cluster.\n  --mc <max size>  : Specify maximum cluster size.\n  --mcc <c1> <c2>  : Specify maximum case and control counts per cluster.\n  --K <min count>  : Specify minimum cluster count.\n  --ibm <val>      : Specify minimum identity-by-missingness.\n  --match <f> [mv] : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type <f> : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \"negative matches\" (i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \"positive matches\" (default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch <f> [m] : Force all members of a cluster to have similar\n  --qt <fname>       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names <...> : These can be used individually or in combination\n  --pca-clusters <fname>      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot <dims> [\\'by-cluster\\'] [\\'eigendecomp\\'] [\\'eigvals\\'] :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell <thresh>  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition <var ID> [{dominant | recessive}] : Add one variant as a --linear\n                                                  or --logistic covariate.\n  --condition-list <f> [{dominant | recessive}] : Add variants named in the\n                                                  file as --linear/--logistic\n                                                  covariates.\n  --parameters <...>  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> [...] : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \"--tests all\" to include all terms.\n  --vif <max VIF>     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model <code> : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars [cov(s)...] : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust [\\'gc\\'] [\\'log10\\'] [\\'qq-plot\\'] : Report some multiple-testing\n                                          corrections.\n  --lambda <val>   : Set genomic control lambda for --adjust.\n  --ci <size>      : Report confidence intervals for odds ratios.\n  --pfilter <val>  : Filter out association test results with higher p-values.\n  --aperm <min perms - 1> [max perms] [alpha] [beta] [init interval] [slope] :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      <slope>p + <init interval> more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p <p-val>        : Adjust set test significant variant p-value ceiling\n                           (default 0.05).\n  --set-r2 [v] [\\'write\\'] : Adjust set test significant variant pairwise r^2\n                           ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to <output prefix>.ldset.\n  --set-max <ct>         : Adjust set test maximum # of significant variants\n                           considered per set (default 5).\n  --set-test-lambda <v>  : Specify genomic control correction for set test.\n  --border <kbs>            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field <nm> : Set --annotate variant ID field name.\n  --clump-p1 <pval> : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 <pval> : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 <r^2>  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb <kbs>  : Set --clump kb radius (default 250).\n  --clump-snp-field <n...>  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field <name...>   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate <hdr...> : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range <filename>  : Report overlaps between clumps and regions.\n  --clump-range-border <kb> : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-chr-field <n...> : Set --meta-analysis chromosome, variant\n  --meta-analysis-snp-field <n...>   ID, position, A1/A2 allele, p-value,\n  --meta-analysis-bp-field <n...>    standard error, and/or effective sample\n  --meta-analysis-a1-field <n...>    size field names.\n  --meta-analysis-a2-field <n...>    Defaults are \\'CHR\\', \\'SNP\\', \\'BP\\', \\'A1\\',\n  --meta-analysis-p-field <n...>     \\'A2\\', \\'P\\', \\'SE\\', and \\'NMISS\\',\n  --meta-analysis-se-field <n...>    respectively.  When multiple parameters\n  --meta-analysis-ess-field <n...>   are given to these flags, earlier names\n                                     take precedence over later ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/<# cases> + 1/<# controls>).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border <kbs>   : Extend --gene-report regions by given # of kbs.\n  --gene-subset <filename>   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field <> : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap <kbs>      : Set \"--fast-epistasis case-only\" min. gap (default 1000).\n  --epi1 <p-value> : Set --[fast-]epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 <p-value> : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin <n> : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range <range file> <data file> [i] [j] [\\'header\\'] :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --R-port <port #>  : Connect to Rserve on a port other than 6311.\n  --R-host <host>    : Connect to Rserve host.\n  --R-socket <sock>  : Connect to Rserve socket.\n  --parallel <k> <n> : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory <val>     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads <val>    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d <char>         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed <val...>    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size <val> : Set number of permutations per batch for some\n                            permutation tests.\n  --output-min-p <p> : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink/1.9 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_125", "messages": [{"role": "user", "content": "picard-2.17.3"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_28", "messages": [{"role": "user", "content": "bedtools-2.26.0gx"}, {"role": "assistant", "content": "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.26.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_234", "messages": [{"role": "user", "content": "vg-1.33.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_155", "messages": [{"role": "user", "content": "salmon-0.13.0"}, {"role": "assistant", "content": "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.13.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_74", "messages": [{"role": "user", "content": "bowtie2-2.3.2"}, {"role": "assistant", "content": "Bowtie 2 version 2.3.2 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-noqname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_128", "messages": [{"role": "user", "content": "picard-2.11.0"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_36", "messages": [{"role": "user", "content": "bedtools-2.28.0"}, {"role": "assistant", "content": "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.28.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_245", "messages": [{"role": "user", "content": "vg-1.41.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_135", "messages": [{"role": "user", "content": "picard-2.14"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_165", "messages": [{"role": "user", "content": "salmon-1.2.1"}, {"role": "assistant", "content": "salmon v1.2.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_247", "messages": [{"role": "user", "content": "vg-1.39.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_238", "messages": [{"role": "user", "content": "vg-1.37.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_178", "messages": [{"role": "user", "content": "salmon-1.10.3"}, {"role": "assistant", "content": "salmon v1.10.3\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_120", "messages": [{"role": "user", "content": "htseq-0.11.3"}, {"role": "assistant", "content": "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the GTF file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        Type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        Whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        Skip all reads with MAPQ alignment quality lower than\n                        the given minimum value (default: 10). MAPQ is the 5th\n                        column of a SAM/BAM file and its usage depends on the\n                        software used to map the reads.\n  -t FEATURETYPE, --type FEATURETYPE\n                        Feature type (3rd column in GTF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GTF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name). Use multiple times\n                        for each different attribute\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        Mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS, --samout SAMOUTS\n                        Write out all SAM alignment records into SAM files\n                        (one per input file needed), annotating each line with\n                        its feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           Suppress progress report\n  --version             Show software version and exit\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL) and Fabio Zanini (fabio.zanini@stanford.edu), Stanford\nUniversity. (c) 2010-2019. Released under the terms of the GNU General Public\nLicense v3. Part of the \\'HTSeq\\'framework, version 0.11.3.\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_111", "messages": [{"role": "user", "content": "htseq-0.11.2"}, {"role": "assistant", "content": "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name). Use multiple times\n                        for each different attribute\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS, --samout SAMOUTS\n                        write out all SAM alignment records into SAM files\n                        (one per input file needed), annotating each line with\n                        its feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.11.2.\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_91", "messages": [{"role": "user", "content": "bwa-0.7.17"}, {"role": "assistant", "content": "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_160", "messages": [{"role": "user", "content": "salmon-1.0.0"}, {"role": "assistant", "content": "salmon v1.0.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_180", "messages": [{"role": "user", "content": "samtools-1.7"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.7 (using htslib 1.7)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_146", "messages": [{"role": "user", "content": "plink-1.90b6.21"}, {"role": "assistant", "content": "PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * <angle brackets> denote a required parameter, where the text between the\n    angle brackets describes its nature.\n  * [\\'square brackets + single-quotes\\'] denotes an optional modifier.  Use the\n    EXACT text in the quotes.\n  * [{bar|separated|braced|bracketed|values}] denotes a collection of mutually\n    exclusive optional modifiers (again, the exact text must be used).  When\n    there are no outer square brackets, one of the choices must be selected.\n  * [\\'quoted_text=\\'<description of value>] denotes an optional modifier that\n    must begin with the quoted text, and be followed by a value with no\n    whitespace in between.  \\'|\\'may also be used here to indicate mutually\n    exclusive options.\n  * [square brackets without quotes or braces] denote an optional parameter,\n    where the text between the brackets describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]\n  plink --help [flag name(s)...]\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile [prefix] : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed <filename> : Specify full name of .bed file.\n  --bim <filename> : Specify full name of .bim file.\n  --fam <filename> : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file [prefix]  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped <filename> : Specify full name of .ped file.\n  --map <filename> : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile [prefix] : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped <fname>   : Specify full name of .tped file.\n  --tfam <fname>   : Specify full name of .tfam file.\n\n  --lfile [prefix] : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen <fname>   : Specify full name of .lgen file.\n  --reference <fn> : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf <filename> : Specify full name of .vcf or .vcf.gz file.\n  --bcf <filename> : Specify full name of BCF2 file.\n\n  --data [prefix]  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen <filename> : Specify full name of .gen or .gen.gz file.\n  --bgen <f> [\\'snpid-chr\\'] : Specify full name of .bgen file.\n  --sample <fname> : Specify full name of .sample file.\n\n  --23file <fname> [FID] [IID] [sex] [pheno] [pat. ID] [mat. ID] :\n    Specify 23andMe input file.\n\n  --grm-gz [prfx]  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin [prfx] : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy <sample ct> <SNP ct> [missing geno freq] [missing pheno freq]\n          [{acgt | 1234 | 12}] [\\'scalar-pheno\\']\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate <simulation parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n  --simulate-qt <sim. parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.<extension>\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out <prefix>   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode <output format> [{01 | 12}] [{tab | tabx | spacex | bgz | gen-gz}]\n           [\\'include-alt\\'] [\\'omit-nonmale-y\\']\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-<chr #>.recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-<chr #>{.ped,.info}\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan [\\'verbose\\']\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster [\\'omit-unassigned\\']\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to <output prefix>.set, while --set-table writes a\n    variant-by-set membership table to <output prefix>.set.table.\n\n  --merge <.ped filename> <.map filename>\n  --merge <text fileset prefix>\n  --bmerge <.bed filename> <.bim filename> <.fam filename>\n  --bmerge <binary fileset prefix>\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to <output prefix>.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list <filename>\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars [\\'require-same-ref\\'] [\\'ids-only\\'] [\\'suppress-first\\']\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq [{counts | case-control}] [\\'gz\\']\n  --freqx [\\'gz\\']\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing [\\'gz\\']\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n    Unlike most other commands, this doesn\\'t treat het. haploids as missing.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy [\\'midp\\'] [\\'gz\\']\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel [\\'summaries-only\\']\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het [\\'small-sample\\'] [\\'gz\\']\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex [female max F] [male min F]\n  --check-sex ycount [female max F] [male min F] [female max Y obs]\n                     [male min Y obs]\n  --check-sex y-only [female max Y obs] [male min Y obs]\n  --impute-sex [female max F] [male min F]\n  --impute-sex ycount [female max F] [male min F] [female max Y obs]\n                      [male min Y obs]\n  --impute-sex y-only [female max Y obs] [male min Y obs]\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst [\\'case-control\\']\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep <window size>[\\'kb\\'] <step size (variant ct)> <VIF threshold>\n  --indep-pairwise <window size>[\\'kb\\'] <step size (variant ct)> <r^2 threshold>\n  --indep-pairphase <window size>[\\'kb\\'] <step size (variant ct)> <r^2 thresh>\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \"--indep-pairwise 500 kb 5 0.5\"\n    and \"--indep-pairwise 500kb 5 0.5\" have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n      [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n      [\\'yes-really\\']\n  --r2 [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n       [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n       [\\'yes-really\\']\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld <variant ID> <variant ID> [\\'hwe-midp\\']\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags <filename>\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks [\\'no-pheno-req\\'] [\\'no-small-max-span\\']\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \"--recode beagle\" to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance [{square | square0 | triangle}] [{gz | bin | bin4}] [\\'ibs\\']\n             [\\'1-ibs\\'] [\\'allele-ct\\'] [\\'flat-missing\\']\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to <output prefix>.dist, and a list of the\n    corresponding sample IDs to <output prefix>.dist.id.  The first row of the\n    .dist file contains a single <genome 1-genome 2> distance, the second row\n    has the <genome 1-genome 3> and <genome 2-genome 3> distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to <output prefix>.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to <output prefix>.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to <output prefix>.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \"--distance 1-ibs flat-missing\n    square\" and \"--distance ibs flat-missing square\", respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel [{square | square0 | triangle}] [{gz | bin | bin4}]\n             [{cov | ibc2 | ibc3}]\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to <output prefix>.rel, and corresponding IDs to <output prefix>.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz [\\'no-gz\\'] [{cov | ibc2 | ibc3}]\n  --make-grm-bin [{cov | ibc2 | ibc3}]\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff [val]\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    <output prefix>.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test [permutation count]\n  --groupdist [iters] [d]\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance [iters] [d]\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to <number of people>^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel [iters] [d]\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome [\\'gz\\'] [\\'rel-check\\'] [\\'full\\'] [\\'unbounded\\'] [\\'nudge\\']\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg [{group | group-verbose}] [\\'consensus-match\\'] [\\'extend\\']\n            [\\'subtract-1-from-lengths\\']\n  --homozyg-snp <min var count>\n  --homozyg-kb <min length>\n  --homozyg-density <max inverse density (kb/var)>\n  --homozyg-gap <max internal gap kb length>\n  --homozyg-het <max hets>\n  --homozyg-window-snp <scanning window size>\n  --homozyg-window-het <max hets in scanning window hit>\n  --homozyg-window-missing <max missing calls in scanning window hit>\n  --homozyg-window-threshold <min scanning window hit rate>\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group[-verbose]\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group[-verbose]\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as <end bp position> -\n        <start bp position> + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster [\\'cc\\'] [{group-avg | old-tiebreaks}] [\\'missing\\'] [\\'only2\\']\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca [count] [\\'header\\'] [\\'tabs\\'] [\\'var-wts\\']\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour <n1> <n2>\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [{fisher | fisher-midp}]\n          [\\'counts\\'] [\\'set-test\\']\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'qt-means\\'] [\\'lin\\']\n          [\\'set-test\\']\n  --model [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n          [{fisher | fisher-midp | trend-only}] [\\'set-test\\']\n          [{dom | rec | gen | trend}]\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=\\'<value> causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n    (alias: --cmh)\n  --bd [\\'perm\\'| \\'perm-bd\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe [covariate index]\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \"--gxe 3\" to base them on the third covariate instead.\n\n  --linear [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n           [{genotypic | hethom | dominant | recessive | no-snp}]\n           [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n           [\\'standard-beta\\'] [\\'intercept\\']\n  --logistic [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n             [{genotypic | hethom | dominant | recessive | no-snp}]\n             [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n             [\\'intercept\\']\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=\\'<value> starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage <allele dosage file> [\\'noheader\\'] [\\'skip0=\\'<i>] [\\'skip1=\\'<j>]\n           [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --dosage <list file> list [{sepheader | noheader}] [\\'skip0=\\'<i>]\n           [\\'skip1=\\'<j>] [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modifier.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format=\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso <h2 estimate> [min lambda] [\\'report-zeroes\\']\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'midp\\']\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  (Het. haploids are treated as missing.)\n    The \\'midp\\'modifier causes Lancaster\\'s mid-p adjustment to be applied.\n\n  --make-perm-pheno <ct>\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt [{exact | exact-midp | poo}] [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n        [{parentdt1 | parentdt2 | pat | mat}] [\\'set-test\\']\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=\\'requests a family-based adaptive or max(T) permutation\n      test.  By default, the permutation test statistic is the basic TDT\n      p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined test\n      p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-parents [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-between [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-total [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=\\'<value> just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate <PLINK report> [\\'attrib=\\'<file>] [\\'ranges=\\'<file>]\n             [\\'filter=\\'<file>] [\\'snps=\\'<file>] [{NA | prune}] [\\'block\\']\n             [\\'subset=\\'<file>] [\\'minimal\\'] [\\'distance\\']\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=\\'<file> specifies a (possibly gzipped) attribute file.\n    * \\'ranges=\\'<file> specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=\\'<file> causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=\\'<file> causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=\\'<file> causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump <PLINK report filename(s)...>\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report <PLINK report> <gene range file>\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis <PLINK report filenames...>\n  --meta-analysis <PLINK report filenames...> + [{logscale | qt}]\n                  [{no-map | no-allele}] [\\'study\\'] [\\'report-all\\']\n                  [\\'weighted-z\\']\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis [{boost | joint-effects | no-ueki}] [\\'case-only\\']\n                   [{set-by-set | set-by-all}] [\\'nop\\']\n  --epistasis [{set-by-set | set-by-all}]\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge <common file prefix> <ct>\n    When a --[fast-]epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus <variant ID> <variant ID>\n    Two-locus joint genotype count report.\n\n  --score <filename> [i] [j] [k] [\\'header\\'] [{sum | no-sum}]\n          [{no-mean-imputation | center}] [\\'include-cnt\\'] [\\'double-dosage\\']\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --R <R script file> [\\'debug\\']\n    Connect to a Rserve (preferably version 1.7 or later) background process,\n    and execute the Rplink function defined in the input file.  (Unless the\n    \\'debug\\'modifier is present; in that case, the R commands that PLINK would\n    have tried to execute are logged to a file.)\n\n  --write-var-ranges <block ct>\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink/1.9/order .)\n  --script <fname> : Include command-line options from file.\n  --rerun [log]    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype <char> : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid [ID]     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim [d]       : Parse sample IDs as <FID><d><IID> (default delim \\'_\\').\n  --vcf-idspace-to <c> : Convert spaces in sample IDs to the given character.\n  --biallelic-only [\\'strict\\'] [\\'list\\'] : Skip VCF variants with 2+ ALT alleles.\n  --vcf-min-qual <val>           : Skip VCF variants with low/missing QUAL.\n  --vcf-filter [exception(s)...] : Skip variants which have FILTER failures.\n  --vcf-require-gt               : Skip variants with no GT field.\n  --vcf-min-gq <val>             : No-call a genotype when GQ is below the\n                                   given threshold.\n  --vcf-min-gp <val>             : No-call a genotype when 0-1 scaled GP is\n                                   below the given threshold.\n  --vcf-half-call <m>  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr <chr nm> : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name <col nm> : Import named phenotype from the .sample file.\n  --hard-call-threshold <val>  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code [string list] : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases <num>   : Set --simulate case count (default 1000).\n  --simulate-ncontrols <n>  : Set --simulate control count (default 1000).\n  --simulate-prevalence <p> : Set --simulate disease prevalence (default 0.01).\n  --simulate-n <num>        : Set --simulate-qt sample count (default 1000).\n  --simulate-label <prefix> : Set --simulate[-qt] FID/IID name prefix.\n  --simulate-missing <freq> : Set --simulate[-qt] missing genotype frequency.\n  --allow-extra-chr [\\'0\\']   : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set <autosome ct> [\\'no-x\\'] [\\'no-y\\'] [\\'no-xy\\'] [\\'no-mt\\'] :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num <value>    : Alias for \"--chr-set <value> no-y no-xy no-mt\".\n  --cm-map <fname pattern> [chr] : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --allow-no-samples : Allow the input fileset to contain no samples.\n  --allow-no-vars    : Allow the input fileset to contain no variants.\n  --pheno <fname>  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno <n>     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name <c> : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype <v> : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.  This also\n                            forces phenotypes to be interpreted as case/ctrl.\n  --make-pheno <fn> <val> : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno <Lt> [Hbt] : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar <filename> [\\'keep-pheno-on-missing-cov\\'] : Specify covariate file.\n  --covar-name <...>       : Specify covariate(s) in --covar file by name.\n                             Separate multiple names with spaces or commas, and\n                             use dashes to designate ranges.\n  --covar-number <...>     : Specify covariate(s) in --covar file by index.\n  --no-const-covar         : Exclude constant covariates.\n  --allow-no-covars        : Allow no covariates to be loaded from --covar\n                             file.\n  --within <f> [\\'keep-NA\\'] : Specify initial cluster assignments.\n  --mwithin <n>            : Load cluster assignments from column n+2.\n  --family                 : Create a cluster for each family ID.\n  --loop-assoc <f> [\\'keep-NA\\']  : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set <filename>              : Load sets from a .set file.\n  --set-names <name(s)...>      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset <filename>           : Load only sets named in the given text file.\n  --set-collapse-all <set name> : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all <s> : --set-collapse-all + inversion.\n  --make-set <filename>         : Define sets from a list of named bp ranges.\n  --make-set-border <kbs>       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep <filename>       : Exclude all samples not named in the file.\n  --remove <filename>     : Exclude all samples named in the file.\n  --keep-fam <filename>   : Exclude all families not named in the file.\n  --remove-fam <filename> : Exclude all families named in the file.\n  --extract [\\'range\\'] <f> : Exclude all variants not named in the file.\n  --exclude [\\'range\\'] <f> : Exclude all variants named in the file.\n  --keep-clusters <filename>          : These can be used individually or in\n  --keep-cluster-names <name(s)...>     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters <filename>        : Exclude all clusters named in the file.\n  --remove-cluster-names <name(s)...> : Exclude the named clusters.\n  --gene <sets...> : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib <f> [att lst] : Given a file assigning attributes to variants, and a\n  --attrib-indiv <f> [a]   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \"negative match conditions\" instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr <chrs...>  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \"--chr 1-4, 22, xy\".\n  --not-chr <...>  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only [\\'just-acgt\\'] : Exclude non-SNP variants.  By default, SNP = both\n                              allele codes are single-character; \\'just-acgt\\'\n                              restricts codes to {A,C,G,T,a,c,g,t,<missing>}.\n  --from <var ID>  : Use ID(s) to specify a variant range to load.  When used\n  --to   <var ID>    together, both variants must be on the same chromosome.\n  --snp  <var ID>  : Specify a single variant to load.\n  --exclude-snp <> : Specify a single variant to exclude.\n  --window  <kbs>  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp <pos>  : Use physical position(s) to define a variant range to\n  --to-bp   <pos>    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb <pos>    values.  You must also specify a single chromosome (using\n  --to-kb   <pos>    e.g. --chr) when using these flags.\n  --from-mb <pos>\n  --to-mb   <pos>\n  --snps <var IDs...>  : Use IDs to specify variant range(s) to load or\n  --exclude-snps <...>   exclude.  E.g. \"--snps rs1111-rs2222, rs3333, rs4444\".\n  --thin <p>       : Randomly remove variants, retaining each with prob. p.\n  --thin-count <n> : Randomly remove variants until n of them remain.\n  --bp-space <bps> : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv <p>         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count <n>   : Randomly remove samples until n of them remain.\n  --filter <f> <val(s)...> : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter <n>            : Match against (n+2)th column instead.\n  --geno [val]     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind [val]     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing <f1> <f2> : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf [freq]        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf <freq>    : Exclude variants with MAF greater than the threshold.\n  --mac <ct>          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac <ct>      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq <fn> : Estimate MAFs and heterozygote frequencies from the given\n                     --freq[x] report, instead of the input fileset.\n  --hwe <p> [\\'midp\\'] [\\'include-nonctrl\\'] : Exclude variants with Hardy-Weinberg\n                                           equilibrium exact test p-values\n                                           below a threshold.\n  --me <t> <v> [\\'var-first\\'] : Filter out trios and variants with Mendel error\n                               rates exceeding the given thresholds.\n  --me-exclude-one [ratio]   : Make --me exclude only one sample per trio.\n  --qual-scores <f> [qcol] [IDcol] [skip] : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold <min qual score>       : Set --qual-scores range floor.\n  --qual-max-threshold <max qual score>   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders [\\'require-2-missing\\'] [\\'first\\'] :\n    Clear parental IDs for those with 1+ missing parent(s).\n  --recode-allele <fn> : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr <MT code> : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype <ch> : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype <s> : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster <f> : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x <bp1> <bp2> [\\'no-fail\\']\n  --split-x <build> [\\'no-fail\\'] :\n    Changes chromosome code of all chrX variants with bp position <= bp1 or >=\n    bp2 to XY.  The following build codes are supported as shorthand:\n    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n    By default, PLINK errors out when no variants would be affected by\n    --split-x; the \\'no-fail\\'modifier (useful in scripts) overrides this.\n  --merge-x [\\'no-fail\\'] : Merge XY chromosome back with X.\n  --set-me-missing      : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2     : Cause --make-bed to replace all missing calls with\n                          homozygous A2 calls.\n  --set-missing-var-ids <t>   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len <n> : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code <string> : Change unnamed variant code (default \\'.\\').\n  --update-chr  <f> [chrcol] [IDcol]  [skip] : Update variant chromosome codes.\n  --update-cm   <f> [cmcol]  [IDcol]  [skip] : Update centimorgan positions.\n  --update-map  <f> [bpcol]  [IDcol]  [skip] : Update variant bp positions.\n  --update-name <f> [newcol] [oldcol] [skip] : Update variant IDs.\n  --update-alleles <fname> : Update variant allele codes.\n  --allele1234 [\\'multichar\\'] : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                               With \\'multichar\\', converts all A/C/G/Ts in\n                               allele names to 1/2/3/4s.\n  --alleleACGT [\\'multichar\\'] : Reverse of --allele1234.\n  --update-ids <f>     : Update sample IDs.\n  --update-parents <f> : Update parental IDs.\n  --update-sex <f> [n] : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip <filename>    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset <fn>   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window <ct+1> : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb <x> : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold <x> : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{,-fid,-iid}.\n  --a1-allele <f> [a1col] [IDcol] [skip] : Force alleles in the file to A1.\n  --a2-allele <filename> [a2col] [IDcol] [skip] :\n    Force alleles in the file to A2.  (\"--a2-allele <VCF filename> 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort <m> [f] : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype [\\'no-parents\\'] [{no-sex | female-2}] :\n    Include more sample info in new .cov file.\n  --dummy-coding [N] [\\'no-round\\'] : Split categorical variables (n categories,\n                                    2 < n <= N, default N is 49) into n-1\n                                    binary dummy variables when writing\n                                    covariate file.\n  --merge-mode <n>   : Adjust --[b]merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window <ct+1> : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb <x> : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm <x> : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 <x> : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp <var ID>  : Set first variant in all --r/--r2 pairs.\n  --ld-snps <vID...> : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list <f>  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb <kbs>     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 <val>     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr <code>   : Set chrX model for --indep[-pairwise], --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb <kbs>      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf <cutoff>  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci <x>  : Set --blocks \"strong LD\" CI thresholds (defaults\n  --blocks-strong-highci <x>   0.70 and 0.98).\n  --blocks-recomb-highci <x> : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac <x>   : Force haploblock <strong LD pairs>:<total\n                               informative pairs> ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=<x>        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists <dist file> [id file] : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap <val>    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min <cutoff>     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max <cutoff>     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match <> : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size <ct>   : Set minimum size of pools in \"--homozyg group\" report.\n  --read-genome <fn> : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc <p-val>    : Specify minimum PPC test p-value within a cluster.\n  --mc <max size>  : Specify maximum cluster size.\n  --mcc <c1> <c2>  : Specify maximum case and control counts per cluster.\n  --K <min count>  : Specify minimum cluster count.\n  --ibm <val>      : Specify minimum identity-by-missingness.\n  --match <f> [mv] : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type <f> : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \"negative matches\" (i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \"positive matches\" (default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch <f> [m] : Force all members of a cluster to have similar\n  --qt <fname>       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names <...> : These can be used individually or in combination\n  --pca-clusters <fname>      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot <dims> [\\'by-cluster\\'] [\\'eigendecomp\\'] [\\'eigvals\\'] :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell <thresh>  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition <var ID> [{dominant | recessive}] : Add one variant as a --linear\n                                                  or --logistic covariate.\n  --condition-list <f> [{dominant | recessive}] : Add variants named in the\n                                                  file as --linear/--logistic\n                                                  covariates.\n  --parameters <...>  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> [...] : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \"--tests all\" to include all terms.\n  --vif <max VIF>     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model <code> : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars [cov(s)...] : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust [\\'gc\\'] [\\'log10\\'] [\\'qq-plot\\'] : Report some multiple-testing\n                                          corrections.\n  --lambda <val>   : Set genomic control lambda for --adjust.\n  --ci <size>      : Report confidence intervals for odds ratios.\n  --pfilter <val>  : Filter out association test results with higher p-values.\n  --aperm <min perms - 1> [max perms] [alpha] [beta] [init interval] [slope] :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      <slope>p + <init interval> more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p <p-val>        : Adjust set test significant variant p-value ceiling\n                           (default 0.05).\n  --set-r2 [v] [\\'write\\'] : Adjust set test significant variant pairwise r^2\n                           ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to <output prefix>.ldset.\n  --set-max <ct>         : Adjust set test maximum # of significant variants\n                           considered per set (default 5).\n  --set-test-lambda <v>  : Specify genomic control correction for set test.\n  --border <kbs>            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field <nm> : Set --annotate variant ID field name.\n  --clump-p1 <pval> : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 <pval> : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 <r^2>  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb <kbs>  : Set --clump kb radius (default 250).\n  --clump-snp-field <n...>  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field <name...>   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate <hdr...> : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range <filename>  : Report overlaps between clumps and regions.\n  --clump-range-border <kb> : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-chr-field <n...> : Set --meta-analysis chromosome, variant\n  --meta-analysis-snp-field <n...>   ID, position, A1/A2 allele, p-value,\n  --meta-analysis-bp-field <n...>    standard error, and/or effective sample\n  --meta-analysis-a1-field <n...>    size field names.\n  --meta-analysis-a2-field <n...>    Defaults are \\'CHR\\', \\'SNP\\', \\'BP\\', \\'A1\\',\n  --meta-analysis-p-field <n...>     \\'A2\\', \\'P\\', \\'SE\\', and \\'NMISS\\',\n  --meta-analysis-se-field <n...>    respectively.  When multiple parameters\n  --meta-analysis-ess-field <n...>   are given to these flags, earlier names\n                                     take precedence over later ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/<# cases> + 1/<# controls>).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border <kbs>   : Extend --gene-report regions by given # of kbs.\n  --gene-subset <filename>   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field <> : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap <kbs>      : Set \"--fast-epistasis case-only\" min. gap (default 1000).\n  --epi1 <p-value> : Set --[fast-]epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 <p-value> : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin <n> : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range <range file> <data file> [i] [j] [\\'header\\'] :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --R-port <port #>  : Connect to Rserve on a port other than 6311.\n  --R-host <host>    : Connect to Rserve host.\n  --R-socket <sock>  : Connect to Rserve socket.\n  --parallel <k> <n> : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory <val>     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads <val>    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d <char>         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed <val...>    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size <val> : Set number of permutations per batch for some\n                            permutation tests.\n  --native           : Allow Intel MKL to use processor-dependent code paths.\n  --output-min-p <p> : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink/1.9 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_188", "messages": [{"role": "user", "content": "samtools-1.2.rglab"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.2-rglab-CRG (using htslib 1.2.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_62", "messages": [{"role": "user", "content": "bowtie-1.2.3"}, {"role": "assistant", "content": "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_182", "messages": [{"role": "user", "content": "samtools-1.3"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.3 (using htslib 1.3)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_25", "messages": [{"role": "user", "content": "bedtools-2.27.1"}, {"role": "assistant", "content": "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.27.1\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_20", "messages": [{"role": "user", "content": "bcftools-1.16"}, {"role": "assistant", "content": "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.16 (using htslib 1.17)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    39 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_235", "messages": [{"role": "user", "content": "vg-1.34.0"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_61", "messages": [{"role": "user", "content": "bowtie-v1.2.2dfsg-4-deb"}, {"role": "assistant", "content": "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_132", "messages": [{"role": "user", "content": "picard-2.15.0"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_86", "messages": [{"role": "user", "content": "bowtie2-2.5.0"}, {"role": "assistant", "content": "Bowtie 2 version 2.5.0 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_228", "messages": [{"role": "user", "content": "vg-1.27.1"}, {"role": "assistant", "content": "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_169", "messages": [{"role": "user", "content": "salmon-1.5.1"}, {"role": "assistant", "content": "salmon v1.5.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_60", "messages": [{"role": "user", "content": "bowtie-1.2.0"}, {"role": "assistant", "content": "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_206", "messages": [{"role": "user", "content": "samtools-1.17"}, {"role": "assistant", "content": "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.17 (using htslib 1.17)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_107", "messages": [{"role": "user", "content": "hisat2-v2.0.5-1-deb"}, {"role": "assistant", "content": "\nHISAT2 version 2.0.5 by Daehwan Kim (infphilo@gmail.com, www.ccb.jhu.edu/people/infphilo)\nUsage: \n  hisat2 [options]* -x <ht2-idx> {-1 <m1> -2 <m2> | -U <r> | --sra-acc <SRA accession number>} [-S <sam>]\n\n  <ht2-idx>  Index filename prefix (minus trailing .X.ht2).\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <SRA accession number>        Comma-separated list of SRA accession numbers, e.g. --sra-acc SRR353653,SRR353654.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n  --sra-acc          SRA accession ID\n\n Alignment:\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n\n Spliced Alignment:\n  --pen-cansplice <int>              penalty for a canonical splice site (0)\n  --pen-noncansplice <int>           penalty for a non-canonical splice site (12)\n  --pen-canintronlen <func>          penalty for long introns (G,-8,1) with canonical splice sites\n  --pen-noncanintronlen <func>       penalty for long introns (G,-8,1) with noncanonical splice sites\n  --min-intronlen <int>              minimum intron length (20)\n  --max-intronlen <int>              maximum intron length (500000)\n  --known-splicesite-infile <path>   provide a list of known splice sites\n  --novel-splicesite-outfile <path>  report a list of splice sites\n  --novel-splicesite-infile <path>   provide a list of novel splice sites\n  --no-temp-splicesite               disable the use of splice sites found\n  --no-spliced-alignment             disable spliced alignment\n  --rna-strandness <string>          Specify strand-specific information (unstranded)\n  --tmo                              Reports only those alignments within known transcriptome\n  --dta                              Reports alignments tailored for transcript assemblers\n  --dta-cufflinks                    Reports alignments tailored specifically for cufflinks\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>,<int>   max and min penalties for mismatch; lower qual = lower penalty <2,6>\n  --sp <int>,<int>   max and min penalties for soft-clipping; lower qual = lower penalty <1,2>\n  --no-softclip      no soft-clipping\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (L,0.0,-0.2)\n\n Reporting:\n  -k <int> (default: 5) report up to <int> alns per read\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0), only valid with --no-spliced-alignment\n  -X/--maxins <int>  maximum fragment length (500), only valid with --no-spliced-alignment\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-head          supppress header lines, i.e. lines starting with @\n  --no-sq            supppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -o/--offrate <int> override offrate of index; must be >= index\\'s offrate\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'hisat2\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --remove-chrname   remove \\'chr\\'from reference names in alignment\n  --add-chrname      add \\'chr\\'to reference names in alignment \n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_83", "messages": [{"role": "user", "content": "bowtie2-2.4.3"}, {"role": "assistant", "content": "Bowtie 2 version 2.4.3 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_140", "messages": [{"role": "user", "content": "picard-2.18.12"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSamErrorMetrics                       \u001b[31m\u001b[36mProgram to collect error metrics on bases stratified in various ways.\u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_89", "messages": [{"role": "user", "content": "bowtie2-2.5.3"}, {"role": "assistant", "content": "Bowtie 2 version 2.5.3 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from the FASTA file\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"prompt": "You are a bioinformatics assistant. Please provide help documentation and details about the arguments for these tools.", "prompt_id": "prompt_134", "messages": [{"role": "user", "content": "picard-2.18.6"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
