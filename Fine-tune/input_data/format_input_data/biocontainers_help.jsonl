{"messages": [{"role": "user", "content":   "bcftools-1.7"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.7 (using htslib 1.7)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.3"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.3 (using htslib 1.3)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.9"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.9 (using htslib 1.9)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.8"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.8 (using htslib 1.8)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.3.1"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.3.1 (using htslib 1.3.1)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.6"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.6 (using htslib 1.6)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.2"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.2 (using htslib 1.2.1)\n\nUsage:   bcftools <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.4.1"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.4.1 (using htslib 1.4.1)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.4"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.4 (using htslib 1.4)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-v1.3.1-1b1-deb"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.3.1 (using htslib 1.3.2)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-v1.9-1-deb"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nVersion: 1.9 (using htslib 1.9)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.10.1"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.10.1 (using htslib 1.10.1)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.10"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.10 (using htslib 1.10)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.10.2"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.10.2 (using htslib 1.10.1)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.11"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.11 (using htslib 1.11)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.12"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.12 (using htslib 1.12)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.13"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.13 (using htslib 1.13)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.14"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.14 (using htslib 1.14)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.15"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.15 (using htslib 1.15)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    38 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.15.1"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.15.1 (using htslib 1.16)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    38 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.16"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.16 (using htslib 1.17)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    39 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.17"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.17 (using htslib 1.17)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    40 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.18"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.18 (using htslib 1.18)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    39 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.19"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.19 (using htslib 1.19.1)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    39 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bcftools-1.20"}, {"role": "assistant", "content":  "\nProgram: bcftools (Tools for variant calling and manipulating VCFs and BCFs)\nLicense: GNU GPLv3+, due to use of the GNU Scientific Library\nVersion: 1.20 (using htslib 1.20)\n\nUsage:   bcftools [--version|--version-only] [--help] <command> <argument>\n\nCommands:\n\n -- Indexing\n    index        index VCF/BCF files\n\n -- VCF/BCF manipulation\n    annotate     annotate and edit VCF/BCF files\n    concat       concatenate VCF/BCF files from the same set of samples\n    convert      convert VCF/BCF files to different formats and back\n    head         view VCF/BCF file headers\n    isec         intersections of VCF/BCF files\n    merge        merge VCF/BCF files files from non-overlapping sample sets\n    norm         left-align and normalize indels\n    plugin       user-defined plugins\n    query        transform VCF/BCF into user-defined formats\n    reheader     modify VCF/BCF header, change sample names\n    sort         sort VCF/BCF file\n    view         VCF/BCF conversion, view, subset and filter VCF/BCF files\n\n -- VCF/BCF analysis\n    call         SNP/indel calling\n    consensus    create consensus sequence by applying VCF variants\n    cnv          HMM CNV calling\n    csq          call variation consequences\n    filter       filter VCF/BCF files using fixed thresholds\n    gtcheck      check sample concordance, detect sample swaps and contamination\n    mpileup      multi-way pileup producing genotype likelihoods\n    polysomy     detect number of chromosomal copies\n    roh          identify runs of autozygosity (HMM)\n    stats        produce VCF/BCF stats\n\n -- Plugins (collection of programs for calling, file manipulation & analysis)\n    39 plugins available, run \"bcftools plugin -lv\" to see a complete list\n\n Most commands accept VCF, bgzipped VCF, and BCF with the file type detected\n automatically even when streaming from a pipe. Indexed VCF and BCF will work\n in all situations. Un-indexed VCF and BCF and streams will work in most but\n not all situations.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.27.1"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.27.1\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.23.0"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the spacing between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.17.0"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistics tools ]\n    jaccard       Calculates the Jaccard statistic b/w two sets of intervals.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.26.0gx"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.26.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.27.0"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.27.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.25.0"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.20.1"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.19.1"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-v2.25.0"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-v2.27.0"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.27.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-v2.26.0dfsg-3-deb"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.26.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.28.0"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.28.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-v2.27.1dfsg-4-deb"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.27.1\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.29.0"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.29.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.29.1"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.29.1\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.29.2"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.29.2\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.30.0"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.30.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General Parameters ]\n     --cram-ref    Reference used by a CRAM input\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.31.0"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.31.0\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General Parameters ]\n     --cram-ref    Reference used by a CRAM input\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.24"}, {"role": "assistant", "content":  "bedtools: flexible tools for genome arithmetic and DNA sequence analysis.\nusage:    bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistrubute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content":   "bedtools-2.31.1"}, {"role": "assistant", "content":  "bedtools is a powerful toolset for genome arithmetic.\n\nVersion:   v2.31.1\nAbout:     developed in the quinlanlab.org and by many contributors worldwide.\nDocs:      http://bedtools.readthedocs.io/\nCode:      https://github.com/arq5x/bedtools2\nMail:      https://groups.google.com/forum/#!forum/bedtools-discuss\n\nUsage:     bedtools <subcommand> [options]\n\nThe bedtools sub-commands include:\n\n[ Genome arithmetic ]\n    intersect     Find overlapping intervals in various ways.\n    window        Find overlapping intervals within a window around an interval.\n    closest       Find the closest, potentially non-overlapping interval.\n    coverage      Compute the coverage over defined intervals.\n    map           Apply a function to a column for each overlapping interval.\n    genomecov     Compute the coverage over an entire genome.\n    merge         Combine overlapping/nearby intervals into a single interval.\n    cluster       Cluster (but don\\'t merge) overlapping/nearby intervals.\n    complement    Extract intervals _not_ represented by an interval file.\n    shift         Adjust the position of intervals.\n    subtract      Remove intervals based on overlaps b/w two files.\n    slop          Adjust the size of intervals.\n    flank         Create new intervals from the flanks of existing intervals.\n    sort          Order the intervals in a file.\n    random        Generate random intervals in a genome.\n    shuffle       Randomly redistribute intervals in a genome.\n    sample        Sample random records from file using reservoir sampling.\n    spacing       Report the gap lengths between intervals in a file.\n    annotate      Annotate coverage of features from multiple files.\n\n[ Multi-way file comparisons ]\n    multiinter    Identifies common intervals among multiple interval files.\n    unionbedg     Combines coverage intervals from multiple BEDGRAPH files.\n\n[ Paired-end manipulation ]\n    pairtobed     Find pairs that overlap intervals in various ways.\n    pairtopair    Find pairs that overlap other pairs in various ways.\n\n[ Format conversion ]\n    bamtobed      Convert BAM alignments to BED (& other) formats.\n    bedtobam      Convert intervals to BAM records.\n    bamtofastq    Convert BAM records to FASTQ records.\n    bedpetobam    Convert BEDPE intervals to BAM records.\n    bed12tobed6   Breaks BED12 intervals into discrete BED6 intervals.\n\n[ Fasta manipulation ]\n    getfasta      Use intervals to extract sequences from a FASTA file.\n    maskfasta     Use intervals to mask sequences from a FASTA file.\n    nuc           Profile the nucleotide content of intervals in a FASTA file.\n\n[ BAM focused tools ]\n    multicov      Counts coverage from multiple BAMs at specific intervals.\n    tag           Tag BAM alignments based on overlaps with interval files.\n\n[ Statistical relationships ]\n    jaccard       Calculate the Jaccard statistic b/w two sets of intervals.\n    reldist       Calculate the distribution of relative distances b/w two files.\n    fisher        Calculate Fisher statistic b/w two feature files.\n\n[ Miscellaneous tools ]\n    overlap       Computes the amount of overlap from two intervals.\n    igv           Create an IGV snapshot batch script.\n    links         Create a HTML page of links to UCSC locations.\n    makewindows   Make interval \"windows\" across a genome.\n    groupby       Group by common cols. & summarize oth. cols. (~ SQL \"groupBy\")\n    expand        Replicate lines based on lists of values in columns.\n    split         Split a file into multiple files with equal records or base pairs.\n    summary       Statistical summary of intervals in a file.\n\n[ General Parameters ]\n     --cram-ref    Reference used by a CRAM input\n\n[ General help ]\n    --help        Print this help menu.\n    --version     What version of bedtools are you using?.\n    --contact     Feature requests, bugs, mailing lists, etc.\n\n"}]}
{"messages": [{"role": "user", "content": "blast-2.5.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.7.1"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.6.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-v2.2.31"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.2.31"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.9.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.10.1"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.11.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.12.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.13.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.14.0"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content": "blast-2.14.1"}, {"role": "assistant", "content":  "BLAST query/options error: Either a BLAST database or subject sequence(s) must be specified\nPlease refer to the BLAST+ user manual.\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.2.1.1"}, {"role": "assistant", "content":  "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.1.2"}, {"role": "assistant", "content":  "Usage: \nbowtie [options]* <ebwt> {-1 <m1> -2 <m2> | --12 <r> | <s>} [<hit>]\n\n  <m1>    Comma-separated list of files containing upstream mates (or the\n          sequences themselves, if -c is set) paired with mates in <m2>\n  <m2>    Comma-separated list of files containing downstream mates (or the\n          sequences themselves if -c is set) paired with mates in <m1>\n  <r>     Comma-separated list of files containing Crossbow-style reads.  Can be\n          a mixture of paired and unpaired.  Specify \"-\" for stdin.\n  <s>     Comma-separated list of files containing unpaired reads, or the\n          sequences themselves, if -c is set.  Specify \"-\" for stdin.\n  <hit>   File to write hits to (default: stdout)\nInput:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 query sequences given on cmd line (as <mates>, <singles>)\n  -C                 reads and index are in colorspace\n  -Q/--quals <file>  QV file(s) corresponding to CSFASTA inputs; use with -f -C\n  --Q1/--Q2 <file>   same as -Q, but for mate files 1 and 2 respectively\n  -s/--skip <int>    skip the first <int> reads/pairs in the input\n  -u/--qupto <int>   stop after first <int> reads/pairs (excl. skipped reads)\n  -5/--trim5 <int>   trim <int> bases from 5\\'(left) end of reads\n  -3/--trim3 <int>   trim <int> bases from 3\\'(right) end of reads\n  --phred33-quals    input quals are Phred+33 (default)\n  --phred64-quals    input quals are Phred+64 (same as --solexa1.3-quals)\n  --solexa-quals     input quals are from GA Pipeline ver. < 1.3\n  --solexa1.3-quals  input quals are from GA Pipeline ver. >= 1.3\n  --integer-quals    qualities are given as space-separated integers (not ASCII)\n  --large-index      force usage of a \\'large\\'index, even if a small one is present\nAlignment:\n  -v <int>           report end-to-end hits w/ <=v mismatches; ignore qualities\n    or\n  -n/--seedmms <int> max mismatches in seed (can be 0-3, default: -n 2)\n  -e/--maqerr <int>  max sum of mismatch quals across alignment for -n (def: 70)\n  -l/--seedlen <int> seed length for -n (default: 28)\n  --nomaqround       disable Maq-like quality rounding for -n (nearest 10 <= 30)\n  -I/--minins <int>  minimum insert size for paired-end alignment (default: 0)\n  -X/--maxins <int>  maximum insert size for paired-end alignment (default: 250)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (default: --fr)\n  --nofw/--norc      do not align to forward/reverse-complement reference strand\n  --maxbts <int>     max # backtracks for -n 2/3 (default: 125, 800 for --best)\n  --pairtries <int>  max # attempts to find mate for anchor hit (default: 100)\n  -y/--tryhard       try hard to find valid alignments, at the expense of speed\n  --chunkmbs <int>   max megabytes of RAM for best-first search frames (def: 64)\nReporting:\n  -k <int>           report up to <int> good alignments per read (default: 1)\n  -a/--all           report all alignments per read (much slower than low -k)\n  -m <int>           suppress all alignments if > <int> exist (def: no limit)\n  -M <int>           like -m, but reports 1 random hit (MAPQ=0); requires --best\n  --best             hits guaranteed best stratum; ties broken by quality\n  --strata           hits in sub-optimal strata aren\\'t reported (requires --best)\nOutput:\n  -t/--time          print wall-clock time taken by search phases\n  -B/--offbase <int> leftmost ref offset = <int> in bowtie output (default: 0)\n  --quiet            print nothing but the alignments\n  --refout           write alignments to files refXXXXX.map, 1 map per reference\n  --refidx           refer to ref. seqs by 0-based index rather than name\n  --al <fname>       write aligned reads/pairs to file(s) <fname>\n  --un <fname>       write unaligned reads/pairs to file(s) <fname>\n  --max <fname>      write reads/pairs over -m limit to file(s) <fname>\n  --suppress <cols>  suppresses given columns (comma-delim\\'ed) in default output\n  --fullref          write entire ref name (default: only up to 1st space)\nColorspace:\n  --snpphred <int>   Phred penalty for SNP when decoding colorspace (def: 30)\n     or\n  --snpfrac <dec>    approx. fraction of SNP bases (e.g. 0.001); sets --snpphred\n  --col-cseq         print aligned colorspace seqs as colors, not decoded bases\n  --col-cqual        print original colorspace quals, not decoded quals\n  --col-keepends     keep nucleotides at extreme ends of decoded alignment\nSAM:\n  -S/--sam           write hits in SAM format\n  --mapq <int>       default mapping quality (MAPQ) to print for SAM alignments\n  --sam-nohead       supppress header lines (starting with @) for SAM output\n  --sam-nosq         supppress @SQ header lines for SAM output\n  --sam-RG <text>    add <text> (usually \"lab=value\") to @RG line of SAM header\nPerformance:\n  -o/--offrate <int> override offrate of index; must be >= index\\'s offrate\n  -p/--threads <int> number of alignment threads to launch (default: 1)\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n  --shmem            use shared mem for index; many \\'bowtie\\'s can share\nOther:\n  --seed <int>       seed for random number generator\n  --verbose          verbose output (for debugging)\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.1.1"}, {"role": "assistant", "content":  "Usage: \nbowtie [options]* <ebwt> {-1 <m1> -2 <m2> | --12 <r> | <s>} [<hit>]\n\n  <m1>    Comma-separated list of files containing upstream mates (or the\n          sequences themselves, if -c is set) paired with mates in <m2>\n  <m2>    Comma-separated list of files containing downstream mates (or the\n          sequences themselves if -c is set) paired with mates in <m1>\n  <r>     Comma-separated list of files containing Crossbow-style reads.  Can be\n          a mixture of paired and unpaired.  Specify \"-\" for stdin.\n  <s>     Comma-separated list of files containing unpaired reads, or the\n          sequences themselves, if -c is set.  Specify \"-\" for stdin.\n  <hit>   File to write hits to (default: stdout)\nInput:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 query sequences given on cmd line (as <mates>, <singles>)\n  -C                 reads and index are in colorspace\n  -Q/--quals <file>  QV file(s) corresponding to CSFASTA inputs; use with -f -C\n  --Q1/--Q2 <file>   same as -Q, but for mate files 1 and 2 respectively\n  -s/--skip <int>    skip the first <int> reads/pairs in the input\n  -u/--qupto <int>   stop after first <int> reads/pairs (excl. skipped reads)\n  -5/--trim5 <int>   trim <int> bases from 5\\'(left) end of reads\n  -3/--trim3 <int>   trim <int> bases from 3\\'(right) end of reads\n  --phred33-quals    input quals are Phred+33 (default)\n  --phred64-quals    input quals are Phred+64 (same as --solexa1.3-quals)\n  --solexa-quals     input quals are from GA Pipeline ver. < 1.3\n  --solexa1.3-quals  input quals are from GA Pipeline ver. >= 1.3\n  --integer-quals    qualities are given as space-separated integers (not ASCII)\n  --large-index      force usage of a \\'large\\'index, even if a small one is present\nAlignment:\n  -v <int>           report end-to-end hits w/ <=v mismatches; ignore qualities\n    or\n  -n/--seedmms <int> max mismatches in seed (can be 0-3, default: -n 2)\n  -e/--maqerr <int>  max sum of mismatch quals across alignment for -n (def: 70)\n  -l/--seedlen <int> seed length for -n (default: 28)\n  --nomaqround       disable Maq-like quality rounding for -n (nearest 10 <= 30)\n  -I/--minins <int>  minimum insert size for paired-end alignment (default: 0)\n  -X/--maxins <int>  maximum insert size for paired-end alignment (default: 250)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (default: --fr)\n  --nofw/--norc      do not align to forward/reverse-complement reference strand\n  --maxbts <int>     max # backtracks for -n 2/3 (default: 125, 800 for --best)\n  --pairtries <int>  max # attempts to find mate for anchor hit (default: 100)\n  -y/--tryhard       try hard to find valid alignments, at the expense of speed\n  --chunkmbs <int>   max megabytes of RAM for best-first search frames (def: 64)\nReporting:\n  -k <int>           report up to <int> good alignments per read (default: 1)\n  -a/--all           report all alignments per read (much slower than low -k)\n  -m <int>           suppress all alignments if > <int> exist (def: no limit)\n  -M <int>           like -m, but reports 1 random hit (MAPQ=0); requires --best\n  --best             hits guaranteed best stratum; ties broken by quality\n  --strata           hits in sub-optimal strata aren\\'t reported (requires --best)\nOutput:\n  -t/--time          print wall-clock time taken by search phases\n  -B/--offbase <int> leftmost ref offset = <int> in bowtie output (default: 0)\n  --quiet            print nothing but the alignments\n  --refout           write alignments to files refXXXXX.map, 1 map per reference\n  --refidx           refer to ref. seqs by 0-based index rather than name\n  --al <fname>       write aligned reads/pairs to file(s) <fname>\n  --un <fname>       write unaligned reads/pairs to file(s) <fname>\n  --max <fname>      write reads/pairs over -m limit to file(s) <fname>\n  --suppress <cols>  suppresses given columns (comma-delim\\'ed) in default output\n  --fullref          write entire ref name (default: only up to 1st space)\nColorspace:\n  --snpphred <int>   Phred penalty for SNP when decoding colorspace (def: 30)\n     or\n  --snpfrac <dec>    approx. fraction of SNP bases (e.g. 0.001); sets --snpphred\n  --col-cseq         print aligned colorspace seqs as colors, not decoded bases\n  --col-cqual        print original colorspace quals, not decoded quals\n  --col-keepends     keep nucleotides at extreme ends of decoded alignment\nSAM:\n  -S/--sam           write hits in SAM format\n  --mapq <int>       default mapping quality (MAPQ) to print for SAM alignments\n  --sam-nohead       supppress header lines (starting with @) for SAM output\n  --sam-nosq         supppress @SQ header lines for SAM output\n  --sam-RG <text>    add <text> (usually \"lab=value\") to @RG line of SAM header\nPerformance:\n  -o/--offrate <int> override offrate of index; must be >= index\\'s offrate\n  -p/--threads <int> number of alignment threads to launch (default: 1)\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n  --shmem            use shared mem for index; many \\'bowtie\\'s can share\nOther:\n  --seed <int>       seed for random number generator\n  --verbose          verbose output (for debugging)\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.2.0"}, {"role": "assistant", "content":  "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-v1.2.2dfsg-4-deb"}, {"role": "assistant", "content":  "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.2.3"}, {"role": "assistant", "content":  "usage: bowtie [-h] [-b | -i] [--verbose] [--debug] [--large-index]\n              [--index INDEX]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  -b, --build\n  -i, --inspect\n  --verbose\n  --debug\n  --large-index\n  --index INDEX\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.3.0"}, {"role": "assistant", "content":  "usage: bowtie [-h] [--verbose] [--debug] [--large-index]\n\noptional arguments:\n  -h, --help     show this help message and exit\n  --verbose\n  --debug\n  --large-index\n"}]}
{"messages": [{"role": "user", "content":   "bowtie-1.3.1"}, {"role": "assistant", "content":  "usage: bowtie [-h] [--verbose] [--debug] [--large-index]\n\noptions:\n  -h, --help     show this help message and exit\n  --verbose\n  --debug\n  --large-index\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.4.1"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.4.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.2.5"}, {"role": "assistant", "content":  "Bowtie 2 version 2.2.5 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          supppress SAM records for unaligned reads\n  --no-head          supppress header lines, i.e. lines starting with @\n  --no-sq            supppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.4.3"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.4.3 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.2.4"}, {"role": "assistant", "content":  "Bowtie 2 version 2.2.4 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          supppress SAM records for unaligned reads\n  --no-head          supppress header lines, i.e. lines starting with @\n  --no-sq            supppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.2.1"}, {"role": "assistant", "content":  "Bowtie 2 version 2.2.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-head          supppress header lines, i.e. lines starting with @\n  --no-sq            supppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.0"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.0 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.3.1"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.3.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.4.2"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.4.2 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.4"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.4 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.2"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.2 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-noqname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.2.8"}, {"role": "assistant", "content":  "Bowtie 2 version 2.2.8 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-v2.2.8"}, {"role": "assistant", "content":  "Bowtie 2 version 2.2.8 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-v2.3.0-2-deb"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.0 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --no-dovetail      not concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.5"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.5 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-v2.3.4.3-1-deb"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.4.3 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.3.5.1"}, {"role": "assistant", "content":  "Bowtie 2 version 2.3.5.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.4.1"}, {"role": "assistant", "content":  "Bowtie 2 version  by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc Suppress standard behavior of truncating readname at first whitespace \n                      at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen Exclude soft-clipped bases when reporting TLEN\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.4.2"}, {"role": "assistant", "content":  "Bowtie 2 version 2.4.2 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.4.3"}, {"role": "assistant", "content":  "Bowtie 2 version 2.4.3 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.4.4"}, {"role": "assistant", "content":  "Bowtie 2 version 2.4.4 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.4.5"}, {"role": "assistant", "content":  "Bowtie 2 version 2.4.5 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.5.0"}, {"role": "assistant", "content":  "Bowtie 2 version 2.5.0 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.5.1"}, {"role": "assistant", "content":  "Bowtie 2 version 2.5.1 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.5.2"}, {"role": "assistant", "content":  "Bowtie 2 version 2.5.2 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from a FASTA file <s>\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.5.3"}, {"role": "assistant", "content":  "Bowtie 2 version 2.5.3 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from the FASTA file\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bowtie2-2.5.4"}, {"role": "assistant", "content":  "Bowtie 2 version 2.5.4 by Ben Langmead (langmea@cs.jhu.edu, www.cs.jhu.edu/~langmea)\nUsage: \n  bowtie2 [options]* -x <bt2-idx> {-1 <m1> -2 <m2> | -U <r> | --interleaved <i> | -b <bam>} [-S <sam>]\n\n  <bt2-idx>  Index filename prefix (minus trailing .X.bt2).\n             NOTE: Bowtie 1 and Bowtie 2 indexes are not compatible.\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <i>        Files with interleaved paired-end FASTQ/FASTA reads\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <bam>      Files are unaligned BAM sorted by read name.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --tab5             query input files are TAB5 .tab5\n  --tab6             query input files are TAB6 .tab6\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -F k:<int>,i:<int> query input files are continuous FASTA where reads\n                     are substrings (k-mers) extracted from the FASTA file\n                     and aligned at offsets 1, 1+i, 1+2i ... end of reference\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --trim-to [3:|5:]<int> trim reads exceeding <int> bases from either 3\\'or 5\\'end\n                     If the read end is not specified then it defaults to 3 (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Presets:                 Same as:\n  For --end-to-end:\n   --very-fast            -D 5 -R 1 -N 0 -L 22 -i S,0,2.50\n   --fast                 -D 10 -R 2 -N 0 -L 22 -i S,0,2.50\n   --sensitive            -D 15 -R 2 -N 0 -L 22 -i S,1,1.15 (default)\n   --very-sensitive       -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n  For --local:\n   --very-fast-local      -D 5 -R 1 -N 0 -L 25 -i S,1,2.00\n   --fast-local           -D 10 -R 2 -N 0 -L 22 -i S,1,1.75\n   --sensitive-local      -D 15 -R 2 -N 0 -L 20 -i S,1,0.75 (default)\n   --very-sensitive-local -D 20 -R 3 -N 0 -L 20 -i S,1,0.50\n\n Alignment:\n  -N <int>           max # mismatches in seed alignment; can be 0 or 1 (0)\n  -L <int>           length of seed substrings; must be >3, <32 (22)\n  -i <func>          interval between seed substrings w/r/t read len (S,1,1.15)\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --dpad <int>       include <int> extra ref chars on sides of DP table (15)\n  --gbar <int>       disallow gaps within <int> nucs of read extremes (4)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n  --no-1mm-upfront   do not allow 1 mismatch alignments before attempting to\n                     scan for the optimal seeded alignments\n  --end-to-end       entire read must align; no clipping (on)\n   OR\n  --local            local alignment; ends might be soft clipped (off)\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>         max penalty for mismatch; lower qual = lower penalty (6)\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (G,20,8 for local, L,-0.6,-0.6 for end-to-end)\n\n Reporting:\n  (default)          look for multiple alignments, report best, with MAPQ\n   OR\n  -k <int>           report up to <int> alns per read; MAPQ not meaningful\n   OR\n  -a/--all           report all alignments; very slow, MAPQ not meaningful\n\n Effort:\n  -D <int>           give up extending after <int> failed extends in a row (15)\n  -R <int>           for reads w/ repetitive seeds, try <int> sets of seeds (2)\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0)\n  -X/--maxins <int>  maximum fragment length (500)\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n  --dovetail         concordant when mates extend past each other\n  --no-contain       not concordant when one mate alignment contains other\n  --no-overlap       not concordant when mates overlap at all\n\n BAM:\n  --align-paired-reads\n                     Bowtie2 will, by default, attempt to align unpaired BAM reads.\n                     Use this option to align paired-end reads instead.\n  --preserve-tags    Preserve tags from the original BAM record by\n                     appending them to the end of the corresponding SAM output.\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>        write unpaired reads that didn\\'t align to <path>\n  --al <path>        write unpaired reads that aligned at least once to <path>\n  --un-conc <path>   write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>   write pairs that aligned concordantly at least once to <path>\n    (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n    --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-unal          suppress SAM records for unaligned reads\n  --no-head          suppress header lines, i.e. lines starting with @\n  --no-sq            suppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n  --sam-no-qname-trunc\n                     Suppress standard behavior of truncating readname at first whitespace \n                     at the expense of generating non-standard SAM.\n  --xeq              Use \\'=\\'/\\'X\\', instead of \\'M,\\'to specify matches/mismatches in SAM record.\n  --soft-clipped-unmapped-tlen\n                     Exclude soft-clipped bases when reporting TLEN.\n  --sam-append-comment\n                     Append FASTA/FASTQ comment to SAM record.\n  --sam-opt-config <config>\n                     Use <config>, example \\'-MD,YP,-AS\\', to toggle SAM Optional fields.\n\n Performance:\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'bowtie\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic\n                     seed rand. gen. arbitrarily instead of using read attributes\n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.17"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.8"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.8-r455\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.4"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.4-r385\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'. There are\n      three alignment algorithms in BWA: `mem\\', `bwasw\\'and `aln/samse/sampe\\'. If\n      you are not sure which to use, try `bwa mem\\'first. Please `man ./bwa.1\\'for\n      for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.6.2"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.6.2-r126\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n         fastmap       identify super-maximal exact matches\n\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n         pac2cspac     convert PAC to color-space PAC\n         stdsw         standard SW/NW alignment\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.5.9"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.5.9-r16\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         pac_rev       generate reverse PAC\n         bwt2sa        generate SA from BWT and Occ\n         pac2cspac     convert PAC to color-space PAC\n         stdsw         standard SW/NW alignment\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.3a"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.3a-r367\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.13"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.13-r1126\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.15"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.15-r1140\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-0.7.16"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.16a-r1181\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-v0.7.12"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.12-r1039\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-v0.7.13"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.13-r1126\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "bwa-v0.7.17-3-deb"}, {"role": "assistant", "content":  "\nProgram: bwa (alignment via Burrows-Wheeler transformation)\nVersion: 0.7.17-r1188\nContact: Heng Li <lh3@sanger.ac.uk>\n\nUsage:   bwa <command> [options]\n\nCommand: index         index sequences in the FASTA format\n         mem           BWA-MEM algorithm\n         fastmap       identify super-maximal exact matches\n         pemerge       merge overlapping paired ends (EXPERIMENTAL)\n         aln           gapped/ungapped alignment\n         samse         generate alignment (single ended)\n         sampe         generate alignment (paired ended)\n         bwasw         BWA-SW for long queries\n\n         shm           manage indices in shared memory\n         fa2pac        convert FASTA to PAC format\n         pac2bwt       generate BWT from PAC\n         pac2bwtgen    alternative algorithm for generating BWT\n         bwtupdate     update .bwt to the new format\n         bwt2sa        generate SA from BWT and Occ\n\nNote: To use BWA, you need to first index the genome with `bwa index\\'.\n      There are three alignment algorithms in BWA: `mem\\', `bwasw\\', and\n      `aln/samse/sampe\\'. If you are not sure which to use, try `bwa mem\\'\n      first. Please `man ./bwa.1\\'for the manual.\n\n"}]}
{"messages": [{"role": "user", "content":   "fastqc-v0.11.5"}, {"role": "assistant", "content":  "\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n        fastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n    \n    If no files to process are specified on the command line then the program\n    will start as an interactive graphical application.  If files are provided\n    on the command line then the program will run with no user interaction\n    required.  In this mode it is suitable for inclusion into a standardised\n    analysis pipeline.\n    \n    The options for the program as as follows:\n    \n    -h --help       Print this help file and exit\n    \n    -v --version    Print the version of the program and exit\n    \n    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n                    \n    --casava        Files come from raw casava output. Files in the same sample\n                    group (differing only by the group number) will be analysed\n                    as a set rather than individually. Sequences with the filter\n                    flag set in the header will be excluded from the analysis.\n                    Files must have the same names given to them by casava\n                    (including being gzipped and ending with .gz) otherwise they\n                    won\\'t be grouped together correctly.\n                    \n    --nano          Files come from naopore sequences and are in fast5 format. In\n                    this mode you can pass in directories to process and the program\n                    will take in all fast5 files within those directories and produce\n                    a single output file from the sequences found in all files.                    \n                    \n    --nofilter      If running with --casava then don\\'t remove read flagged by\n                    casava as poor quality when performing the QC analysis.\n                   \n    --extract       If set then the zipped output file will be uncompressed in\n                    the same directory after it has been created.  By default\n                    this option will be set if fastqc is run in non-interactive\n                    mode.\n                    \n    -j --java       Provides the full path to the java binary you want to use to\n                    launch fastqc. If not supplied then java is assumed to be in\n                    your path.\n                   \n    --noextract     Do not uncompress the output file after creating it.  You\n                    should set this option if you do not wish to uncompress\n                    the output when running in non-interactive mode.\n                    \n    --nogroup       Disable grouping of bases for reads >50bp. All reports will\n                    show data for every base in the read.  WARNING: Using this\n                    option will cause fastqc to crash and burn if you use it on\n                    really long reads, and your plots may end up a ridiculous size.\n                    You have been warned!\n                    \n    -f --format     Bypasses the normal sequence file format detection and\n                    forces the program to use the specified format.  Valid\n                    formats are bam,sam,bam_mapped,sam_mapped and fastq\n                    \n    -t --threads    Specifies the number of files which can be processed\n                    simultaneously.  Each thread will be allocated 250MB of\n                    memory so you shouldn\\'t run more threads than your\n                    available memory will cope with, and not more than\n                    6 threads on a 32 bit machine\n                  \n    -c              Specifies a non-default file which contains the list of\n    --contaminants  contaminants to screen overrepresented sequences against.\n                    The file must contain sets of named contaminants in the\n                    form name[tab]sequence.  Lines prefixed with a hash will\n                    be ignored.\n\n    -a              Specifies a non-default file which contains the list of\n    --adapters      adapter sequences which will be explicity searched against\n                    the library. The file must contain sets of named adapters\n                    in the form name[tab]sequence.  Lines prefixed with a hash\n                    will be ignored.\n                    \n    -l              Specifies a non-default file which contains a set of criteria\n    --limits        which will be used to determine the warn/error limits for the\n                    various modules.  This file can also be used to selectively \n                    remove some modules from the output all together.  The format\n                    needs to mirror the default limits.txt file found in the\n                    Configuration folder.\n                    \n   -k --kmers       Specifies the length of Kmer to look for in the Kmer content\n                    module. Specified Kmer length must be between 2 and 10. Default\n                    length is 7 if not specified.\n                    \n   -q --quiet       Supress all progress messages on stdout and only report errors.\n   \n   -d --dir         Selects a directory to be used for temporary files written when\n                    generating report images. Defaults to system temp directory if\n                    not specified.\n                    \nBUGS\n\n    Any bugs in fastqc should be reported either to simon.andrews@babraham.ac.uk\n    or in www.bioinformatics.babraham.ac.uk/bugzilla/"}]}
{"messages": [{"role": "user", "content":   "fastqc-v0.11.8dfsg-2-deb"}, {"role": "assistant", "content":  "\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n\tfastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n    \n    If no files to process are specified on the command line then the program\n    will start as an interactive graphical application.  If files are provided\n    on the command line then the program will run with no user interaction\n    required.  In this mode it is suitable for inclusion into a standardised\n    analysis pipeline.\n    \n    The options for the program as as follows:\n    \n    -h --help       Print this help file and exit\n    \n    -v --version    Print the version of the program and exit\n    \n    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n                    \n    --casava        Files come from raw casava output. Files in the same sample\n                    group (differing only by the group number) will be analysed\n                    as a set rather than individually. Sequences with the filter\n                    flag set in the header will be excluded from the analysis.\n                    Files must have the same names given to them by casava\n                    (including being gzipped and ending with .gz) otherwise they\n                    won\\'t be grouped together correctly.\n                    \n    --nofilter      If running with --casava then don\\'t remove read flagged by\n                    casava as poor quality when performing the QC analysis.\n                   \n    --extract       If set then the zipped output file will be uncompressed in\n                    the same directory after it has been created.  By default\n                    this option will be set if fastqc is run in non-interactive\n                    mode.\n                    \n    -j --java       Provides the full path to the java binary you want to use to\n                    launch fastqc. If not supplied then java is assumed to be in\n                    your path.\n                   \n    --noextract     Do not uncompress the output file after creating it.  You\n                    should set this option if you do not wish to uncompress\n                    the output when running in non-interactive mode.\n                    \n    --nogroup       Disable grouping of bases for reads >50bp. All reports will\n                    show data for every base in the read.  WARNING: Using this\n                    option will cause fastqc to crash and burn if you use it on\n                    really long reads, and your plots may end up a ridiculous size.\n                    You have been warned!\n                    \n    --min_length    Sets an artificial lower limit on the length of the sequence\n                    to be shown in the report.  As long as you set this to a value\n                    greater or equal to your longest read length then this will be\n                    the sequence length used to create your read groups.  This can\n                    be useful for making directly comaparable statistics from \n                    datasets with somewhat variable read lengths.\n                    \n    -f --format     Bypasses the normal sequence file format detection and\n                    forces the program to use the specified format.  Valid\n                    formats are bam,sam,bam_mapped,sam_mapped and fastq\n                    \n    -t --threads    Specifies the number of files which can be processed\n                    simultaneously.  Each thread will be allocated 250MB of\n                    memory so you shouldn\\'t run more threads than your\n                    available memory will cope with, and not more than\n                    6 threads on a 32 bit machine\n                  \n    -c              Specifies a non-default file which contains the list of\n    --contaminants  contaminants to screen overrepresented sequences against.\n                    The file must contain sets of named contaminants in the\n                    form name[tab]sequence.  Lines prefixed with a hash will\n                    be ignored.\n\n    -a              Specifies a non-default file which contains the list of\n    --adapters      adapter sequences which will be explicity searched against\n                    the library. The file must contain sets of named adapters\n                    in the form name[tab]sequence.  Lines prefixed with a hash\n                    will be ignored.\n                    \n    -l              Specifies a non-default file which contains a set of criteria\n    --limits        which will be used to determine the warn/error limits for the\n                    various modules.  This file can also be used to selectively \n                    remove some modules from the output all together.  The format\n                    needs to mirror the default limits.txt file found in the\n                    Configuration folder.\n                    \n   -k --kmers       Specifies the length of Kmer to look for in the Kmer content\n                    module. Specified Kmer length must be between 2 and 10. Default\n                    length is 7 if not specified.\n                    \n   -q --quiet       Supress all progress messages on stdout and only report errors.\n   \n   -d --dir         Selects a directory to be used for temporary files written when\n                    generating report images. Defaults to system temp directory if\n                    not specified.\n                    \nBUGS\n\n    Any bugs in fastqc should be reported either to simon.andrews@babraham.ac.uk\n    or in www.bioinformatics.babraham.ac.uk/bugzilla/\n                   \n    \n"}]}
{"messages": [{"role": "user", "content":   "fastqc-v0.11.9"}, {"role": "assistant", "content":  "\n\n            FastQC - A high throughput sequence QC analysis tool\n\nSYNOPSIS\n\n\tfastqc seqfile1 seqfile2 .. seqfileN\n\n    fastqc [-o output dir] [--(no)extract] [-f fastq|bam|sam] \n           [-c contaminant file] seqfile1 .. seqfileN\n\nDESCRIPTION\n\n    FastQC reads a set of sequence files and produces from each one a quality\n    control report consisting of a number of different modules, each one of \n    which will help to identify a different potential type of problem in your\n    data.\n    \n    If no files to process are specified on the command line then the program\n    will start as an interactive graphical application.  If files are provided\n    on the command line then the program will run with no user interaction\n    required.  In this mode it is suitable for inclusion into a standardised\n    analysis pipeline.\n    \n    The options for the program as as follows:\n    \n    -h --help       Print this help file and exit\n    \n    -v --version    Print the version of the program and exit\n    \n    -o --outdir     Create all output files in the specified output directory.\n                    Please note that this directory must exist as the program\n                    will not create it.  If this option is not set then the \n                    output file for each sequence file is created in the same\n                    directory as the sequence file which was processed.\n                    \n    --casava        Files come from raw casava output. Files in the same sample\n                    group (differing only by the group number) will be analysed\n                    as a set rather than individually. Sequences with the filter\n                    flag set in the header will be excluded from the analysis.\n                    Files must have the same names given to them by casava\n                    (including being gzipped and ending with .gz) otherwise they\n                    won\\'t be grouped together correctly.\n                    \n    --nano          Files come from nanopore sequences and are in fast5 format. In\n                    this mode you can pass in directories to process and the program\n                    will take in all fast5 files within those directories and produce\n                    a single output file from the sequences found in all files.                    \n                    \n    --nofilter      If running with --casava then don\\'t remove read flagged by\n                    casava as poor quality when performing the QC analysis.\n                   \n    --extract       If set then the zipped output file will be uncompressed in\n                    the same directory after it has been created.  By default\n                    this option will be set if fastqc is run in non-interactive\n                    mode.\n                    \n    -j --java       Provides the full path to the java binary you want to use to\n                    launch fastqc. If not supplied then java is assumed to be in\n                    your path.\n                   \n    --noextract     Do not uncompress the output file after creating it.  You\n                    should set this option if you do not wish to uncompress\n                    the output when running in non-interactive mode.\n                    \n    --nogroup       Disable grouping of bases for reads >50bp. All reports will\n                    show data for every base in the read.  WARNING: Using this\n                    option will cause fastqc to crash and burn if you use it on\n                    really long reads, and your plots may end up a ridiculous size.\n                    You have been warned!\n                    \n    --min_length    Sets an artificial lower limit on the length of the sequence\n                    to be shown in the report.  As long as you set this to a value\n                    greater or equal to your longest read length then this will be\n                    the sequence length used to create your read groups.  This can\n                    be useful for making directly comaparable statistics from \n                    datasets with somewhat variable read lengths.\n                    \n    -f --format     Bypasses the normal sequence file format detection and\n                    forces the program to use the specified format.  Valid\n                    formats are bam,sam,bam_mapped,sam_mapped and fastq\n                    \n    -t --threads    Specifies the number of files which can be processed\n                    simultaneously.  Each thread will be allocated 250MB of\n                    memory so you shouldn\\'t run more threads than your\n                    available memory will cope with, and not more than\n                    6 threads on a 32 bit machine\n                  \n    -c              Specifies a non-default file which contains the list of\n    --contaminants  contaminants to screen overrepresented sequences against.\n                    The file must contain sets of named contaminants in the\n                    form name[tab]sequence.  Lines prefixed with a hash will\n                    be ignored.\n\n    -a              Specifies a non-default file which contains the list of\n    --adapters      adapter sequences which will be explicity searched against\n                    the library. The file must contain sets of named adapters\n                    in the form name[tab]sequence.  Lines prefixed with a hash\n                    will be ignored.\n                    \n    -l              Specifies a non-default file which contains a set of criteria\n    --limits        which will be used to determine the warn/error limits for the\n                    various modules.  This file can also be used to selectively \n                    remove some modules from the output all together.  The format\n                    needs to mirror the default limits.txt file found in the\n                    Configuration folder.\n                    \n   -k --kmers       Specifies the length of Kmer to look for in the Kmer content\n                    module. Specified Kmer length must be between 2 and 10. Default\n                    length is 7 if not specified.\n                    \n   -q --quiet       Supress all progress messages on stdout and only report errors.\n   \n   -d --dir         Selects a directory to be used for temporary files written when\n                    generating report images. Defaults to system temp directory if\n                    not specified.\n                    \nBUGS\n\n    Any bugs in fastqc should be reported either to simon.andrews@babraham.ac.uk\n    or in www.bioinformatics.babraham.ac.uk/bugzilla/\n                   \n    \n"}]}
{"messages": [{"role": "user", "content":   "gatk-4.2.6.1"}, {"role": "assistant", "content":  "\nRunning:\n    java -Dsamjdk.use_async_io_read_samtools=false -Dsamjdk.use_async_io_write_samtools=true -Dsamjdk.use_async_io_write_tribble=false -Dsamjdk.compression_level=2 -jar /usr/local/share/gatk4-4.2.6.1-0/gatk-package-4.2.6.1-local.jar -list\nUSAGE:  <program name> [-h]\n\nAvailable Programs:\n--------------------------------------------------------------------------------------\nBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\n    CheckIlluminaDirectory (Picard)              Asserts the validity for specified Illumina basecalling data.  \n    CollectIlluminaBasecallingMetrics (Picard)   Collects Illumina Basecalling metrics for a sequencing run.  \n    CollectIlluminaLaneMetrics (Picard)          Collects Illumina lane metrics for the given BaseCalling analysis directory.\n    ExtractIlluminaBarcodes (Picard)             Tool determines the barcode for each read in an Illumina lane.  \n    IlluminaBasecallsToFastq (Picard)            Generate FASTQ file(s) from Illumina basecall read data.  \n    IlluminaBasecallsToSam (Picard)              Transforms raw Illumina sequencing data into an unmapped SAM, BAM or CRAM file.\n    MarkIlluminaAdapters (Picard)                Reads a SAM/BAM/CRAM file and rewrites it with new adapter-trimming tags.  \n\n--------------------------------------------------------------------------------------\nCopy Number Variant Discovery:                   Tools that analyze read coverage to detect copy number variants.\n    AnnotateIntervals                            Annotates intervals with GC content, mappability, and segmental-duplication content\n    CallCopyRatioSegments                        Calls copy-ratio segments as amplified, deleted, or copy-number neutral\n    CombineSegmentBreakpoints                    (EXPERIMENTAL Tool) Combine the breakpoints of two segment files and annotate the resulting intervals with chosen columns from each file.\n    CreateReadCountPanelOfNormals                Creates a panel of normals for read-count denoising\n    DenoiseReadCounts                            Denoises read counts to produce denoised copy ratios\n    DetermineGermlineContigPloidy                Determines the baseline contig ploidy for germline samples given counts data\n    FilterIntervals                              Filters intervals based on annotations and/or count statistics\n    GermlineCNVCaller                            Calls copy-number variants in germline samples given their counts and the output of DetermineGermlineContigPloidy\n    MergeAnnotatedRegions                        (EXPERIMENTAL Tool) Merge annotated genomic regions based entirely on touching/overlapping intervals.\n    MergeAnnotatedRegionsByAnnotation            (EXPERIMENTAL Tool) Merge annotated genomic regions within specified distance if annotation value(s) are exactly the same.\n    ModelSegments                                Models segmented copy ratios from denoised copy ratios and segmented minor-allele fractions from allelic counts\n    PlotDenoisedCopyRatios                       Creates plots of denoised copy ratios\n    PlotModeledSegments                          Creates plots of denoised and segmented copy-ratio and minor-allele-fraction estimates\n    PostprocessGermlineCNVCalls                  Postprocesses the output of GermlineCNVCaller and generates VCFs and denoised copy ratios\n    TagGermlineEvents                            (EXPERIMENTAL Tool) Do a simplistic tagging of germline events in a tumor segment file.\n\n--------------------------------------------------------------------------------------\nCoverage Analysis:                               Tools that count coverage, e.g. depth per allele\n    ASEReadCounter                               Generates table of filtered base counts at het sites for allele specific expression\n    AnalyzeSaturationMutagenesis                 (BETA Tool) (EXPERIMENTAL) Processes reads from a MITESeq or other saturation mutagenesis experiment.\n    CollectAllelicCounts                         Collects reference and alternate allele counts at specified sites\n    CollectAllelicCountsSpark                    Collects reference and alternate allele counts at specified sites\n    CollectF1R2Counts                            Collect F1R2 read counts for the Mutect2 orientation bias mixture model filter\n    CollectReadCounts                            Collects read counts at specified intervals\n    CountBases                                   Count bases in a SAM/BAM/CRAM file\n    CountBasesSpark                              Counts bases in the input SAM/BAM\n    CountReads                                   Count reads in a SAM/BAM/CRAM file\n    CountReadsSpark                              Counts reads in the input SAM/BAM\n    DepthOfCoverage                              (BETA Tool) Generate coverage summary information for reads data\n    GatherNormalArtifactData                     Combine output files from GetNormalArtifactData in the order defined by a sequence dictionary\n    GeneExpressionEvaluation                     (BETA Tool) Evaluate gene expression from RNA-seq reads aligned to genome.\n    GetNormalArtifactData                        Collects data for training normal artifact filter\n    GetPileupSummaries                           Tabulates pileup metrics for inferring contamination\n    LocalAssembler                               (BETA Tool) Local assembler for SVs\n    Pileup                                       Prints read alignments in samtools pileup format\n    PileupSpark                                  (BETA Tool) Prints read alignments in samtools pileup format\n\n--------------------------------------------------------------------------------------\nDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\n    AccumulateQualityYieldMetrics (Picard)       Combines multiple QualityYieldMetrics files into a single file.\n    AccumulateVariantCallingMetrics (Picard)     Combines multiple Variant Calling Metrics files into a single file\n    AnalyzeCovariates                            Evaluate and compare base quality score recalibration (BQSR) tables\n    BamIndexStats (Picard)                       Generate index statistics from a BAM file\n    CalcMetadataSpark                            (BETA Tool) (Internal) Collects read metrics relevant to structural variant discovery\n    CalculateContamination                       Calculate the fraction of reads coming from cross-sample contamination\n    CalculateFingerprintMetrics (Picard)         Calculate statistics on fingerprints, checking their viability\n    CalculateReadGroupChecksum (Picard)          Creates a hash code based on the read groups (RG).  \n    CheckDuplicateMarking (Picard)               Checks the consistency of duplicate markings.\n    CheckFingerprint (Picard)                    Computes a fingerprint from the supplied input (SAM/BAM/CRAM or VCF) file and compares it to the provided genotypes\n    CheckPileup                                  Compare GATKs internal pileup to a reference Samtools mpileup\n    CheckTerminatorBlock (Picard)                Asserts the provided gzip files (e.g., BAM) last block is well-formed; RC 100 otherwise\n    ClusterCrosscheckMetrics (Picard)            Clusters the results of a CrosscheckFingerprints run by LOD score\n    CollectAlignmentSummaryMetrics (Picard)      <b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \n    CollectArraysVariantCallingMetrics (Picard)  Collects summary and per-sample from the provided arrays VCF file\n    CollectBaseDistributionByCycle (Picard)      Chart the nucleotide distribution per cycle in a SAM or BAM file\n    CollectBaseDistributionByCycleSpark          (BETA Tool) Collects base distribution per cycle in SAM/BAM/CRAM file(s).\n    CollectGcBiasMetrics (Picard)                Collect metrics regarding GC bias. \n    CollectHiSeqXPfFailMetrics (Picard)          Classify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\n    CollectHsMetrics (Picard)                    Collects hybrid-selection (HS) metrics for a SAM or BAM file.  \n    CollectIndependentReplicateMetrics (Picard)  (EXPERIMENTAL Tool) Estimates the rate of independent replication rate of reads within a bam. \n\n    CollectInsertSizeMetrics (Picard)            Collect metrics about the insert size distribution of a paired-end library. \n    CollectInsertSizeMetricsSpark                (BETA Tool) Collects insert size distribution information on alignment data\n    CollectJumpingLibraryMetrics (Picard)        Collect jumping library metrics. \n    CollectMultipleMetrics (Picard)              Collect multiple classes of metrics. \n    CollectMultipleMetricsSpark                  (BETA Tool) Runs multiple metrics collection modules for a given alignment file\n    CollectOxoGMetrics (Picard)                  Collect metrics to assess oxidative artifacts.\n    CollectQualityYieldMetrics (Picard)          Collect metrics about reads that pass quality thresholds and Illumina-specific filters.  \n    CollectQualityYieldMetricsSpark              (BETA Tool) Collects quality yield metrics from SAM/BAM/CRAM file(s).\n    CollectRawWgsMetrics (Picard)                Collect whole genome sequencing-related metrics.  \n    CollectRnaSeqMetrics (Picard)                Produces RNA alignment metrics for a SAM or BAM file.  \n    CollectRrbsMetrics (Picard)                  <b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \n    CollectSamErrorMetrics (Picard)              Program to collect error metrics on bases stratified in various ways.\n    CollectSequencingArtifactMetrics (Picard)    Collect metrics to quantify single-base sequencing artifacts.  \n    CollectTargetedPcrMetrics (Picard)           Calculate PCR-related metrics from targeted sequencing data. \n    CollectVariantCallingMetrics (Picard)        Collects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\n    CollectWgsMetrics (Picard)                   Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\n    CollectWgsMetricsWithNonZeroCoverage (Picard)(EXPERIMENTAL Tool) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \n    CompareBaseQualities                         Compares the base qualities of two SAM/BAM/CRAM files\n    CompareDuplicatesSpark                       (BETA Tool) Determine if two potentially identical BAMs have the same duplicate reads\n    CompareMetrics (Picard)                      Compare two metrics files.\n    CompareSAMs (Picard)                         Compare two input SAM/BAM/CRAM files.  \n    ConvertHaplotypeDatabaseToVcf (Picard)       Convert Haplotype database file to vcf\n    ConvertSequencingArtifactToOxoG (Picard)     Extract OxoG metrics from generalized artifacts metrics.  \n    CrosscheckFingerprints (Picard)              Checks that all data in the input files appear to have come from the same individual\n    CrosscheckReadGroupFingerprints (Picard)     DEPRECATED: USE CrosscheckFingerprints. \n    EstimateLibraryComplexity (Picard)           Estimates the numbers of unique molecules in a sequencing library.  \n    ExtractFingerprint (Picard)                  Computes a fingerprint from the input file.\n    FlagStat                                     Accumulate flag statistics given a BAM file\n    FlagStatSpark                                Spark tool to accumulate flag statistics\n    GatherPileupSummaries                        Combine output files from GetPileupSummary in the order defined by a sequence dictionary\n    GetSampleName                                Emit a single sample name\n    IdentifyContaminant (Picard)                 Computes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\n    LiftOverHaplotypeMap (Picard)                Lifts over a haplotype database from one reference to another\n    MeanQualityByCycle (Picard)                  Collect mean quality by cycle.\n    MeanQualityByCycleSpark                      (BETA Tool) MeanQualityByCycle on Spark\n    QualityScoreDistribution (Picard)            Chart the distribution of quality scores.  \n    QualityScoreDistributionSpark                (BETA Tool) QualityScoreDistribution on Spark\n    ValidateSamFile (Picard)                     Validates a SAM/BAM/CRAM file.\n    ViewSam (Picard)                             Prints a SAM or BAM file to the screen\n\n--------------------------------------------------------------------------------------\nExample Tools:                                   Example tools that show developers how to implement new tools\n    ExampleMultiFeatureWalker                    Example of a MultiFeatureWalker subclass.\n    HtsgetReader                                 (EXPERIMENTAL Tool) Download a file using htsget\n\n--------------------------------------------------------------------------------------\nGenotyping Arrays Manipulation:                  Tools that manipulate data generated by Genotyping arrays\n    BpmToNormalizationManifestCsv (Picard)       Program to convert an Illumina bpm file into a bpm.csv file.\n    CombineGenotypingArrayVcfs (Picard)          Program to combine multiple genotyping array VCF files into one VCF.\n    CompareGtcFiles (Picard)                     Compares two GTC files.\n    CreateBafRegressMetricsFile (Picard)         Program to generate a picard metrics file from the output of the bafRegress tool.\n    CreateExtendedIlluminaManifest (Picard)      Create an Extended Illumina Manifest for usage by the Picard tool GtcToVcf\n    CreateVerifyIDIntensityContaminationMetricsFile (Picard)    Program to generate a picard metrics file from the output of the VerifyIDIntensity tool.\n    GtcToVcf (Picard)                            Program to convert an Illumina GTC file to a VCF\n    MergePedIntoVcf (Picard)                     Program to merge a single-sample ped file from zCall into a single-sample VCF.\n    VcfToAdpc (Picard)                           Program to convert an Arrays VCF to an ADPC file.\n\n--------------------------------------------------------------------------------------\nIntervals Manipulation:                          Tools that process genomic intervals in various formats\n    BedToIntervalList (Picard)                   Converts a BED file to a Picard Interval List.  \n    CompareIntervalLists                         Compare two interval lists for equality\n    IntervalListToBed (Picard)                   Converts an Picard IntervalList file to a BED file.\n    IntervalListTools (Picard)                   A tool for performing various IntervalList manipulations\n    LiftOverIntervalList (Picard)                Lifts over an interval list from one reference build to another. \n    PreprocessIntervals                          Prepares bins for coverage collection\n    SplitIntervals                               Split intervals into sub-interval files.\n\n--------------------------------------------------------------------------------------\nMetagenomics:                                    Tools that perform metagenomic analysis, e.g. microbial community composition and pathogen detection\n    PathSeqBuildKmers                            Builds set of host reference k-mers\n    PathSeqBuildReferenceTaxonomy                Builds a taxonomy datafile of the microbe reference\n    PathSeqBwaSpark                              Step 2: Aligns reads to the microbe reference\n    PathSeqFilterSpark                           Step 1: Filters low quality, low complexity, duplicate, and host reads\n    PathSeqPipelineSpark                         Combined tool that performs all steps: read filtering, microbe reference alignment, and abundance scoring\n    PathSeqScoreSpark                            Step 3: Classifies pathogen-aligned reads and generates abundance scores\n\n--------------------------------------------------------------------------------------\nMethylation-Specific Tools:                      Tools that perform methylation calling, processing bisulfite sequenced, methylation-aware aligned BAM\n    MethylationTypeCaller                        (EXPERIMENTAL Tool) Identify methylated bases from bisulfite sequenced, methylation-aware BAMs\n\n--------------------------------------------------------------------------------------\nOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\n    CreateHadoopBamSplittingIndex                (BETA Tool) Create a Hadoop BAM splitting index\n    FifoBuffer (Picard)                          Provides a large, FIFO buffer that can be used to buffer input and output streams between programs.\n    GatherBQSRReports                            Gathers scattered BQSR recalibration reports into a single file\n    GatherTranches                               (BETA Tool) Gathers scattered VQSLOD tranches into a single file\n    IndexFeatureFile                             Creates an index for a feature file, e.g. VCF or BED file.\n    ParallelCopyGCSDirectoryIntoHDFSSpark        (BETA Tool) Parallel copy a file or directory from Google Cloud Storage into the HDFS file system used by Spark\n    PrintBGZFBlockInformation                    (EXPERIMENTAL Tool) Print information about the compressed blocks in a BGZF format file\n    ReadAnonymizer                               (EXPERIMENTAL Tool) Replace bases in reads with reference bases.\n    ReblockGVCF                                  Condenses homRef blocks in a single-sample GVCF\n    SortGff (Picard)                             Sorts a gff3 file, and adds flush directives\n\n--------------------------------------------------------------------------------------\nRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\n    AddCommentsToBam (Picard)                    Adds comments to the header of a BAM file.\n    AddOATag (Picard)                            Record current alignment information to OA tag.\n    AddOrReplaceReadGroups (Picard)              Assigns all the reads in a file to a single new read-group.\n    AddOriginalAlignmentTags                     (EXPERIMENTAL Tool) Adds Original Alignment tag and original mate contig tag\n    ApplyBQSR                                    Apply base quality score recalibration\n    ApplyBQSRSpark                               (BETA Tool) Apply base quality score recalibration on Spark\n    BQSRPipelineSpark                            (BETA Tool) Both steps of BQSR (BaseRecalibrator and ApplyBQSR) on Spark\n    BamToBfq (Picard)                            Converts a BAM file into a BFQ (binary fastq formatted) file\n    BaseRecalibrator                             Generates recalibration table for Base Quality Score Recalibration (BQSR)\n    BaseRecalibratorSpark                        (BETA Tool) Generate recalibration table for Base Quality Score Recalibration (BQSR) on Spark\n    BuildBamIndex (Picard)                       Generates a BAM index .bai file.  \n    BwaAndMarkDuplicatesPipelineSpark            (BETA Tool) Takes name-sorted file and runs BWA and MarkDuplicates.\n    BwaSpark                                     (BETA Tool) Align reads to a given reference using BWA on Spark\n    CleanSam (Picard)                            Cleans a SAM/BAM/CRAM files, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\n    ClipReads                                    Clip reads in a SAM/BAM/CRAM file\n    CollectDuplicateMetrics (Picard)             Collect Duplicate metrics from marked file.\n    ConvertHeaderlessHadoopBamShardToBam         (BETA Tool) Convert a headerless BAM shard into a readable BAM\n    DownsampleByDuplicateSet                     (BETA Tool) Discard a set fraction of duplicate sets from a UMI-grouped bam\n    DownsampleSam (Picard)                       Downsample a SAM or BAM file.\n    ExtractOriginalAlignmentRecordsByNameSpark   (BETA Tool) Subsets reads by name\n    FastqToSam (Picard)                          Converts a FASTQ file to an unaligned BAM or SAM file\n    FilterSamReads (Picard)                      Subsets reads from a SAM/BAM/CRAM file by applying one of several filters.\n    FixMateInformation (Picard)                  Verify mate-pair information between mates and fix if needed.\n    FixMisencodedBaseQualityReads                Fix Illumina base quality scores in a SAM/BAM/CRAM file\n    GatherBamFiles (Picard)                      Concatenate efficiently BAM files that resulted from a scattered parallel analysis\n    LeftAlignIndels                              Left-aligns indels from reads in a SAM/BAM/CRAM file\n    MarkDuplicates (Picard)                      Identifies duplicate reads.  \n    MarkDuplicatesSpark                          MarkDuplicates on Spark\n    MarkDuplicatesWithMateCigar (Picard)         Identifies duplicate reads, accounting for mate CIGAR.  \n    MergeBamAlignment (Picard)                   Merge alignment data from a SAM or BAM with data in an unmapped BAM file.  \n    MergeSamFiles (Picard)                       Merges multiple SAM/BAM/CRAM (and/or) files into a single file.  \n    PositionBasedDownsampleSam (Picard)          Downsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\n    PostProcessReadsForRSEM                      (BETA Tool) Reorder reads before running RSEM\n    PrintDistantMates                            Unmaps reads with distant mates.\n    PrintReads                                   Print reads in the SAM/BAM/CRAM file\n    PrintReadsHeader                             Print the header from a SAM/BAM/CRAM file\n    PrintReadsSpark                              PrintReads on Spark\n    ReorderSam (Picard)                          Reorders reads in a SAM or BAM file to match ordering in a second reference file.\n    ReplaceSamHeader (Picard)                    Replaces the SAMFileHeader in a SAM/BAM/CRAM file.  \n    RevertBaseQualityScores                      Revert Quality Scores in a SAM/BAM/CRAM file\n    RevertOriginalBaseQualitiesAndAddMateCigar (Picard)Reverts the original base qualities and adds the mate cigar tag to read-group files\n    RevertSam (Picard)                           Reverts SAM/BAM/CRAM files to a previous state.  \n    RevertSamSpark                               (BETA Tool) Reverts SAM, BAM or CRAM files to a previous state.\n    SamFormatConverter (Picard)                  Convert a BAM file to a SAM file, or a SAM to a BAM\n    SamToFastq (Picard)                          Converts a SAM/BAM/CRAM file to FASTQ.\n    SamToFastqWithTags (Picard)                  Converts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\n    SetNmAndUqTags (Picard)                      DEPRECATED: Use SetNmMdAndUqTags instead.\n    SetNmMdAndUqTags (Picard)                    Fixes the NM, MD, and UQ tags in a SAM/BAM/CRAM file \n    SimpleMarkDuplicatesWithMateCigar (Picard)   (EXPERIMENTAL Tool) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\n    SortSam (Picard)                             Sorts a SAM, BAM or CRAM file.  \n    SortSamSpark                                 (BETA Tool) SortSam on Spark (works on SAM/BAM/CRAM)\n    SplitNCigarReads                             Split Reads with N in Cigar\n    SplitReads                                   Outputs reads from a SAM/BAM/CRAM by read group, sample and library name\n    SplitSamByLibrary (Picard)                   Splits a SAM/BAM/CRAM file into individual files by library\n    SplitSamByNumberOfReads (Picard)             Splits a SAM/BAM/CRAM file to multiple files.\n    TransferReadTags                             (EXPERIMENTAL Tool) Incorporate read tags in a SAM file to that of a matching SAM file\n    UmiAwareMarkDuplicatesWithMateCigar (Picard) (EXPERIMENTAL Tool) Identifies duplicate reads using information from read positions and UMIs. \n    UnmarkDuplicates                             Clears the 0x400 duplicate SAM flag\n\n--------------------------------------------------------------------------------------\nReference:                                       Tools that analyze and manipulate FASTA format references\n    BaitDesigner (Picard)                        Designs oligonucleotide baits for hybrid selection reactions.\n    BwaMemIndexImageCreator                      Create a BWA-MEM index image file for use with GATK BWA tools\n    ComposeSTRTableFile                          Composes a genome-wide STR location table used for DragSTR model auto-calibration\n    CountBasesInReference                        Count the numbers of each base in a reference file\n    CreateSequenceDictionary (Picard)            Creates a sequence dictionary for a reference sequence.  \n    ExtractSequences (Picard)                    Subsets intervals from a reference sequence to a new FASTA file.\n    FastaAlternateReferenceMaker                 Create an alternative reference by combining a fasta with a vcf.\n    FastaReferenceMaker                          Create snippets of a fasta file\n    FindBadGenomicKmersSpark                     (BETA Tool) Identifies sequences that occur at high frequency in a reference\n    NonNFastaSize (Picard)                       Counts the number of non-N bases in a fasta file.\n    NormalizeFasta (Picard)                      Normalizes lines of sequence in a FASTA file to be of the same length.\n    ScatterIntervalsByNs (Picard)                Writes an interval list created by splitting a reference at Ns.\n    ShiftFasta                                   (BETA Tool) Creates a shifted fasta file and shift_back file\n\n--------------------------------------------------------------------------------------\nShort Variant Discovery:                         Tools that perform variant calling and genotyping for short variants (SNPs, SNVs and Indels)\n    CalibrateDragstrModel                        estimates the parameters for the DRAGstr model\n    CombineGVCFs                                 Merges one or more HaplotypeCaller GVCF files into a single GVCF with appropriate annotations\n    GenomicsDBImport                             Import VCFs to GenomicsDB\n    GenotypeGVCFs                                Perform joint genotyping on one or more samples pre-called with HaplotypeCaller\n    GnarlyGenotyper                              (BETA Tool) Perform quick and dirty joint genotyping on one or more samples pre-called with HaplotypeCaller\n    HaplotypeCaller                              Call germline SNPs and indels via local re-assembly of haplotypes\n    HaplotypeCallerSpark                         (BETA Tool) HaplotypeCaller on Spark\n    LearnReadOrientationModel                    Get the maximum likelihood estimates of artifact prior probabilities in the orientation bias mixture model filter\n    MergeMutectStats                             Merge the stats output by scatters of a single Mutect2 job\n    Mutect2                                      Call somatic SNVs and indels via local assembly of haplotypes\n    ReadsPipelineSpark                           (BETA Tool) Runs BWA (if specified), MarkDuplicates, BQSR, and HaplotypeCaller on unaligned or aligned reads to generate a VCF.\n\n--------------------------------------------------------------------------------------\nStructural Variant Discovery:                    Tools that detect structural variants        \n    CollectSVEvidence                            (BETA Tool) Gathers paired-end and split read evidence files for use in the GATK-SV pipeline.\n    CpxVariantReInterpreterSpark                 (BETA Tool) (Internal) Tries to extract simple variants from a provided GATK-SV CPX.vcf\n    DiscoverVariantsFromContigAlignmentsSAMSpark (BETA Tool) (Internal) Examines aligned contigs from local assemblies and calls structural variants\n    ExtractSVEvidenceSpark                       (BETA Tool) (Internal) Extracts evidence of structural variations from reads\n    FindBreakpointEvidenceSpark                  (BETA Tool) (Internal) Produces local assemblies of genomic regions that may harbor structural variants\n    JointGermlineCNVSegmentation                 (BETA Tool) Combine segmented gCNV VCFs.\n    PrintSVEvidence                              (EXPERIMENTAL Tool) Merges SV evidence records.\n    SVAnnotate                                   Adds gene overlap and variant consequence annotations to SV VCF from GATK-SV pipeline\n    SVCluster                                    (BETA Tool) Clusters structural variants\n    StructuralVariantDiscoverer                  (BETA Tool) (Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\n    StructuralVariationDiscoveryPipelineSpark    (BETA Tool) Runs the structural variation discovery workflow on a single sample\n    SvDiscoverFromLocalAssemblyContigAlignmentsSpark    (BETA Tool) (Internal) Examines aligned contigs from local assemblies and calls structural variants or their breakpoints\n\n--------------------------------------------------------------------------------------\nVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\n    AlleleFrequencyQC                            (BETA Tool) General-purpose tool for variant evaluation (% in dbSNP, genotype concordance, Ti/Tv ratios, and a lot more)\n    AnnotateVcfWithBamDepth                      (Internal) Annotate a vcf with a bams read depth at each variant locus\n    AnnotateVcfWithExpectedAlleleFraction        (Internal) Annotate a vcf with expected allele fractions in pooled sequencing\n    CalculateGenotypePosteriors                  Calculate genotype posterior probabilities given family and/or known population genotypes\n    CalculateMixingFractions                     (Internal) Calculate proportions of different samples in a pooled bam\n    Concordance                                  Evaluate concordance of an input VCF against a validated truth VCF\n    CountFalsePositives                          (BETA Tool) Count PASS variants\n    CountVariants                                Counts variant records in a VCF file, regardless of filter status.\n    CountVariantsSpark                           CountVariants on Spark\n    EvaluateInfoFieldConcordance                 (BETA Tool) Evaluate concordance of info fields in an input VCF against a validated truth VCF\n    FilterFuncotations                           (EXPERIMENTAL Tool) Filter variants based on clinically-significant Funcotations.\n    FindMendelianViolations (Picard)             Finds mendelian violations of all types within a VCF\n    FuncotateSegments                            (BETA Tool) Functional annotation for segment files.  The output formats are not well-defined and subject to change.\n    Funcotator                                   Functional Annotator\n    FuncotatorDataSourceDownloader               Data source downloader for Funcotator.\n    GenotypeConcordance (Picard)                 Calculates the concordance between genotype data of one sample in each of two VCFs - truth (or reference) vs. calls.\n    MergeMutect2CallsWithMC3                     (EXPERIMENTAL Tool) UNSUPPORTED.  FOR EVALUATION ONLY. Merge M2 calls with MC\n    ReferenceBlockConcordance                    Evaluate GVCF reference block concordance of an input GVCF against a truth GVCF\n    ValidateBasicSomaticShortMutations           (EXPERIMENTAL Tool) Check variants against tumor-normal bams representing the same samples, though not the ones from the actual calls.\n    ValidateVariants                             Validate VCF\n    VariantEval                                  (BETA Tool) General-purpose tool for variant evaluation (% in dbSNP, genotype concordance, Ti/Tv ratios, and a lot more)\n    VariantsToTable                              Extract fields from a VCF file to a tab-delimited table\n\n--------------------------------------------------------------------------------------\nVariant Filtering:                               Tools that filter variants by annotating the FILTER column\n    ApplyVQSR                                     Apply a score cutoff to filter variants based on a recalibration table\n    CNNScoreVariants                             Apply a Convolutional Neural Net to filter annotated variants\n    CNNVariantTrain                              (EXPERIMENTAL Tool) Train a CNN model for filtering variants\n    CNNVariantWriteTensors                       (EXPERIMENTAL Tool) Write variant tensors for training a CNN to filter variants\n    CreateSomaticPanelOfNormals                  (BETA Tool) Make a panel of normals for use with Mutect2\n    FilterAlignmentArtifacts                     (EXPERIMENTAL Tool) Filter alignment artifacts from a vcf callset.\n    FilterMutectCalls                            Filter somatic SNVs and indels called by Mutect2\n    FilterVariantTranches                        Apply tranche filtering\n    FilterVcf (Picard)                           Hard filters a VCF.\n    MTLowHeteroplasmyFilterTool                  If too many low het sites, filter all low het sites\n    NuMTFilterTool                               Uses the median autosomal coverage and the allele depth to determine whether the allele might be a NuMT\n    VariantFiltration                            Filter variant calls based on INFO and/or FORMAT annotations\n    VariantRecalibrator                          Build a recalibration model to score variant quality for filtering purposes\n\n--------------------------------------------------------------------------------------\nVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\n    FixVcfHeader (Picard)                        Replaces or fixes a VCF header.\n    GatherVcfs (Picard)                          Gathers multiple VCF files from a scatter operation into a single VCF file\n    GatherVcfsCloud                              (BETA Tool) Gathers multiple VCF files from a scatter operation into a single VCF file\n    LeftAlignAndTrimVariants                     Left align and trim vairants\n    LiftoverVcf (Picard)                         Lifts over a VCF file from one reference build to another.  \n    MakeSitesOnlyVcf (Picard)                    Creates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\n    MakeVcfSampleNameMap (Picard)                Creates a TSV from sample name to VCF/GVCF path, with one line per input.\n    MergeVcfs (Picard)                           Combines multiple variant files into a single variant file\n    PrintVariantsSpark                           Prints out variants from the input VCF.\n    RemoveNearbyIndels                           (Internal) Remove indels from the VCF file that are close to each other.\n    RenameSampleInVcf (Picard)                   Renames a sample within a VCF or BCF.\n    SelectVariants                               Select a subset of variants from a VCF file\n    SortVcf (Picard)                             Sorts one or more VCF files.  \n    SplitVcfs (Picard)                           Splits SNPs and INDELs into separate files.  \n    UpdateVCFSequenceDictionary                  Updates the sequence dictionary in a variant file.\n    UpdateVcfSequenceDictionary (Picard)         Takes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\n    VariantAnnotator                             Tool for adding annotations to VCF files\n    VcfFormatConverter (Picard)                  Converts VCF to BCF or BCF to VCF.  \n    VcfToIntervalList (Picard)                   Converts a VCF or BCF file to a Picard Interval List\n\n--------------------------------------------------------------------------------------\n\n***********************************************************************\n"}]}
{"messages": [{"role": "user", "content":   "hisat2-v2.0.5-1-deb"}, {"role": "assistant", "content":  "\nHISAT2 version 2.0.5 by Daehwan Kim (infphilo@gmail.com, www.ccb.jhu.edu/people/infphilo)\nUsage: \n  hisat2 [options]* -x <ht2-idx> {-1 <m1> -2 <m2> | -U <r> | --sra-acc <SRA accession number>} [-S <sam>]\n\n  <ht2-idx>  Index filename prefix (minus trailing .X.ht2).\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <SRA accession number>        Comma-separated list of SRA accession numbers, e.g. --sra-acc SRR353653,SRR353654.\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n  --sra-acc          SRA accession ID\n\n Alignment:\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n\n Spliced Alignment:\n  --pen-cansplice <int>              penalty for a canonical splice site (0)\n  --pen-noncansplice <int>           penalty for a non-canonical splice site (12)\n  --pen-canintronlen <func>          penalty for long introns (G,-8,1) with canonical splice sites\n  --pen-noncanintronlen <func>       penalty for long introns (G,-8,1) with noncanonical splice sites\n  --min-intronlen <int>              minimum intron length (20)\n  --max-intronlen <int>              maximum intron length (500000)\n  --known-splicesite-infile <path>   provide a list of known splice sites\n  --novel-splicesite-outfile <path>  report a list of splice sites\n  --novel-splicesite-infile <path>   provide a list of novel splice sites\n  --no-temp-splicesite               disable the use of splice sites found\n  --no-spliced-alignment             disable spliced alignment\n  --rna-strandness <string>          Specify strand-specific information (unstranded)\n  --tmo                              Reports only those alignments within known transcriptome\n  --dta                              Reports alignments tailored for transcript assemblers\n  --dta-cufflinks                    Reports alignments tailored specifically for cufflinks\n\n Scoring:\n  --ma <int>         match bonus (0 for --end-to-end, 2 for --local) \n  --mp <int>,<int>   max and min penalties for mismatch; lower qual = lower penalty <2,6>\n  --sp <int>,<int>   max and min penalties for soft-clipping; lower qual = lower penalty <1,2>\n  --no-softclip      no soft-clipping\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (L,0.0,-0.2)\n\n Reporting:\n  -k <int> (default: 5) report up to <int> alns per read\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0), only valid with --no-spliced-alignment\n  -X/--maxins <int>  maximum fragment length (500), only valid with --no-spliced-alignment\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-head          supppress header lines, i.e. lines starting with @\n  --no-sq            supppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -o/--offrate <int> override offrate of index; must be >= index\\'s offrate\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'hisat2\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --remove-chrname   remove \\'chr\\'from reference names in alignment\n  --add-chrname      add \\'chr\\'to reference names in alignment \n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "hisat2-v2.1.0-2-deb"}, {"role": "assistant", "content":  "\nHISAT2 version 2.1.0 by Daehwan Kim (infphilo@gmail.com, www.ccb.jhu.edu/people/infphilo)\nUsage: \n  hisat2 [options]* -x <ht2-idx> {-1 <m1> -2 <m2> | -U <r>} [-S <sam>]\n\n  <ht2-idx>  Index filename prefix (minus trailing .X.ht2).\n  <m1>       Files with #1 mates, paired with files in <m2>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <m2>       Files with #2 mates, paired with files in <m1>.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <r>        Files with unpaired reads.\n             Could be gzip\\'ed (extension: .gz) or bzip2\\'ed (extension: .bz2).\n  <sam>      File for SAM output (default: stdout)\n\n  <m1>, <m2>, <r> can be comma-separated lists (no whitespace) and can be\n  specified many times.  E.g. \\'-U file1.fq,file2.fq -U file3.fq\\'.\n\nOptions (defaults in parentheses):\n\n Input:\n  -q                 query input files are FASTQ .fq/.fastq (default)\n  --qseq             query input files are in Illumina\\'s qseq format\n  -f                 query input files are (multi-)FASTA .fa/.mfa\n  -r                 query input files are raw one-sequence-per-line\n  -c                 <m1>, <m2>, <r> are sequences themselves, not files\n  -s/--skip <int>    skip the first <int> reads/pairs in the input (none)\n  -u/--upto <int>    stop after first <int> reads/pairs (no limit)\n  -5/--trim5 <int>   trim <int> bases from 5\\'/left end of reads (0)\n  -3/--trim3 <int>   trim <int> bases from 3\\'/right end of reads (0)\n  --phred33          qualities are Phred+33 (default)\n  --phred64          qualities are Phred+64\n  --int-quals        qualities encoded as space-delimited integers\n\n Alignment:\n  --n-ceil <func>    func for max # non-A/C/G/Ts permitted in aln (L,0,0.15)\n  --ignore-quals     treat all quality values as 30 on Phred scale (off)\n  --nofw             do not align forward (original) version of read (off)\n  --norc             do not align reverse-complement version of read (off)\n\n Spliced Alignment:\n  --pen-cansplice <int>              penalty for a canonical splice site (0)\n  --pen-noncansplice <int>           penalty for a non-canonical splice site (12)\n  --pen-canintronlen <func>          penalty for long introns (G,-8,1) with canonical splice sites\n  --pen-noncanintronlen <func>       penalty for long introns (G,-8,1) with noncanonical splice sites\n  --min-intronlen <int>              minimum intron length (20)\n  --max-intronlen <int>              maximum intron length (500000)\n  --known-splicesite-infile <path>   provide a list of known splice sites\n  --novel-splicesite-outfile <path>  report a list of splice sites\n  --novel-splicesite-infile <path>   provide a list of novel splice sites\n  --no-temp-splicesite               disable the use of splice sites found\n  --no-spliced-alignment             disable spliced alignment\n  --rna-strandness <string>          specify strand-specific information (unstranded)\n  --tmo                              reports only those alignments within known transcriptome\n  --dta                              reports alignments tailored for transcript assemblers\n  --dta-cufflinks                    reports alignments tailored specifically for cufflinks\n  --avoid-pseudogene                 tries to avoid aligning reads to pseudogenes (experimental option)\\x13\n  --no-templatelen-adjustment        disables template length adjustment for RNA-seq reads\n\n Scoring:\n  --mp <int>,<int>   max and min penalties for mismatch; lower qual = lower penalty <6,2>\n  --sp <int>,<int>   max and min penalties for soft-clipping; lower qual = lower penalty <2,1>\n  --no-softclip      no soft-clipping\n  --np <int>         penalty for non-A/C/G/Ts in read/ref (1)\n  --rdg <int>,<int>  read gap open, extend penalties (5,3)\n  --rfg <int>,<int>  reference gap open, extend penalties (5,3)\n  --score-min <func> min acceptable alignment score w/r/t read length\n                     (L,0.0,-0.2)\n\n Reporting:\n  -k <int> (default: 5) report up to <int> alns per read\n\n Paired-end:\n  -I/--minins <int>  minimum fragment length (0), only valid with --no-spliced-alignment\n  -X/--maxins <int>  maximum fragment length (500), only valid with --no-spliced-alignment\n  --fr/--rf/--ff     -1, -2 mates align fw/rev, rev/fw, fw/fw (--fr)\n  --no-mixed         suppress unpaired alignments for paired reads\n  --no-discordant    suppress discordant alignments for paired reads\n\n Output:\n  -t/--time          print wall-clock time taken by search phases\n  --un <path>           write unpaired reads that didn\\'t align to <path>\n  --al <path>           write unpaired reads that aligned at least once to <path>\n  --un-conc <path>      write pairs that didn\\'t align concordantly to <path>\n  --al-conc <path>      write pairs that aligned concordantly at least once to <path>\n  (Note: for --un, --al, --un-conc, or --al-conc, add \\'-gz\\'to the option name, e.g.\n  --un-gz <path>, to gzip compress output, or add \\'-bz2\\'to bzip2 compress output.)\n  --summary-file     print alignment summary to this file.\n  --new-summary      print alignment summary in a new style, which is more machine-friendly.\n  --quiet            print nothing to stderr except serious errors\n  --met-file <path>  send metrics to file at <path> (off)\n  --met-stderr       send metrics to stderr (off)\n  --met <int>        report internal counters & metrics every <int> secs (1)\n  --no-head          supppress header lines, i.e. lines starting with @\n  --no-sq            supppress @SQ header lines\n  --rg-id <text>     set read group id, reflected in @RG line and RG:Z: opt field\n  --rg <text>        add <text> (\"lab:value\") to @RG line of SAM header.\n                     Note: @RG line only printed when --rg-id is set.\n  --omit-sec-seq     put \\'*\\'in SEQ and QUAL fields for secondary alignments.\n\n Performance:\n  -o/--offrate <int> override offrate of index; must be >= index\\'s offrate\n  -p/--threads <int> number of alignment threads to launch (1)\n  --reorder          force SAM output order to match order of input reads\n  --mm               use memory-mapped I/O for index; many \\'hisat2\\'s can share\n\n Other:\n  --qc-filter        filter out reads that are bad according to QSEQ filter\n  --seed <int>       seed for random number generator (0)\n  --non-deterministic seed rand. gen. arbitrarily instead of using read attributes\n  --remove-chrname   remove \\'chr\\'from reference names in alignment\n  --add-chrname      add \\'chr\\'to reference names in alignment \n  --version          print version information and quit\n  -h/--help          print this usage message\n"}]}
{"messages": [{"role": "user", "content":   "hmmer-2.3.2"}, {"role": "assistant", "content":  "\nhmmalign - align sequences to an HMM profile\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmalign [-options] <hmm file> <sequence file>\nAvailable options are:\n   -h     : help; print brief help on version and usage\n   -m     : only print symbols aligned to match states\n   -o <f> : save alignment in file <f>\n   -q     : quiet - suppress verbose banner\n\n   --informat <s>  : sequence file is in format <s>\n   --mapali <f>    : include alignment in file <f> using map in HMM\n   --oneline       : output Stockholm fmt with 1 line/seq, not interleaved\n   --outformat <s> : output alignment in format <s> [default: Stockholm]\n                       formats include: MSF, Clustal, Phylip, SELEX\n   --withali <f>   : include alignment to (fixed) alignment in file <f>\n\n\nhmmbuild - build a hidden Markov model from an alignment\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmbuild [-options] <hmmfile output> <alignment file>\n  Available options are:\n   -h     : help; print brief help on version and usage\n   -n <s> : name; name this (first) HMM <s>\n   -o <f> : re-save annotated alignment to <f>\n   -A     : append; append this HMM to <hmmfile>\n   -F     : force; allow overwriting of <hmmfile>\n\n  Alternative search algorithm styles: (default: hmmls domain alignment)\n   -f     : multi-hit local (hmmfs style)\n   -g     : global alignment (hmms style, Needleman/Wunsch)\n   -s     : local alignment (hmmsw style, Smith/Waterman)\n\n  Alternative model construction strategies: (default: MAP)\n   --fast        : Krogh/Haussler fast heuristic construction (see --gapmax)\n   --hand        : manual construction (requires annotated alignment)\n\n  Expert customization of parameters and priors:\n   --null  <f>   : read null (random sequence) model from <f>\n   --pam   <f>   : heuristic PAM-based prior, using BLAST PAM matrix in <f>\n   --prior <f>   : read Dirichlet prior parameters from <f>\n\n  Alternative sequence weighting strategies: (default: GSC weights)\n   --wblosum     : Henikoff simple filter weights (see --idlevel)\n   --wgsc        : Gerstein/Sonnhammer/Chothia tree weights (default)\n   --wme         : maximum entropy (ME)\n   --wpb         : Henikoff position-based weights\n   --wvoronoi    : Sibbald/Argos Voronoi weights\n   --wnone       : don\\'t do any weighting\n   --noeff       : don\\'t use effective sequence number; just use nseq\n   --pbswitch <n>: set switch from GSC to position-based wgts at > n seqs\n\n  Forcing an alphabet: (normally autodetected)\n   --amino   : override autodetection, assert that seqs are protein\n   --nucleic : override autodetection, assert that seqs are DNA/RNA\n\n  Other expert options:\n   --archpri <x> : set architecture size prior to <x> {0.85} [0..1]\n   --binary      : save the model in binary format, not ASCII text\n   --cfile <f>   : save count vectors to <f>\n   --gapmax <x>  : max fraction of gaps in mat column {0.50} [0..1]\n   --idlevel <x> : set frac. id level used by eff. nseq and --wblosum {0.62}\n   --informat <s>: input alignment is in format <s>, not Stockholm\n   --pamwgt <x>  : set weight on PAM-based prior to <x> {20.}[>=0]\n   --swentry <x> : set S/W aggregate entry prob. to <x> {0.5}\n   --swexit <x>  : set S/W aggregate exit prob. to <x>  {0.5}\n   --verbose     : print boring information\n\n\nhmmcalibrate -- calibrate HMM search statistics\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmcalibrate [-options] <hmmfile>\nAvailable options are:\n  -h             : print short usage and version info, then exit\n\n  --cpu <n>      : run <n> threads in parallel (if threaded)\n  --fixed <n>    : fix random sequence length at <n>\n  --histfile <f> : save histogram(s) to file <f>\n  --mean <x>     : set random seq length mean at <x> [350]\n  --num <n>      : set number of sampled seqs to <n> [5000]\n  --pvm          : run on a Parallel Virtual Machine (PVM)\n  --sd <x>       : set random seq length std. dev to <x> [350]\n  --seed <n>     : set random seed to <n> [time()]\n\nhmmconvert - convert between profile HMM file formats\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmconvert [-options] <old hmm file> <new hmm file>\n  Available options are:\n   -h        : help; print brief help on version and usage\n\n   -a        : convert to HMMER ASCII file (the default)\n   -b        : convert to HMMER binary file\n   -p        : convert to GCG Profile .prf format\n   -P        : convert to Compugen extended .eprf profile format\n\n   -A        : append mode; append to <new hmm file>\n   -F        : force mode; allow overwriting of existing files\n\n\n\nhmmemit - generate sequences from a profile HMM\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmemit [-options] <hmm file>\nAvailable options are:\n   -a     : write generated sequences as an alignment, not FASTA\n   -c     : generate a single \"consensus\" sequence\n   -h     : help; print brief help on version and usage\n   -n <n> : emit <n> sequences (default 10)\n   -o <f> : save sequences in file <f>\n   -q     : quiet - suppress verbose banner\n\n   --seed <n>     : set random number seed to <n>\n\nhmmfetch -- retrieve specific HMM from an HMM database\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmfetch [-options] <hmmfile> <HMM name>\nAvailable options are:\n  -h   : print short usage and version info, then exit\n  -n   : interpret <HMM name> instead as an HMM number (0..nhmm-1)\n\n\nhmmindex -- create SSI index for an HMM database\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmindex [-options] <hmmfile>\nAvailable options are:\n  -h             : print short usage and version info, then exit\n\n\nhmmpfam - search one or more sequences against HMM database\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmpfam [-options] <hmm database> <sequence file or database>\n  Available options are:\n   -h        : help; print brief help on version and usage\n   -n        : nucleic acid models/sequence (default protein)\n   -A <n>    : sets alignment output limit to <n> best domain alignments\n   -E <x>    : sets E value cutoff (globE) to <x>; default 10\n   -T <x>    : sets T bit threshold (globT) to <x>; no threshold by default\n   -Z <n>    : sets Z (# models) for E-value calculation\n\n   --acc         : use HMM accession numbers instead of names in output\n   --compat      : make best effort to use last version\\'s output style\n   --cpu <n>     : run <n> threads in parallel (if threaded)\n   --cut_ga      : use Pfam GA gathering threshold cutoffs\n   --cut_nc      : use Pfam NC noise threshold cutoffs\n   --cut_tc      : use Pfam TC trusted threshold cutoffs\n   --domE <x>    : sets domain Eval cutoff (2nd threshold) to <x>\n   --domT <x>    : sets domain T bit thresh (2nd threshold) to <x>\n   --forward     : use the full Forward() algorithm instead of Viterbi\n   --informat <s>: sequence file is in format <s>\n   --null2       : turn OFF the post hoc second null model\n   --pvm         : run on a PVM (Parallel Virtual Machine) cluster\n   --xnu         : turn ON XNU filtering of query protein sequence\n\n\nhmmsearch - search a sequence database with a profile HMM\nHMMER 2.3.2 (Oct 2003)\nCopyright (C) 1992-2003 HHMI/Washington University School of Medicine\nFreely distributed under the GNU General Public License (GPL)\n- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmsearch [-options] <hmmfile> <sequence file or database>\n  Available options are:\n   -h        : help; print brief help on version and usage\n   -A <n>    : sets alignment output limit to <n> best domain alignments\n   -E <x>    : sets E value cutoff (globE) to <= x\n   -T <x>    : sets T bit threshold (globT) to >= x\n   -Z <n>    : sets Z (# seqs) for E-value calculation\n\n   --compat       : make best effort to use last version\\'s output style\n   --cpu <n>      : run <n> threads in parallel (if threaded)\n   --cut_ga       : use Pfam GA gathering threshold cutoffs\n   --cut_nc       : use Pfam NC noise threshold cutoffs\n   --cut_tc       : use Pfam TC trusted threshold cutoffs\n   --domE <x>     : sets domain Eval cutoff (2nd threshold) to <= x\n   --domT <x>     : sets domain T bit thresh (2nd threshold) to >= x\n   --forward      : use the full Forward() algorithm instead of Viterbi\n   --informat <s> : sequence file is in format <s>\n   --null2        : turn OFF the post hoc second null model\n   --pvm          : run on a Parallel Virtual Machine (PVM)\n   --xnu          : turn ON XNU filtering of target protein sequences\n\n"}]}
{"messages": [{"role": "user", "content":   "hmmer-3.2.1"}, {"role": "assistant", "content":  "\n# hmmalign :: align sequences to a profile HMM\n# HMMER 3.1b2 (February 2015); http://hmmer.org/\n# Copyright (C) 2015 Howard Hughes Medical Institute.\n# Freely distributed under the GNU General Public License (GPLv3).\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmalign [-options] <hmmfile> <seqfile>\n\nBasic options:\n  -h     : show brief help on version and usage\n  -o <f> : output alignment to file <f>, not stdout\n\nLess common options:\n  --mapali <f>    : include alignment in file <f> (same ali that HMM came from)\n  --trim          : trim terminal tails of nonaligned residues from alignment\n  --amino         : assert <seqfile>, <hmmfile> both protein: no autodetection\n  --dna           : assert <seqfile>, <hmmfile> both DNA: no autodetection\n  --rna           : assert <seqfile>, <hmmfile> both RNA: no autodetection\n  --informat <s>  : assert <seqfile> is in format <s>: no autodetection\n  --outformat <s> : output alignment in format <s>  [Stockholm]\n\nSequence input formats include:   FASTA, EMBL, GenBank, UniProt\nAlignment output formats include: Stockholm, Pfam, A2M, PSIBLAST\n\n# hmmbuild :: profile HMM construction from multiple sequence alignments\n# HMMER 3.1b2 (February 2015); http://hmmer.org/\n# Copyright (C) 2015 Howard Hughes Medical Institute.\n# Freely distributed under the GNU General Public License (GPLv3).\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmbuild [-options] <hmmfile_out> <msafile>\n\nBasic options:\n  -h     : show brief help on version and usage\n  -n <s> : name the HMM <s>\n  -o <f> : direct summary output to file <f>, not stdout\n  -O <f> : resave annotated, possibly modified MSA to file <f>\n\nOptions for selecting alphabet rather than guessing it:\n  --amino : input alignment is protein sequence data\n  --dna   : input alignment is DNA sequence data\n  --rna   : input alignment is RNA sequence data\n\nAlternative model construction strategies:\n  --fast           : assign cols w/ >= symfrac residues as consensus  [default]\n  --hand           : manual construction (requires reference annotation)\n  --symfrac <x>    : sets sym fraction controlling --fast construction  [0.5]\n  --fragthresh <x> : if L <= x*alen, tag sequence as a fragment  [0.5]\n\nAlternative relative sequence weighting strategies:\n  --wpb     : Henikoff position-based weights  [default]\n  --wgsc    : Gerstein/Sonnhammer/Chothia tree weights\n  --wblosum : Henikoff simple filter weights\n  --wnone   : dont do any relative weighting; set all to 1\n  --wgiven  : use weights as given in MSA file\n  --wid <x> : for --wblosum: set identity cutoff  [0.62]  (0<=x<=1)\n\nAlternative effective sequence weighting strategies:\n  --eent       : adjust eff seq # to achieve relative entropy target  [default]\n  --eclust     : eff seq # is # of single linkage clusters\n  --enone      : no effective seq # weighting: just use nseq\n  --eset <x>   : set eff seq # for all models to <x>\n  --ere <x>    : for --eent: set minimum rel entropy/position to <x>\n  --esigma <x> : for --eent: set sigma param to <x>  [45.0]\n  --eid <x>    : for --eclust: set fractional identity cutoff to <x>  [0.62]\n\nAlternative prior strategies:\n  --pnone       : dont use any prior; parameters are frequencies\n  --plaplace    : use a Laplace +1 prior\n  --popen <x>   : force gap open prob. (w/ --singlemx, aa default 0.02, nt 0.031)\n  --pextend <x> : force gap extend prob. (w/ --singlemx, aa default 0.4, nt 0.75)\n\nHandling single sequence inputs:\n  --singlemx   : use substitution score matrix for single-sequence inputs\n  --mx <s>     : substitution score matrix (built-in matrices, with --singlemx)\n  --mxfile <f> : read substitution score matrix from file <f> (with --singlemx)\n\nControl of E-value calibration:\n  --EmL <n> : length of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EmN <n> : number of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EvL <n> : length of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EvN <n> : number of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EfL <n> : length of sequences for Forward exp tail tau fit  [100]  (n>0)\n  --EfN <n> : number of sequences for Forward exp tail tau fit  [200]  (n>0)\n  --Eft <x> : tail mass for Forward exponential tail tau fit  [0.04]  (0<x<1)\n\nOther options:\n  --cpu <n>          : number of parallel CPU workers for multithreads\n  --stall            : arrest after start: for attaching debugger to process\n  --informat <s>     : assert input alifile is in format <s> (no autodetect)\n  --seed <n>         : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --w_beta <x>       : tail mass at which window length is determined\n  --w_length <n>     : window length \n  --maxinsertlen <n> : pretend all inserts are length <= <n>\n# hmmconvert :: convert profile file to a HMMER format\n# HMMER 3.1b2 (February 2015); http://hmmer.org/\n# Copyright (C) 2015 Howard Hughes Medical Institute.\n# Freely distributed under the GNU General Public License (GPLv3).\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmconvert [-options] <hmmfile>\n\nOptions:\n  -h           : show brief help on version and usage\n  -a           : ascii:  output models in HMMER3 ASCII format  [default]\n  -b           : binary: output models in HMMER3 binary format\n  -2           : HMMER2: output backward compatible HMMER2 ASCII format (ls mode)\n  --outfmt <s> : choose output legacy 3.x file formats by name, such as 3/a\n# hmmemit :: sample sequence(s) from a profile HMM\n# HMMER 3.1b2 (February 2015); http://hmmer.org/\n# Copyright (C) 2015 Howard Hughes Medical Institute.\n# Freely distributed under the GNU General Public License (GPLv3).\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmemit [-options] <hmmfile (single)>\n\nCommon options are:\n  -h     : show brief help on version and usage\n  -o <f> : send sequence output to file <f>, not stdout\n  -N <n> : number of seqs to sample  [1]  (n>0)\n\nOptions controlling what to emit:\n  -a : emit alignment\n  -c : emit simple majority-rule consensus sequence\n  -C : emit fancier consensus sequence (reqs --minl, --minu)\n  -p : sample sequences from profile, not core model\n\nOptions controlling emission from profiles with -p:\n  -L <n>      : set expected length from profile to <n>  [400]\n  --local     : configure profile in multihit local mode  [default]\n  --unilocal  : configure profile in unilocal mode\n  --glocal    : configure profile in multihit glocal mode\n  --uniglocal : configure profile in unihit glocal mode\n\nOptions controlling fancy consensus emission with -C:\n  --minl <x> : show consensus as any (X/N) unless >= this fraction  [0.0]\n  --minu <x> : show consensus as upper case if >= this fraction  [0.0]\n\nOther options::\n  --seed <n> : set RNG seed to <n>  [0]  (n>=0)\n# hmmfetch :: retrieve profile HMM(s) from a file\n# Easel h3.1b2 (February 2015)\n# Copyright (C) 2015 Howard Hughes Medical Institute.\n# Freely distributed under the Janelia Farm Software License.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmfetch [options] <hmmfile> <key>         (retrieves HMM named <key>)\nUsage: hmmfetch [options] -f <hmmfile> <keyfile>  (retrieves all HMMs in <keyfile>)\nUsage: hmmfetch [options] --index <hmmfile>       (indexes <hmmfile>)\n\nOptions:\n  -h      : help; show brief info on version and usage\n  -f      : second cmdline arg is a file of names to retrieve\n  -o <f>  : output HMM to file <f> instead of stdout\n  -O      : output HMM to file named <key>\n  --index : index the <hmmfile>, creating <hmmfile>.ssi\n# hmmsearch :: search profile(s) against a sequence database\n# HMMER 3.1b2 (February 2015); http://hmmer.org/\n# Copyright (C) 2015 Howard Hughes Medical Institute.\n# Freely distributed under the GNU General Public License (GPLv3).\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmsearch [options] <hmmfile> <seqdb>\n\nBasic options:\n  -h : show brief help on version and usage\n\nOptions directing output:\n  -o <f>           : direct output to file <f>, not stdout\n  -A <f>           : save multiple alignment of all hits to file <f>\n  --tblout <f>     : save parseable table of per-sequence hits to file <f>\n  --domtblout <f>  : save parseable table of per-domain hits to file <f>\n  --pfamtblout <f> : save table of hits and domains to file, in Pfam format <f>\n  --acc            : prefer accessions over names in output\n  --noali          : dont output alignments, so output is smaller\n  --notextw        : unlimit ASCII text output line width\n  --textw <n>      : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling reporting thresholds:\n  -E <x>     : report sequences <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x>     : report sequences >= this score threshold in output\n  --domE <x> : report domains <= this E-value threshold in output  [10.0]  (x>0)\n  --domT <x> : report domains >= this score cutoff in output\n\nOptions controlling inclusion (significance) thresholds:\n  --incE <x>    : consider sequences <= this E-value threshold as significant\n  --incT <x>    : consider sequences >= this score threshold as significant\n  --incdomE <x> : consider domains <= this E-value threshold as significant\n  --incdomT <x> : consider domains >= this score threshold as significant\n\nOptions controlling model-specific thresholding:\n  --cut_ga : use profiles GA gathering cutoffs to set all thresholding\n  --cut_nc : use profiles NC noise cutoffs to set all thresholding\n  --cut_tc : use profiles TC trusted cutoffs to set all thresholding\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : Stage 1 (MSV) threshold: promote hits w/ P <= F1  [0.02]\n  --F2 <x> : Stage 2 (Vit) threshold: promote hits w/ P <= F2  [1e-3]\n  --F3 <x> : Stage 3 (Fwd) threshold: promote hits w/ P <= F3  [1e-5]\n  --nobias : turn off composition bias filter\n\nOther expert options:\n  --nonull2     : turn off biased composition score corrections\n  -Z <x>        : set # of comparisons done, for E-value calculation\n  --domZ <x>    : set # of significant seqs, for domain E-value calculation\n  --seed <n>    : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --tformat <s> : assert target <seqfile> is in format <s>: no autodetection\n  --cpu <n>     : number of parallel CPU workers to use for multithreads\n# hmmbuild :: profile HMM construction from multiple sequence alignments\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmbuild [-options] <hmmfile_out> <msafile>\n\nBasic options:\n  -h     : show brief help on version and usage\n  -n <s> : name the HMM <s>\n  -o <f> : direct summary output to file <f>, not stdout\n  -O <f> : resave annotated, possibly modified MSA to file <f>\n\nOptions for selecting alphabet rather than guessing it:\n  --amino : input alignment is protein sequence data\n  --dna   : input alignment is DNA sequence data\n  --rna   : input alignment is RNA sequence data\n\nAlternative model construction strategies:\n  --fast           : assign cols w/ >= symfrac residues as consensus  [default]\n  --hand           : manual construction (requires reference annotation)\n  --symfrac <x>    : sets sym fraction controlling --fast construction  [0.5]\n  --fragthresh <x> : if L <= x*alen, tag sequence as a fragment  [0.5]\n\nAlternative relative sequence weighting strategies:\n  --wpb     : Henikoff position-based weights  [default]\n  --wgsc    : Gerstein/Sonnhammer/Chothia tree weights\n  --wblosum : Henikoff simple filter weights\n  --wnone   : dont do any relative weighting; set all to 1\n  --wgiven  : use weights as given in MSA file\n  --wid <x> : for --wblosum: set identity cutoff  [0.62]  (0<=x<=1)\n\nAlternative effective sequence weighting strategies:\n  --eent       : adjust eff seq # to achieve relative entropy target  [default]\n  --eclust     : eff seq # is # of single linkage clusters\n  --enone      : no effective seq # weighting: just use nseq\n  --eset <x>   : set eff seq # for all models to <x>\n  --ere <x>    : for --eent: set minimum rel entropy/position to <x>\n  --esigma <x> : for --eent: set sigma param to <x>  [45.0]\n  --eid <x>    : for --eclust: set fractional identity cutoff to <x>  [0.62]\n\nAlternative prior strategies:\n  --pnone    : dont use any prior; parameters are frequencies\n  --plaplace : use a Laplace +1 prior\n\nHandling single sequence inputs:\n  --singlemx    : use substitution score matrix for single-sequence inputs\n  --mx <s>      : substitution score matrix (built-in matrices, with --singlemx)\n  --mxfile <f>  : read substitution score matrix from file <f> (with --singlemx)\n  --popen <x>   : force gap open prob. (w/ --singlemx, aa default 0.02, nt 0.031)\n  --pextend <x> : force gap extend prob. (w/ --singlemx, aa default 0.4, nt 0.75)\n  --tmm <x>     : MM transition\n  --tmi <x>     : MI transition\n  --tmd <x>     : MD transition\n  --tim <x>     : IM transition\n  --tii <x>     : II transition\n  --tdm <x>     : DM transition\n  --tdd <x>     : DD transition\n\nControl of E-value calibration:\n  --EmL <n> : length of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EmN <n> : number of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EvL <n> : length of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EvN <n> : number of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EfL <n> : length of sequences for Forward exp tail tau fit  [100]  (n>0)\n  --EfN <n> : number of sequences for Forward exp tail tau fit  [200]  (n>0)\n  --Eft <x> : tail mass for Forward exponential tail tau fit  [0.04]  (0<x<1)\n\nOther options:\n  --cpu <n>          : number of parallel CPU workers for multithreads  [2]\n  --stall            : arrest after start: for attaching debugger to process\n  --informat <s>     : assert input alifile is in format <s> (no autodetect)\n  --seed <n>         : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --w_beta <x>       : tail mass at which window length is determined\n  --w_length <n>     : window length \n  --maxinsertlen <n> : pretend all inserts are length <= <n>\n# hmmalign :: align sequences to a profile HMM\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmalign [-options] <hmmfile> <seqfile>\n\nBasic options:\n  -h     : show brief help on version and usage\n  -o <f> : output alignment to file <f>, not stdout\n\nLess common options:\n  --mapali <f>    : include alignment in file <f> (same ali that HMM came from)\n  --trim          : trim terminal tails of nonaligned residues from alignment\n  --amino         : assert <seqfile>, <hmmfile> both protein: no autodetection\n  --dna           : assert <seqfile>, <hmmfile> both DNA: no autodetection\n  --rna           : assert <seqfile>, <hmmfile> both RNA: no autodetection\n  --informat <s>  : assert <seqfile> is in format <s>: no autodetection\n  --outformat <s> : output alignment in format <s>  [Stockholm]\n\nSequence input formats include:   FASTA, EMBL, GenBank, UniProt\nAlignment output formats include: Stockholm, Pfam, A2M, PSIBLAST\n\n# hmmsearch :: search profile(s) against a sequence database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmsearch [options] <hmmfile> <seqdb>\n\nBasic options:\n  -h : show brief help on version and usage\n\nOptions directing output:\n  -o <f>           : direct output to file <f>, not stdout\n  -A <f>           : save multiple alignment of all hits to file <f>\n  --tblout <f>     : save parseable table of per-sequence hits to file <f>\n  --domtblout <f>  : save parseable table of per-domain hits to file <f>\n  --pfamtblout <f> : save table of hits and domains to file, in Pfam format <f>\n  --acc            : prefer accessions over names in output\n  --noali          : dont output alignments, so output is smaller\n  --notextw        : unlimit ASCII text output line width\n  --textw <n>      : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling reporting thresholds:\n  -E <x>     : report sequences <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x>     : report sequences >= this score threshold in output\n  --domE <x> : report domains <= this E-value threshold in output  [10.0]  (x>0)\n  --domT <x> : report domains >= this score cutoff in output\n\nOptions controlling inclusion (significance) thresholds:\n  --incE <x>    : consider sequences <= this E-value threshold as significant\n  --incT <x>    : consider sequences >= this score threshold as significant\n  --incdomE <x> : consider domains <= this E-value threshold as significant\n  --incdomT <x> : consider domains >= this score threshold as significant\n\nOptions controlling model-specific thresholding:\n  --cut_ga : use profiles GA gathering cutoffs to set all thresholding\n  --cut_nc : use profiles NC noise cutoffs to set all thresholding\n  --cut_tc : use profiles TC trusted cutoffs to set all thresholding\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : Stage 1 (MSV) threshold: promote hits w/ P <= F1  [0.02]\n  --F2 <x> : Stage 2 (Vit) threshold: promote hits w/ P <= F2  [1e-3]\n  --F3 <x> : Stage 3 (Fwd) threshold: promote hits w/ P <= F3  [1e-5]\n  --nobias : turn off composition bias filter\n\nOther expert options:\n  --nonull2     : turn off biased composition score corrections\n  -Z <x>        : set # of comparisons done, for E-value calculation\n  --domZ <x>    : set # of significant seqs, for domain E-value calculation\n  --seed <n>    : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --tformat <s> : assert target <seqfile> is in format <s>: no autodetection\n  --cpu <n>     : number of parallel CPU workers to use for multithreads  [2]\n# hmmscan :: search sequence(s) against a profile database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmscan [-options] <hmmdb> <seqfile>\n\nBasic options:\n  -h : show brief help on version and usage\n\nOptions controlling output:\n  -o <f>           : direct output to file <f>, not stdout\n  --tblout <f>     : save parseable table of per-sequence hits to file <f>\n  --domtblout <f>  : save parseable table of per-domain hits to file <f>\n  --pfamtblout <f> : save table of hits and domains to file, in Pfam format <f>\n  --acc            : prefer accessions over names in output\n  --noali          : dont output alignments, so output is smaller\n  --notextw        : unlimit ASCII text output line width\n  --textw <n>      : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling reporting thresholds:\n  -E <x>     : report models <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x>     : report models >= this score threshold in output\n  --domE <x> : report domains <= this E-value threshold in output  [10.0]  (x>0)\n  --domT <x> : report domains >= this score cutoff in output\n\nOptions controlling inclusion (significance) thresholds:\n  --incE <x>    : consider models <= this E-value threshold as significant\n  --incT <x>    : consider models >= this score threshold as significant\n  --incdomE <x> : consider domains <= this E-value threshold as significant\n  --incdomT <x> : consider domains >= this score threshold as significant\n\nOptions for model-specific thresholding:\n  --cut_ga : use profiles GA gathering cutoffs to set all thresholding\n  --cut_nc : use profiles NC noise cutoffs to set all thresholding\n  --cut_tc : use profiles TC trusted cutoffs to set all thresholding\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : MSV threshold: promote hits w/ P <= F1  [0.02]\n  --F2 <x> : Vit threshold: promote hits w/ P <= F2  [1e-3]\n  --F3 <x> : Fwd threshold: promote hits w/ P <= F3  [1e-5]\n  --nobias : turn off composition bias filter\n\nOther expert options:\n  --nonull2     : turn off biased composition score corrections\n  -Z <x>        : set # of comparisons done, for E-value calculation\n  --domZ <x>    : set # of significant seqs, for domain E-value calculation\n  --seed <n>    : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --qformat <s> : assert input <seqfile> is in format <s>: no autodetection\n  --cpu <n>     : number of parallel CPU workers to use for multithreads  [2]\n# hmmpress :: prepare an HMM database for faster hmmscan searches\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmpress [-options] <hmmfile>\n\nOptions:\n  -h : show brief help on version and usage\n  -f : force: overwrite any previous pressed files\n# phmmer :: search a protein sequence against a protein database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: phmmer [-options] <seqfile> <seqdb>\n\nBasic options:\n  -h : show brief help on version and usage\n\nOptions directing output:\n  -o <f>           : direct output to file <f>, not stdout\n  -A <f>           : save multiple alignment of hits to file <f>\n  --tblout <f>     : save parseable table of per-sequence hits to file <f>\n  --domtblout <f>  : save parseable table of per-domain hits to file <f>\n  --pfamtblout <f> : save table of hits and domains to file, in Pfam format <f>\n  --acc            : prefer accessions over names in output\n  --noali          : dont output alignments, so output is smaller\n  --notextw        : unlimit ASCII text output line width\n  --textw <n>      : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling scoring system:\n  --popen <x>   : gap open probability\n  --pextend <x> : gap extend probability\n  --mx <s>      : substitution score matrix choice (of some built-in matrices)\n  --mxfile <f>  : read substitution score matrix from file <f>\n\nOptions controlling reporting thresholds:\n  -E <x>     : report sequences <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x>     : report sequences >= this score threshold in output\n  --domE <x> : report domains <= this E-value threshold in output  [10.0]  (x>0)\n  --domT <x> : report domains >= this score cutoff in output\n\nOptions controlling inclusion (significance) thresholds:\n  --incE <x>    : consider sequences <= this E-value threshold as significant\n  --incT <x>    : consider sequences >= this score threshold as significant\n  --incdomE <x> : consider domains <= this E-value threshold as significant\n  --incdomT <x> : consider domains >= this score threshold as significant\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : Stage 1 (MSV) threshold: promote hits w/ P <= F1  [0.02]\n  --F2 <x> : Stage 2 (Vit) threshold: promote hits w/ P <= F2  [1e-3]\n  --F3 <x> : Stage 3 (Fwd) threshold: promote hits w/ P <= F3  [1e-5]\n  --nobias : turn off composition bias filter\n\nOptions controlling E value calibration:\n  --EmL <n> : length of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EmN <n> : number of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EvL <n> : length of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EvN <n> : number of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EfL <n> : length of sequences for Forward exp tail tau fit  [100]  (n>0)\n  --EfN <n> : number of sequences for Forward exp tail tau fit  [200]  (n>0)\n  --Eft <x> : tail mass for Forward exponential tail tau fit  [0.04]  (0<x<1)\n\nOther expert options:\n  --nonull2     : turn off biased composition score corrections\n  -Z <x>        : set # of comparisons done, for E-value calculation\n  --domZ <x>    : set # of significant seqs, for domain E-value calculation\n  --seed <n>    : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --qformat <s> : assert query <seqfile> is in format <s>: no autodetection\n  --tformat <s> : assert target <seqdb> is in format <s>>: no autodetection\n  --cpu <n>     : number of parallel CPU workers to use for multithreads  [2]\n# jackhmmer :: iteratively search a protein sequence against a protein database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: jackhmmer [-options] <seqfile> <seqdb>\n\nBasic options:\n  -h     : show brief help on version and usage\n  -N <n> : set maximum number of iterations to <n>  [5]  (n>0)\n\nOptions directing output:\n  -o <f>          : direct output to file <f>, not stdout\n  -A <f>          : save multiple alignment of hits to file <f>\n  --tblout <f>    : save parseable table of per-sequence hits to file <f>\n  --domtblout <f> : save parseable table of per-domain hits to file <f>\n  --chkhmm <f>    : save HMM checkpoints to files <f>-<iteration>.hmm\n  --chkali <f>    : save alignment checkpoints to files <f>-<iteration>.sto\n  --acc           : prefer accessions over names in output\n  --noali         : dont output alignments, so output is smaller\n  --notextw       : unlimit ASCII text output line width\n  --textw <n>     : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling scoring system in first iteration:\n  --popen <x>   : gap open probability\n  --pextend <x> : gap extend probability\n  --mx <s>      : substitution score matrix choice (of some built-in matrices)\n  --mxfile <f>  : read substitution score matrix from file <f>\n\nOptions controlling reporting thresholds:\n  -E <x>     : report sequences <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x>     : report sequences >= this score threshold in output\n  --domE <x> : report domains <= this E-value threshold in output  [10.0]  (x>0)\n  --domT <x> : report domains >= this score cutoff in output\n\nOptions controlling significance thresholds for inclusion in next round:\n  --incE <x>    : consider sequences <= this E-value threshold as significant\n  --incT <x>    : consider sequences >= this score threshold as significant\n  --incdomE <x> : consider domains <= this E-value threshold as significant\n  --incdomT <x> : consider domains >= this score threshold as significant\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : Stage 1 (MSV) threshold: promote hits w/ P <= F1  [0.02]\n  --F2 <x> : Stage 2 (Vit) threshold: promote hits w/ P <= F2  [1e-3]\n  --F3 <x> : Stage 3 (Fwd) threshold: promote hits w/ P <= F3  [1e-5]\n  --nobias : turn off composition bias filter\n\nOptions controlling model construction after first iteration:\n  --fragthresh <x> : if L <= x*alen, tag sequence as a fragment  [0.5]  (0<=x<=1)\n\nOptions controlling relative weights in models after first iteration:\n  --wpb     : Henikoff position-based weights  [default]\n  --wgsc    : Gerstein/Sonnhammer/Chothia tree weights\n  --wblosum : Henikoff simple filter weights\n  --wnone   : dont do any relative weighting; set all to 1\n  --wid <x> : for --wblosum: set identity cutoff  [0.62]  (0<=x<=1)\n\nOptions controlling effective seq number in models after first iteration:\n  --eent       : adjust eff seq # to achieve relative entropy target\n  --eentexp    : adjust eff seq # to reach rel. ent. target using exp scaling\n  --eclust     : eff seq # is # of single linkage clusters\n  --enone      : no effective seq # weighting: just use nseq\n  --eset <x>   : set eff seq # for all models to <x>\n  --ere <x>    : for --eent[exp]: set minimum rel entropy/position to <x>\n  --esigma <x> : for --eent[exp]: set sigma param to <x>\n  --eid <x>    : for --eclust: set fractional identity cutoff to <x>\n\nOptions controlling prior strategy in models after first iteration:\n  --pnone    : dont use any prior; parameters are frequencies\n  --plaplace : use a Laplace +1 prior\n\nOptions controlling E value calibration:\n  --EmL <n> : length of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EmN <n> : number of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EvL <n> : length of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EvN <n> : number of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EfL <n> : length of sequences for Forward exp tail tau fit  [100]  (n>0)\n  --EfN <n> : number of sequences for Forward exp tail tau fit  [200]  (n>0)\n  --Eft <x> : tail mass for Forward exponential tail tau fit  [0.04]  (0<x<1)\n\nOther expert options:\n  --nonull2     : turn off biased composition score corrections\n  -Z <x>        : set # of comparisons done, for E-value calculation\n  --domZ <x>    : set # of significant seqs, for domain E-value calculation\n  --seed <n>    : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --qformat <s> : assert query <seqfile> is in format <s>: no autodetection\n  --tformat <s> : assert target <seqdb> is in format <s>>: no autodetection\n  --cpu <n>     : number of parallel CPU workers to use for multithreads  [2]\n# nhmmer :: search a DNA model, alignment, or sequence against a DNA database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: nhmmer [options] <query hmmfile|alignfile|seqfile> <target seqfile>\n\nBasic options:\n  -h : show brief help on version and usage\n\nOptions directing output:\n  -o <f>             : direct output to file <f>, not stdout\n  -A <f>             : save multiple alignment of all hits to file <f>\n  --tblout <f>       : save parseable table of hits to file <f>\n  --dfamtblout <f>   : save table of hits to file, in Dfam format <f>\n  --aliscoresout <f> : save scores for each position in each alignment to <f>\n  --hmmout <f>       : if input is alignment(s), write produced hmms to file <f>\n  --acc              : prefer accessions over names in output\n  --noali            : dont output alignments, so output is smaller\n  --notextw          : unlimit ASCII text output line width\n  --textw <n>        : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling scoring system:\n  --singlemx    : use substitution score matrix w/ single-sequence MSA-format inputs\n  --popen <x>   : gap open probability  [0.03125]  (0<=x<0.5)\n  --pextend <x> : gap extend probability  [0.75]  (0<=x<1)\n  --mxfile <f>  : read substitution score matrix from file <f>\n\nOptions controlling reporting thresholds:\n  -E <x> : report sequences <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x> : report sequences >= this score threshold in output\n\nOptions controlling inclusion (significance) thresholds:\n  --incE <x> : consider sequences <= this E-value threshold as significant  [0.01]  (x>0)\n  --incT <x> : consider sequences >= this score threshold as significant\n\nOptions controlling model-specific thresholding:\n  --cut_ga : use profiles GA gathering cutoffs to set all thresholding\n  --cut_nc : use profiles NC noise cutoffs to set all thresholding\n  --cut_tc : use profiles TC trusted cutoffs to set all thresholding\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : Stage 1 (SSV) threshold: promote hits w/ P <= F1\n  --F2 <x> : Stage 2 (Vit) threshold: promote hits w/ P <= F2  [3e-3]\n  --F3 <x> : Stage 3 (Fwd) threshold: promote hits w/ P <= F3  [3e-5]\n  --nobias : turn off composition bias filter\n\nOptions for selecting query alphabet rather than guessing it:\n  --dna : input alignment is DNA sequence data\n  --rna : input alignment is RNA sequence data\n\nOptions controlling seed search heuristic:\n  --seed_max_depth <n>     : seed length at which bit threshold must be met  [15]\n  --seed_sc_thresh <x>     : Default req. score for FM seed (bits)  [15]\n  --seed_sc_density <x>    : seed must maintain this bit density from one of two ends  [0.8]\n  --seed_drop_max_len <n>  : maximum run length with score under (max - [fm_drop_lim])  [4]\n  --seed_drop_lim <x>      : in seed, max drop in a run of length [fm_drop_max_len]  [0.3]\n  --seed_req_pos <n>       : minimum number consecutive positive scores in seed  [5]\n  --seed_consens_match <n> : <n> consecutive matches to consensus will override score threshold  [11]\n  --seed_ssv_length <n>    : length of window around FM seed to get full SSV diagonal  [70]\n\nOther expert options:\n  --qhmm             : assert query is an hmm file\n  --qfasta           : assert query is an unaligned fasta file\n  --qmsa             : assert query is an aligned file format; can use --qformat to specify\n  --qformat <s>      : assert query msa <seqfile> is in format <s>\n  --tformat <s>      : assert target <seqdb> is in format <s>\n  --nonull2          : turn off biased composition score corrections\n  -Z <x>             : set database size (Megabases) to <x> for E-value calculations  (x>0)\n  --seed <n>         : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]  (n>=0)\n  --w_beta <x>       : tail mass at which window length is determined\n  --w_length <n>     : window length - essentially max expected hit length\n  --block_length <n> : length of blocks read from target database (threaded)   (n>=50000)\n  --watson           : only search the top strand\n  --crick            : only search the bottom strand\n  --cpu <n>          : number of parallel CPU workers to use for multithreads  [2]  (n>=0)\n# nhmmscan :: search DNA sequence(s) against a DNA profile database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: nhmmscan [-options] <hmmdb> <seqfile>\n\nBasic options:\n  -h : show brief help on version and usage\n\nOptions controlling output:\n  -o <f>             : direct output to file <f>, not stdout\n  --tblout <f>       : save parseable table of per-sequence hits to file <f>\n  --dfamtblout <f>   : save table of hits to file, in Dfam format <f>\n  --aliscoresout <f> : save of scores for each position in each alignment to <f>\n  --acc              : prefer accessions over names in output\n  --noali            : dont output alignments, so output is smaller\n  --notextw          : unlimit ASCII text output line width\n  --textw <n>        : set max width of ASCII text output lines  [120]  (n>=120)\n\nOptions controlling reporting thresholds:\n  -E <x> : report models <= this E-value threshold in output  [10.0]  (x>0)\n  -T <x> : report models >= this score threshold in output\n\nOptions controlling inclusion (significance) thresholds:\n  --incE <x> : consider models <= this E-value threshold as significant  [0.01]\n  --incT <x> : consider models >= this score threshold as significant\n\nOptions for model-specific thresholding:\n  --cut_ga : use profiles GA gathering cutoffs to set all thresholding\n  --cut_nc : use profiles NC noise cutoffs to set all thresholding\n  --cut_tc : use profiles TC trusted cutoffs to set all thresholding\n\nOptions controlling acceleration heuristics:\n  --max    : Turn all heuristic filters off (less speed, more power)\n  --F1 <x> : MSV threshold: promote hits w/ P <= F1  [0.02]\n  --F2 <x> : Vit threshold: promote hits w/ P <= F2  [3e-3]\n  --F3 <x> : Fwd threshold: promote hits w/ P <= F3  [3e-5]\n  --nobias : turn off composition bias filter\n\nOther expert options:\n  --qformat <s>  : assert input <seqfile> is in format <s>\n  --nonull2      : turn off biased composition score corrections\n  -Z <x>         : set # of comparisons done, for E-value calculation\n  --seed <n>     : set RNG seed to <n> (if 0: one-time arbitrary seed)  [42]\n  --w_beta <x>   : tail mass at which window length is determined\n  --w_length <n> : window length - essentially max expected hit length \n  --watson       : only search the top strand\n  --crick        : only search the bottom strand\n  --cpu <n>      : number of parallel CPU workers to use for multithreads  [2]\n# hmmfetch :: retrieve profile HMM(s) from a file\n# Easel 0.45h (June 2018)\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmfetch [options] <hmmfile> <key>         (retrieves HMM named <key>)\nUsage: hmmfetch [options] -f <hmmfile> <keyfile>  (retrieves all HMMs in <keyfile>)\nUsage: hmmfetch [options] --index <hmmfile>       (indexes <hmmfile>)\n\nOptions:\n  -h      : help; show brief info on version and usage\n  -f      : second cmdline arg is a file of names to retrieve\n  -o <f>  : output HMM to file <f> instead of stdout\n  -O      : output HMM to file named <key>\n  --index : index the <hmmfile>, creating <hmmfile>.ssi\n# hmmstat :: display summary statistics for a profile file\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmstat [-options] <hmmfile>\n\nOptions:\n  -h           : show brief help on version and usage\n  --eval2score : compute score for E-value (E) for database of (Z) sequences\n  --score2eval : compute E-value for score (S) for database of (Z) sequences\n  -Z <n>       : database size (in seqs) for --eval2score or --score2eval\n  --baseZ <n>  : database size (M bases) (DNA only, if search on both strands)\n  --baseZ1 <n> : database size (M bases) (DNA only, if search on single strand)\n  -E <x>       : E-value threshold, for --eval2score\n  -S <x>       : Score input for --score2eval\n# hmmemit :: sample sequence(s) from a profile HMM\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmemit [-options] <hmmfile (single)>\n\nCommon options are:\n  -h     : show brief help on version and usage\n  -o <f> : send sequence output to file <f>, not stdout\n  -N <n> : number of seqs to sample  [1]  (n>0)\n\nOptions controlling what to emit:\n  -a : emit alignment\n  -c : emit simple majority-rule consensus sequence\n  -C : emit fancier consensus sequence (reqs --minl, --minu)\n  -p : sample sequences from profile, not core model\n\nOptions controlling emission from profiles with -p:\n  -L <n>      : set expected length from profile to <n>  [400]\n  --local     : configure profile in multihit local mode  [default]\n  --unilocal  : configure profile in unilocal mode\n  --glocal    : configure profile in multihit glocal mode\n  --uniglocal : configure profile in unihit glocal mode\n\nOptions controlling fancy consensus emission with -C:\n  --minl <x> : show consensus as any (X/N) unless >= this fraction  [0.0]\n  --minu <x> : show consensus as upper case if >= this fraction  [0.0]\n\nOther options::\n  --seed <n> : set RNG seed to <n>  [0]  (n>=0)\n# hmmlogo :: given an hmm, produce data required to build an hmm logo\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmlogo [options] <hmmfile>\n\nOptions:\n  -h                      : show brief help on version and usage\n  --height_relent_all     : total height = relative entropy ; all letters shown (default)\n  --height_relent_abovebg : total height = relative entropy ; only letters >bg shown\n  --height_score          : total height = sums of (pos|neg) scores; residue height = score\n  --no_indel              : dont provide indel rate values\n# hmmconvert :: convert profile file to a HMMER format\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmconvert [-options] <hmmfile>\n\nOptions:\n  -h           : show brief help on version and usage\n  -a           : ascii:  output models in HMMER3 ASCII format  [default]\n  -b           : binary: output models in HMMER3 binary format\n  -2           : HMMER2: output backward compatible HMMER2 ASCII format (ls mode)\n  --outfmt <s> : choose output legacy 3.x file formats by name, such as 3/a\n# hmmpgmd :: search a query against a database\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmpgmd [options]\n\nBasic options:\n  -h : show brief help on version and usage\n\nOther expert options:\n  --master     : run program as the master server\n  --worker <s> : run program as a worker with server at <s>\n  --cport <n>  : port to use for client/server communication  [51371]\n  --wport <n>  : port to use for server/worker communication  [51372]\n  --ccncts <n> : maximum number of client side connections to accept  [16]\n  --wcncts <n> : maximum number of worker side connections to accept  [32]\n  --pid <f>    : file to write process id to\n  --daemon     : run as a daemon using config file: /etc/hmmpgmd.conf\n  --seqdb <f>  : protein database to cache for searches\n  --hmmdb <f>  : hmm database to cache for searches\n  --cpu <n>    : number of parallel CPU workers to use for multithreads  [2]\n# makehmmerdb :: build a HMMER binary-formatted database from an input sequence file\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: makehmmerdb [options] <seqfile> <binaryfile>\n\nBasic options:\n  -h : show brief help on version and usage\n\nSpecial options:\n  --informat <s>   : specify that input file is in format <s>\n  --bin_length <n> : bin length (power of 2;  32<=b<=4096)  [256]\n  --sa_freq <n>    : suffix array sample rate (power of 2)  [8]\n  --block_size <n> : input sequence broken into blocks this size (Mbases)  [50]\n# hmmsim :: collect profile HMM score distributions on random sequences\n# HMMER 3.2.1 (June 2018); http://hmmer.org/\n# Copyright (C) 2018 Howard Hughes Medical Institute.\n# Freely distributed under the BSD open source license.\n# - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -\nUsage: hmmsim [-options] <hmmfile>\n\nCommon options:\n  -h     : show brief help on version and usage\n  -a     : obtain alignment length statistics too\n  -v     : verbose: print scores\n  -L <n> : length of random target seqs  [100]  (n>0)\n  -N <n> : number of random target seqs  [1000]  (n>0)\n\nOutput options (only in serial mode, for single HMM input):\n  -o <f>      : direct output to file <f>, not stdout\n  --afile <f> : output alignment lengths to file <f>\n  --efile <f> : output E vs. E plots to <f> in xy format\n  --ffile <f> : output filter fraction: # seqs passing P thresh\n  --pfile <f> : output P(S>x) plots to <f> in xy format\n  --xfile <f> : output bitscores as binary double vector to <f>\n\nAlternative alignment styles :\n  --fs : multihit local alignment  [default]\n  --sw : unihit local alignment\n  --ls : multihit glocal alignment\n  --s  : unihit glocal alignment\n\nAlternative scoring algorithms :\n  --vit  : score seqs with the Viterbi algorithm  [default]\n  --fwd  : score seqs with the Forward algorithm\n  --hyb  : score seqs with the Hybrid algorithm\n  --msv  : score seqs with the MSV algorithm\n  --fast : use the optimized versions of the above\n\nControlling range of fitted tail masses :\n  --tmin <x>    : set lower bound tail mass for fwd,island  [0.02]\n  --tmax <x>    : set lower bound tail mass for fwd,island  [0.02]\n  --tpoints <n> : set number of tail probs to try  [1]\n  --tlinear     : use linear not log spacing of tail probs\n\nControlling E-value calibration :\n  --EmL <n> : length of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EmN <n> : number of sequences for MSV Gumbel mu fit  [200]  (n>0)\n  --EvL <n> : length of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EvN <n> : number of sequences for Viterbi Gumbel mu fit  [200]  (n>0)\n  --EfL <n> : length of sequences for Forward exp tail tau fit  [100]  (n>0)\n  --EfN <n> : number of sequences for Forward exp tail tau fit  [200]  (n>0)\n  --Eft <x> : tail mass for Forward exponential tail tau fit  [0.04]  (0<x<1)\n\nDebugging :\n  --stall    : arrest after start: for debugging MPI under gdb\n  --seed <n> : set random number seed to <n>  [0]\n\nExperiments :\n  --bgflat           : set uniform background frequencies\n  --bgcomp           : set bg frequencies to models average composition\n  --x-no-lengthmodel : turn the H3 length model off\n  --nu <x>           : set nu parameter (# expected HSPs) for GMSV  [2.0]\n  --pthresh <x>      : set P-value threshold for --ffile  [0.02]\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.11.2"}, {"role": "assistant", "content":  "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name). Use multiple times\n                        for each different attribute\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS, --samout SAMOUTS\n                        write out all SAM alignment records into SAM files\n                        (one per input file needed), annotating each line with\n                        its feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.11.2.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.9.1"}, {"role": "assistant", "content":  "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR [ADDITIONAL_ATTR ...]\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name)\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS [SAMOUTS ...], --samout SAMOUTS [SAMOUTS ...]\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.9.1.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.7.2"}, {"role": "assistant", "content":  "Usage: htseq-count [options] alignment_file gff_file\n\nThis script takes an alignment file in SAM/BAM format and a feature file in\nGFF format and calculates for each feature the number of reads mapping to it.\nSee http://www-huber.embl.de/users/anders/HTSeq/doc/count.html for details.\n\nOptions:\n  -h, --help            show this help message and exit\n  -f SAMTYPE, --format=SAMTYPE\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r ORDER, --order=ORDER\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  -s STRANDED, --stranded=STRANDED\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual=MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type=FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr=IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  -m MODE, --mode=MODE  mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  -o SAMOUT, --samout=SAMOUT\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.7.2.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.11.1"}, {"role": "assistant", "content":  "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name). Use multiple times\n                        for each different attribute\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS [SAMOUTS ...], --samout SAMOUTS [SAMOUTS ...]\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.11.1.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.11.0"}, {"role": "assistant", "content":  "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR [ADDITIONAL_ATTR ...]\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name)\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS [SAMOUTS ...], --samout SAMOUTS [SAMOUTS ...]\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.11.0.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.6.1.post1"}, {"role": "assistant", "content":  "Usage: htseq-count [options] alignment_file gff_file\n\nThis script takes an alignment file in SAM/BAM format and a feature file in\nGFF format and calculates for each feature the number of reads mapping to it.\nSee http://www-huber.embl.de/users/anders/HTSeq/doc/count.html for details.\n\nOptions:\n  -h, --help            show this help message and exit\n  -f SAMTYPE, --format=SAMTYPE\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r ORDER, --order=ORDER\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  -s STRANDED, --stranded=STRANDED\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual=MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type=FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr=IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  -m MODE, --mode=MODE  mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  -o SAMOUT, --samout=SAMOUT\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.6.1p1.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.6.1"}, {"role": "assistant", "content":  "Usage: htseq-count [options] alignment_file gff_file\n\nThis script takes an alignment file in SAM/BAM format and a feature file in\nGFF format and calculates for each feature the number of reads mapping to it.\nSee http://www-huber.embl.de/users/anders/HTSeq/doc/count.html for details.\n\nOptions:\n  -h, --help            show this help message and exit\n  -f SAMTYPE, --format=SAMTYPE\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r ORDER, --order=ORDER\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  -s STRANDED, --stranded=STRANDED\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual=MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type=FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr=IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  -m MODE, --mode=MODE  mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  -o SAMOUT, --samout=SAMOUT\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.6.0.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-v0.6.1p1-4-deb-py2"}, {"role": "assistant", "content":  "Usage: htseq-count [options] alignment_file gff_file\n\nThis script takes an alignment file in SAM/BAM format and a feature file in\nGFF format and calculates for each feature the number of reads mapping to it.\nSee http://www-huber.embl.de/users/anders/HTSeq/doc/count.html for details.\n\nOptions:\n  -h, --help            show this help message and exit\n  -f SAMTYPE, --format=SAMTYPE\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r ORDER, --order=ORDER\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  -s STRANDED, --stranded=STRANDED\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual=MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type=FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr=IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  -m MODE, --mode=MODE  mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  -o SAMOUT, --samout=SAMOUT\n                        write out all SAM alignment records into an output SAM\n                        file called SAMOUT, annotating each line with its\n                        feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.6.1p1.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-v0.11.2-1-deb-py3"}, {"role": "assistant", "content":  "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        skip all reads with alignment quality lower than the\n                        given minimum value (default: 10)\n  -t FEATURETYPE, --type FEATURETYPE\n                        feature type (3rd column in GFF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GFF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name). Use multiple times\n                        for each different attribute\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS, --samout SAMOUTS\n                        write out all SAM alignment records into SAM files\n                        (one per input file needed), annotating each line with\n                        its feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           suppress progress report\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL). (c) 2010. Released under the terms of the GNU General\nPublic License v3. Part of the \\'HTSeq\\'framework, version 0.11.2.\n"}]}
{"messages": [{"role": "user", "content":   "htseq-0.11.3"}, {"role": "assistant", "content":  "usage: htseq-count [options] alignment_file gff_file\n\nThis script takes one or more alignment files in SAM/BAM format and a feature\nfile in GFF format and calculates for each feature the number of reads mapping\nto it. See http://htseq.readthedocs.io/en/master/count.html for details.\n\npositional arguments:\n  samfilenames          Path to the SAM/BAM files containing the mapped reads.\n                        If \\'-\\'is selected, read from standard input\n  featuresfilename      Path to the GTF file containing the features\n\noptional arguments:\n  -h, --help            show this help message and exit\n  -f {sam,bam}, --format {sam,bam}\n                        Type of <alignment_file> data, either \\'sam\\'or \\'bam\\'\n                        (default: sam)\n  -r {pos,name}, --order {pos,name}\n                        \\'pos\\'or \\'name\\'. Sorting order of <alignment_file>\n                        (default: name). Paired-end sequencing data must be\n                        sorted either by position or by read name, and the\n                        sorting order must be specified. Ignored for single-\n                        end data.\n  --max-reads-in-buffer MAX_BUFFER_SIZE\n                        When <alignment_file> is paired end sorted by\n                        position, allow only so many reads to stay in memory\n                        until the mates are found (raising this number will\n                        use more memory). Has no effect for single end or\n                        paired end sorted by name\n  -s {yes,no,reverse}, --stranded {yes,no,reverse}\n                        Whether the data is from a strand-specific assay.\n                        Specify \\'yes\\', \\'no\\', or \\'reverse\\'(default: yes).\n                        \\'reverse\\'means \\'yes\\'with reversed strand\n                        interpretation\n  -a MINAQUAL, --minaqual MINAQUAL\n                        Skip all reads with MAPQ alignment quality lower than\n                        the given minimum value (default: 10). MAPQ is the 5th\n                        column of a SAM/BAM file and its usage depends on the\n                        software used to map the reads.\n  -t FEATURETYPE, --type FEATURETYPE\n                        Feature type (3rd column in GTF file) to be used, all\n                        features of other type are ignored (default, suitable\n                        for Ensembl GTF files: exon)\n  -i IDATTR, --idattr IDATTR\n                        GTF attribute to be used as feature ID (default,\n                        suitable for Ensembl GTF files: gene_id)\n  --additional-attr ADDITIONAL_ATTR\n                        Additional feature attributes (default: none, suitable\n                        for Ensembl GTF files: gene_name). Use multiple times\n                        for each different attribute\n  -m {union,intersection-strict,intersection-nonempty}, --mode {union,intersection-strict,intersection-nonempty}\n                        Mode to handle reads overlapping more than one feature\n                        (choices: union, intersection-strict, intersection-\n                        nonempty; default: union)\n  --nonunique {none,all}\n                        Whether to score reads that are not uniquely aligned\n                        or ambiguously assigned to features\n  --secondary-alignments {score,ignore}\n                        Whether to score secondary alignments (0x100 flag)\n  --supplementary-alignments {score,ignore}\n                        Whether to score supplementary alignments (0x800 flag)\n  -o SAMOUTS, --samout SAMOUTS\n                        Write out all SAM alignment records into SAM files\n                        (one per input file needed), annotating each line with\n                        its feature assignment (as an optional field with tag\n                        \\'XF\\')\n  -q, --quiet           Suppress progress report\n  --version             Show software version and exit\n\nWritten by Simon Anders (sanders@fs.tum.de), European Molecular Biology\nLaboratory (EMBL) and Fabio Zanini (fabio.zanini@stanford.edu), Stanford\nUniversity. (c) 2010-2019. Released under the terms of the GNU General Public\nLicense v3. Part of the \\'HTSeq\\'framework, version 0.11.3.\n"}]}
{"messages": [{"role": "user", "content":   "macs-v2.1.1.20160309-1-deb"}, {"role": "assistant", "content":  "usage: macs2 [-h] [--version]\n             {callpeak,bdgpeakcall,bdgbroadcall,bdgcmp,bdgopt,cmbreps,bdgdiff,filterdup,predictd,pileup,randsample,refinepeak}\n             ...\n\nmacs2 -- Model-based Analysis for ChIP-Sequencing\n\npositional arguments:\n  {callpeak,bdgpeakcall,bdgbroadcall,bdgcmp,bdgopt,cmbreps,bdgdiff,filterdup,predictd,pileup,randsample,refinepeak}\n    callpeak            Main MACS2 Function: Call peaks from alignment\n                        results.\n    bdgpeakcall         Call peaks from bedGraph output. Note: All regions on\n                        the same chromosome in the bedGraph file should be\n                        continuous so only bedGraph files from MACS2 are\n                        accpetable.\n    bdgbroadcall        Call broad peaks from bedGraph output. Note: All\n                        regions on the same chromosome in the bedGraph file\n                        should be continuous so only bedGraph files from MACS2\n                        are accpetable.\n    bdgcmp              Deduct noise by comparing two signal tracks in\n                        bedGraph. Note: All regions on the same chromosome in\n                        the bedGraph file should be continuous so only\n                        bedGraph files from MACS2 are accpetable.\n    bdgopt              Operations on score column of bedGraph file. Note: All\n                        regions on the same chromosome in the bedGraph file\n                        should be continuous so only bedGraph files from MACS2\n                        are accpetable.\n    cmbreps             Combine BEDGraphs of scores from replicates. Note: All\n                        regions on the same chromosome in the bedGraph file\n                        should be continuous so only bedGraph files from MACS2\n                        are accpetable.\n    bdgdiff             Differential peak detection based on paired four\n                        bedgraph files. Note: All regions on the same\n                        chromosome in the bedGraph file should be continuous\n                        so only bedGraph files from MACS2 are accpetable.\n    filterdup           Remove duplicate reads at the same position, then\n                        convert acceptable format to BED format.\n    predictd            Predict d or fragment size from alignment results.\n                        *Will NOT filter duplicates*\n    pileup              Pileup aligned reads with a given extension size\n                        (fragment size or d in MACS language). Note there will\n                        be no step for duplicate reads filtering or sequencing\n                        depth scaling, so you may need to do certain pre/post-\n                        processing.\n    randsample          Randomly sample number/percentage of total reads.\n    refinepeak          (Experimental) Take raw reads alignment, refine peak\n                        summits and give scores measuring balance of\n                        waston/crick tags. Inspired by SPP.\n\noptional arguments:\n  -h, --help            show this help message and exit\n  --version             show program\\'s version number and exit\n\nFor command line options of each command, type: macs2 COMMAND -h\n"}]}
{"messages": [{"role": "user", "content":   "macs-1.4.3"}, {"role": "assistant", "content":  "\nUsage: macs <-t tfile> [-n name] [-g genomesize] [options]\n\nExample: macs -t ChIP.bam -c Control.bam -f BAM -g h -n test -w --call-subpeaks\n\n\nmacs -- Model-based Analysis for ChIP-Sequencing\n\nOptions:\n  --version             show program\\'s version number and exit\n  -h, --help            show this help message and exit.\n  -t TFILE, --treatment=TFILE\n                        ChIP-seq treatment files. REQUIRED. When ELANDMULTIPET\n                        is selected, you must provide two files separated by\n                        comma, e.g.\n                        s_1_1_eland_multi.txt,s_1_2_eland_multi.txt\n  -c CFILE, --control=CFILE\n                        Control files. When ELANDMULTIPET is selected, you\n                        must provide two files separated by comma, e.g.\n                        s_2_1_eland_multi.txt,s_2_2_eland_multi.txt\n  -O OUTDIR, --outdir=OUTDIR\n                        Optional output directory name. DEFAULT: current\n                        working directory \".\"\n  -n NAME, --name=NAME  Experiment name, which will be used to generate output\n                        file names. DEFAULT: \"NA\"\n  -f FORMAT, --format=FORMAT\n                        Format of tag file, \"AUTO\", \"BED\" or \"ELAND\" or\n                        \"ELANDMULTI\" or \"ELANDMULTIPET\" or \"ELANDEXPORT\" or\n                        \"SAM\" or \"BAM\" or \"BOWTIE\". The default AUTO option\n                        will let MACS decide which format the file is. Please\n                        check the definition in 00README file if you choose EL\n                        AND/ELANDMULTI/ELANDMULTIPET/ELANDEXPORT/SAM/BAM/BOWTIE\n                        E. DEFAULT: \"AUTO\"\n  --petdist=PETDIST     Best distance between Pair-End Tags. Only available\n                        when format is \\'ELANDMULTIPET\\'. DEFAULT: 200\n  -g GSIZE, --gsize=GSIZE\n                        Effective genome size. It can be 1.0e+9 or 1000000000,\n                        or shortcuts:\\'hs\\'for human (2.7e9), \\'mm\\'for mouse\n                        (1.87e9), \\'ce\\'for C. elegans (9e7) and \\'dm\\'for\n                        fruitfly (1.2e8), Default:hs\n  -s TSIZE, --tsize=TSIZE\n                        Tag size. This will overide the auto detected tag\n                        size.\n  --bw=BW               Band width. This value is only used while building the\n                        shifting model. DEFAULT: 300\n  -p PVALUE, --pvalue=PVALUE\n                        Pvalue cutoff for peak detection. DEFAULT: 1e-5\n  -m MFOLD, --mfold=MFOLD\n                        Select the regions within MFOLD range of high-\n                        confidence enrichment ratio against background to\n                        build model. The regions must be lower than upper\n                        limit, and higher than the lower limit. DEFAULT:10,30\n  --nolambda            If True, MACS will use fixed background lambda as\n                        local lambda for every peak region. Normally, MACS\n                        calculates a dynamic local lambda to reflect the local\n                        bias due to potential chromatin structure.\n  --slocal=SMALLLOCAL   The small nearby region in basepairs to calculate\n                        dynamic lambda. This is used to capture the bias near\n                        the peak summit region. Invalid if there is no control\n                        data. DEFAULT: 1000\n  --llocal=LARGELOCAL   The large nearby region in basepairs to calculate\n                        dynamic lambda. This is used to capture the surround\n                        bias. DEFAULT: 10000.\n  --on-auto             Whether turn on the auto pair model process. If set,\n                        when MACS failed to build paired model, it will use\n                        the nomodel settings, the \\'--shiftsize\\'parameter to\n                        shift and extend each tags. DEFAULT: False\n  --nomodel             Whether or not to build the shifting model. If True,\n                        MACS will not build model. by default it means\n                        shifting size = 100, try to set shiftsize to change\n                        it. DEFAULT: False\n  --shiftsize=SHIFTSIZE\n                        The arbitrary shift size in bp. When nomodel is true,\n                        MACS will use this value as 1/2 of fragment size.\n                        DEFAULT: 100\n  --keep-dup=KEEPDUPLICATES\n                        It controls the MACS behavior towards duplicate tags\n                        at the exact same location -- the same coordination\n                        and the same strand. The \\'auto\\'option makes MACS\n                        calculate the maximum tags at the exact same location\n                        based on binomal distribution using 1e-5 as pvalue\n                        cutoff; and the \\'all\\'option keeps every tags. If an\n                        integer is given, at most this number of tags will be\n                        kept at the same location. Default: 1. To only keep\n                        one performs the best in terms of detecting enriched\n                        regions, from our internal study.\n  --to-large            When set, scale the small sample up to the bigger\n                        sample. By default, the bigger dataset will be scaled\n                        down towards the smaller dataset, which will lead to\n                        smaller pvalues and more specific results. Keep in\n                        mind that scaling down will bring down background\n                        noise more. DEFAULT: False\n  -w, --wig             Whether or not to save extended fragment pileup at\n                        every WIGEXTEND bps into a wiggle file. When --single-\n                        profile is on, only one file for the whole genome is\n                        saved. WARNING: this process is time/space consuming!!\n  -B, --bdg             Whether or not to save extended fragment pileup at\n                        every bp into a bedGraph file. When it\\'s on, -w,\n                        --space and --call-subpeaks will be ignored. When\n                        --single-profile is on, only one file for the whole\n                        genome is saved. WARNING: this process is time/space\n                        consuming!!\n  -S, --single-profile  When set, a single wiggle file will be saved for\n                        treatment and input. Default: False\n  --space=SPACE         The resoluation for saving wiggle files, by default,\n                        MACS will save the raw tag count every 10 bps. Usable\n                        only with \\'--wig\\'option.\n  --call-subpeaks       If set, MACS will invoke Mali Salmon\\'s PeakSplitter\n                        soft through system call. If PeakSplitter can\\'t be\n                        found, an instruction will be shown for downloading\n                        and installing the PeakSplitter package. -w option\n                        needs to be on and -B should be off to let it work.\n                        DEFAULT: False\n  --verbose=VERBOSE     Set verbose level. 0: only show critical message, 1:\n                        show additional warning message, 2: show process\n                        information, 3: show debug messages. DEFAULT:2\n  --diag                Whether or not to produce a diagnosis report. It\\'s up\n                        to 9X time consuming. Please check 00README file for\n                        detail. DEFAULT: False\n  --fe-min=FEMIN        For diagnostics, min fold enrichment to consider.\n                        DEFAULT: 0\n  --fe-max=FEMAX        For diagnostics, max fold enrichment to consider.\n                        DEFAULT: maximum fold enrichment\n  --fe-step=FESTEP      For diagnostics, fold enrichment step.  DEFAULT: 20\nsh-5.0# macs -h\nUsage: macs <-t tfile> [-n name] [-g genomesize] [options]\n\nExample: macs -t ChIP.bam -c Control.bam -f BAM -g h -n test -w --call-subpeaks\n\n\nmacs -- Model-based Analysis for ChIP-Sequencing\n\nOptions:\n  --version             show program\\'s version number and exit\n  -h, --help            show this help message and exit.\n  -t TFILE, --treatment=TFILE\n                        ChIP-seq treatment files. REQUIRED. When ELANDMULTIPET\n                        is selected, you must provide two files separated by\n                        comma, e.g.\n                        s_1_1_eland_multi.txt,s_1_2_eland_multi.txt\n  -c CFILE, --control=CFILE\n                        Control files. When ELANDMULTIPET is selected, you\n                        must provide two files separated by comma, e.g.\n                        s_2_1_eland_multi.txt,s_2_2_eland_multi.txt\n  -O OUTDIR, --outdir=OUTDIR\n                        Optional output directory name. DEFAULT: current\n                        working directory \".\"\n  -n NAME, --name=NAME  Experiment name, which will be used to generate output\n                        file names. DEFAULT: \"NA\"\n  -f FORMAT, --format=FORMAT\n                        Format of tag file, \"AUTO\", \"BED\" or \"ELAND\" or\n                        \"ELANDMULTI\" or \"ELANDMULTIPET\" or \"ELANDEXPORT\" or\n                        \"SAM\" or \"BAM\" or \"BOWTIE\". The default AUTO option\n                        will let MACS decide which format the file is. Please\n                        check the definition in 00README file if you choose EL\n                        AND/ELANDMULTI/ELANDMULTIPET/ELANDEXPORT/SAM/BAM/BOWTI\n                        E. DEFAULT: \"AUTO\"\n  --petdist=PETDIST     Best distance between Pair-End Tags. Only available\n                        when format is \\'ELANDMULTIPET\\'. DEFAULT: 200\n  -g GSIZE, --gsize=GSIZE\n                        Effective genome size. It can be 1.0e+9 or 1000000000,\n                        or shortcuts:\\'hs\\'for human (2.7e9), \\'mm\\'for mouse\n                        (1.87e9), \\'ce\\'for C. elegans (9e7) and \\'dm\\'for\n                        fruitfly (1.2e8), Default:hs\n  -s TSIZE, --tsize=TSIZE\n                        Tag size. This will overide the auto detected tag\n                        size.\n  --bw=BW               Band width. This value is only used while building the\n                        shifting model. DEFAULT: 300\n  -p PVALUE, --pvalue=PVALUE\n                        Pvalue cutoff for peak detection. DEFAULT: 1e-5\n  -m MFOLD, --mfold=MFOLD\n                        Select the regions within MFOLD range of high-\n                        confidence enrichment ratio against background to\n                        build model. The regions must be lower than upper\n                        limit, and higher than the lower limit. DEFAULT:10,30\n  --nolambda            If True, MACS will use fixed background lambda as\n                        local lambda for every peak region. Normally, MACS\n                        calculates a dynamic local lambda to reflect the local\n                        bias due to potential chromatin structure.\n  --slocal=SMALLLOCAL   The small nearby region in basepairs to calculate\n                        dynamic lambda. This is used to capture the bias near\n                        the peak summit region. Invalid if there is no control\n                        data. DEFAULT: 1000\n  --llocal=LARGELOCAL   The large nearby region in basepairs to calculate\n                        dynamic lambda. This is used to capture the surround\n                        bias. DEFAULT: 10000.\n  --on-auto             Whether turn on the auto pair model process. If set,\n                        when MACS failed to build paired model, it will use\n                        the nomodel settings, the \\'--shiftsize\\'parameter to\n                        shift and extend each tags. DEFAULT: False\n  --nomodel             Whether or not to build the shifting model. If True,\n                        MACS will not build model. by default it means\n                        shifting size = 100, try to set shiftsize to change\n                        it. DEFAULT: False\n  --shiftsize=SHIFTSIZE\n                        The arbitrary shift size in bp. When nomodel is true,\n                        MACS will use this value as 1/2 of fragment size.\n                        DEFAULT: 100\n  --keep-dup=KEEPDUPLICATES\n                        It controls the MACS behavior towards duplicate tags\n                        at the exact same location -- the same coordination\n                        and the same strand. The \\'auto\\'option makes MACS\n                        calculate the maximum tags at the exact same location\n                        based on binomal distribution using 1e-5 as pvalue\n                        cutoff; and the \\'all\\'option keeps every tags. If an\n                        integer is given, at most this number of tags will be\n                        kept at the same location. Default: 1. To only keep\n                        one performs the best in terms of detecting enriched\n                        regions, from our internal study.\n  --to-large            When set, scale the small sample up to the bigger\n                        sample. By default, the bigger dataset will be scaled\n                        down towards the smaller dataset, which will lead to\n                        smaller pvalues and more specific results. Keep in\n                        mind that scaling down will bring down background\n                        noise more. DEFAULT: False\n  -w, --wig             Whether or not to save extended fragment pileup at\n                        every WIGEXTEND bps into a wiggle file. When --single-\n                        profile is on, only one file for the whole genome is\n                        saved. WARNING: this process is time/space consuming!!\n  -B, --bdg             Whether or not to save extended fragment pileup at\n                        every bp into a bedGraph file. When it\\'s on, -w,\n                        --space and --call-subpeaks will be ignored. When\n                        --single-profile is on, only one file for the whole\n                        genome is saved. WARNING: this process is time/space\n                        consuming!!\n  -S, --single-profile  When set, a single wiggle file will be saved for\n                        treatment and input. Default: False\n  --space=SPACE         The resoluation for saving wiggle files, by default,\n                        MACS will save the raw tag count every 10 bps. Usable\n                        only with \\'--wig\\'option.\n  --call-subpeaks       If set, MACS will invoke Mali Salmon\\'s PeakSplitter\n                        soft through system call. If PeakSplitter can\\'t be\n                        found, an instruction will be shown for downloading\n                        and installing the PeakSplitter package. -w option\n                        needs to be on and -B should be off to let it work.\n                        DEFAULT: False\n  --verbose=VERBOSE     Set verbose level. 0: only show critical message, 1:\n                        show additional warning message, 2: show process\n                        information, 3: show debug messages. DEFAULT:2\n  --diag                Whether or not to produce a diagnosis report. It\\'s up\n                        to 9X time consuming. Please check 00README file for\n                        detail. DEFAULT: False\n  --fe-min=FEMIN        For diagnostics, min fold enrichment to consider.\n                        DEFAULT: 0\n  --fe-max=FEMAX        For diagnostics, max fold enrichment to consider.\n                        DEFAULT: maximum fold enrichment\n  --fe-step=FESTEP      For diagnostics, fold enrichment step.  DEFAULT: 20\n"}]}
{"messages": [{"role": "user", "content": "picard-2.9.2"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mChecks if all read groups appear to come from the same individual\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36m<b>Designs oligonucleotide baits for hybrid selection reactions.</b>  \u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.20"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSamErrorMetrics                       \u001b[31m\u001b[36mProgram to collect error metrics on bases stratified in various ways.\u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.17.3"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.3"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.13"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.11.0"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.17.2"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.22"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSamErrorMetrics                       \u001b[31m\u001b[36mProgram to collect error metrics on bases stratified in various ways.\u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.0"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.15.0"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.5"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.6"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.14"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mAlpha Tools:                                     Tools that are currently UNSUPPORTED until further testing and maturation.\u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFingerprinting Tools:                            Tools for manipulating fingerprints, or related data.\u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score.\u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks if all fingerprints appear to come from the same individual.\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mManipulates interval lists.  \u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another.  \u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting a reference by Ns.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplace read groups in a BAM file.\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the maq aligner.  \u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.  \u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file.  \u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubset read data from a SAM or BAM file\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate one or more BAM files as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.  \u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file.  \u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file.  \u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.  \u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mUnit Testing:                                    Unit testing                                 \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mEvaluate genotype concordance between callsets.\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.  \u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-1.141"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mFasta:                                           Tools for manipulating FASTA, or related data.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a SAM or BAM file from reference sequence in fasta format\u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mExtracts intervals from a reference sequence, writing them to a FASTA file\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a fasta file to be of the same length\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIllumina Tools:                                  Tools for manipulating data specific to Illumina sequencers.\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity of the data in the specified Illumina basecalling data\u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mGiven an Illumina basecalling and a lane, produces per-lane-barcode basecalling metrics\u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given basecalling analysis directory\u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool to determine the barcode for each read in an Illumina lane\u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate fastq file(s) from data in an Illumina basecalls output directory\u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mGenerate a SAM or BAM file from data in an Illumina basecalls output directory\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mInterval Tools:                                  Tools for manipulating Picard interval lists.\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to an Picard Interval List.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mGeneral tool for manipulating interval lists\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list based on splitting the reference by Ns\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMetrics:                                         Tools for reporting metrics on various data types.\u001b[0m\n\u001b[32m    CalculateHsMetrics                           \u001b[36mCalculates Hybrid Selection-specific metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36mProduces a file containing summary alignment metrics from a SAM or BAM.\u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mProgram to chart the nucleotide distribution per cycle in a SAM or BAM file.\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollects information about GC bias in the reads in the provided SAM or BAM\u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mWrites insert size distribution metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mProduces jumping library metrics for the provided SAM/BAMs\u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mA \"meta-metrics\" calculating program that produces multiple metrics for the provided SAM/BAM\u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollects metrics quantifying the CpCG -> CpCA error rate from the provided SAM/BAM\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollects a set of metrics that quantify the quality and yield of sequence data from the provided SAM/BAM\u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mWrites whole genome sequencing-related metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36mCollects metrics about bisulfite conversion for RRBS data\u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.\u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mProduces Targeted PCR-related metrics given the provided SAM/BAM\u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mWrites whole genome sequencing-related metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CollectWgsMetricsFromQuerySorted             \u001b[36mWrites sequencing-related metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CollectWgsMetricsFromSampledSites            \u001b[36mWrites whole genome sequencing-related metrics for a SAM or BAM file\u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files\u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics format from generalized artifact metrics.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates library complexity from the sequence of read pairs\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mWrites mean quality by cycle for a SAM or BAM file\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mCharts quality score distributions for a SAM or BAM file\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mMiscellaneous Tools:                             A set of miscellaneous tools.                \u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns baits or oligos for hybrid selection reactions.\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mFIFO buffer used to buffer input and output streams with a customizable buffer size \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mSAM/BAM:                                         Tools for manipulating SAM, BAM, or related data.\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mReplaces read groups in a BAM or SAM file with a single new read group\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerates index statistics from a BAM file\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mCreate BFQ files from a BAM file for use by the Maq aligner.\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index (.bai) file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG) in the SAM or BAM header.\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompares two input SAM or BAM files\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDown-sample a SAM or BAM file to retain a random subset of the reads\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a fastq file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mCreates a new SAM or BAM file by including or excluding aligned reads\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mEnsure that all mate-pair information is in sync between each read and its mate pair\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenates one or more BAM files together as efficiently as possible\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerges alignment data from a SAM or BAM with data in an unmapped BAM file\u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM or BAM files into one file\u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in reference\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplace the SAMFileHeader in a SAM file with the given header\u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state\u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM/BAM into a FASTQ\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVCF/BCF:                                         Tools for manipulating VCF, BCF, or related data.\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data for two samples in two different VCFs\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts a VCF between genome builds.\u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF bereft of genotype information from an input VCF or BCF\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mMerges multiple VCF or BCF files into one VCF file or BCF\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRename a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files\u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits an input VCF or BCF file into two VCF or BCF files\u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts a VCF file to a BCF file, or BCF to VCF\u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.17.4"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.1"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.17.11"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.18.12"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[31m\u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[31m\u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[31m\u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[31m\u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[31m\u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[31m\u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[31m\u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[31m\u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[31m\u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[31m\u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[31m\u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[31m\u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[31m\u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[31m\u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[31m\u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[31m\u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[31m\u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[31m\u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mEstimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[31m\u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[31m\u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[31m\u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[31m\u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[31m\u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[31m\u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[31m\u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[31m\u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSamErrorMetrics                       \u001b[31m\u001b[36mProgram to collect error metrics on bases stratified in various ways.\u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[31m\u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[31m\u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[31m\u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[31m\u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[31m\u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[31m\u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[31m\u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[31m\u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[31m\u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[31m\u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    IdentifyContaminant                          \u001b[31m\u001b[36mComputes a fingerprint from the supplied SAM/BAM file, given a contamination estimate.\u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[31m\u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[31m\u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[31m\u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[31m\u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[31m\u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[31m\u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[31m\u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[31m\u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[31m\u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[31m\u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[31m\u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[31m\u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[31m\u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[31m\u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[31m\u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[31m\u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[31m\u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[31m\u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[31m\u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[31m\u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[31m\u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[31m\u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[31m\u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[31m\u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[31m\u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[31m\u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[31m\u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[31m\u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[31m\u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SamToFastqWithTags                           \u001b[31m\u001b[36mConverts a SAM or BAM file to FASTQ alongside FASTQs created from tags.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[31m\u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[31m\u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mExamines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[31m\u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[31m\u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[31m\u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[31m**EXPERIMENTAL FEATURE - USE AT YOUR OWN RISK** \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[31m\u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[31m\u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[31m\u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[31m\u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[31m\u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[31m\u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[31m\u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[31m\u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[31m\u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[31m\u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[31m\u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[31m\u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[31m\u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MakeVcfSampleNameMap                         \u001b[31m\u001b[36mCreates a TSV from sample name to VCF/GVCF path, with one line per input.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[31m\u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[31m\u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[31m\u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[31m\u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[31m\u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[31m\u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[31m\u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content": "picard-2.17.10"}, {"role": "assistant", "content": "/usr/local/bin/picard: line 5: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\n\u001b[1m\u001b[31mUSAGE: PicardCommandLine \u001b[32m<program name>\u001b[1m\u001b[31m [-h]\n\n\u001b[0m\u001b[1m\u001b[31mAvailable Programs:\n\u001b[0m\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mBase Calling:                                    Tools that process sequencing machine data, e.g. Illumina base calls, and detect sequencing level attributes, e.g. adapters\u001b[0m\n\u001b[32m    CheckIlluminaDirectory                       \u001b[36mAsserts the validity for specified Illumina basecalling data.  \u001b[0m\n\u001b[32m    CollectIlluminaBasecallingMetrics            \u001b[36mCollects Illumina Basecalling metrics for a sequencing run.  \u001b[0m\n\u001b[32m    CollectIlluminaLaneMetrics                   \u001b[36mCollects Illumina lane metrics for the given BaseCalling analysis directory.  \u001b[0m\n\u001b[32m    ExtractIlluminaBarcodes                      \u001b[36mTool determines the barcode for each read in an Illumina lane.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToFastq                     \u001b[36mGenerate FASTQ file(s) from Illumina basecall read data.  \u001b[0m\n\u001b[32m    IlluminaBasecallsToSam                       \u001b[36mTransforms raw Illumina sequencing data into an unmapped SAM or BAM file.\u001b[0m\n\u001b[32m    MarkIlluminaAdapters                         \u001b[36mReads a SAM or BAM file and rewrites it with new adapter-trimming tags.  \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mDiagnostics and Quality Control:                 Tools that collect sequencing quality related and comparative metrics\u001b[0m\n\u001b[32m    AccumulateVariantCallingMetrics              \u001b[36mCombines multiple Variant Calling Metrics files into a single file\u001b[0m\n\u001b[32m    BamIndexStats                                \u001b[36mGenerate index statistics from a BAM file\u001b[0m\n\u001b[32m    CalculateReadGroupChecksum                   \u001b[36mCreates a hash code based on the read groups (RG).  \u001b[0m\n\u001b[32m    CheckFingerprint                             \u001b[36mComputes a fingerprint from the supplied input (SAM/BAM or VCF) file and compares it to the provided genotypes\u001b[0m\n\u001b[32m    CheckTerminatorBlock                         \u001b[36mAsserts the provided gzip file\\'s (e.g., BAM) last block is well-formed; RC 100 otherwise\u001b[0m\n\u001b[32m    ClusterCrosscheckMetrics                     \u001b[36mClusters the results of a CrosscheckFingerprints run by LOD score\u001b[0m\n\u001b[32m    CollectAlignmentSummaryMetrics               \u001b[36m<b>Produces a summary of alignment metrics from a SAM or BAM file.</b>  \u001b[0m\n\u001b[32m    CollectBaseDistributionByCycle               \u001b[36mChart the nucleotide distribution per cycle in a SAM or BAM file\u001b[0m\n\u001b[32m    CollectGcBiasMetrics                         \u001b[36mCollect metrics regarding GC bias. \u001b[0m\n\u001b[32m    CollectHiSeqXPfFailMetrics                   \u001b[36mClassify PF-Failing reads in a HiSeqX Illumina Basecalling directory into various categories.\u001b[0m\n\u001b[32m    CollectHsMetrics                             \u001b[36mCollects hybrid-selection (HS) metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectIndependentReplicateMetrics           \u001b[36m(Experimental) Estimates the rate of independent replication of reads within a bam.\u001b[0m\n\u001b[32m    CollectInsertSizeMetrics                     \u001b[36mCollect metrics about the insert size distribution of a paired-end library.\u001b[0m\n\u001b[32m    CollectJumpingLibraryMetrics                 \u001b[36mCollect jumping library metrics. \u001b[0m\n\u001b[32m    CollectMultipleMetrics                       \u001b[36mCollect multiple classes of metrics.  \u001b[0m\n\u001b[32m    CollectOxoGMetrics                           \u001b[36mCollect metrics to assess oxidative artifacts.\u001b[0m\n\u001b[32m    CollectQualityYieldMetrics                   \u001b[36mCollect metrics about reads that pass quality thresholds and Illumina-specific filters.  \u001b[0m\n\u001b[32m    CollectRawWgsMetrics                         \u001b[36mCollect whole genome sequencing-related metrics.  \u001b[0m\n\u001b[32m    CollectRnaSeqMetrics                         \u001b[36mProduces RNA alignment metrics for a SAM or BAM file.  \u001b[0m\n\u001b[32m    CollectRrbsMetrics                           \u001b[36m<b>Collects metrics from reduced representation bisulfite sequencing (Rrbs) data.</b>  \u001b[0m\n\u001b[32m    CollectSequencingArtifactMetrics             \u001b[36mCollect metrics to quantify single-base sequencing artifacts.  \u001b[0m\n\u001b[32m    CollectTargetedPcrMetrics                    \u001b[36mCalculate PCR-related metrics from targeted sequencing data. \u001b[0m\n\u001b[32m    CollectVariantCallingMetrics                 \u001b[36mCollects per-sample and aggregate (spanning all samples) metrics from the provided VCF file\u001b[0m\n\u001b[32m    CollectWgsMetrics                            \u001b[36mCollect metrics about coverage and performance of whole genome sequencing (WGS) experiments.\u001b[0m\n\u001b[32m    CollectWgsMetricsWithNonZeroCoverage         \u001b[36m(Experimental) Collect metrics about coverage and performance of whole genome sequencing (WGS) experiments.  \u001b[0m\n\u001b[32m    CompareMetrics                               \u001b[36mCompare two metrics files.\u001b[0m\n\u001b[32m    CompareSAMs                                  \u001b[36mCompare two input \".sam\" or \".bam\" files.  \u001b[0m\n\u001b[32m    ConvertSequencingArtifactToOxoG              \u001b[36mExtract OxoG metrics from generalized artifacts metrics.  \u001b[0m\n\u001b[32m    CrosscheckFingerprints                       \u001b[36mChecks that all data in the input files appear to have come from the same individual\u001b[0m\n\u001b[32m    CrosscheckReadGroupFingerprints              \u001b[36mDEPRECATED: USE CrosscheckFingerprints. Checks if all read groups appear to come from the same individual.\u001b[0m\n\u001b[32m    EstimateLibraryComplexity                    \u001b[36mEstimates the numbers of unique molecules in a sequencing library.  \u001b[0m\n\u001b[32m    MeanQualityByCycle                           \u001b[36mCollect mean quality by cycle.\u001b[0m\n\u001b[32m    QualityScoreDistribution                     \u001b[36mChart the distribution of quality scores.  \u001b[0m\n\u001b[32m    ValidateSamFile                              \u001b[36mValidates a SAM or BAM file.\u001b[0m\n\u001b[32m    ViewSam                                      \u001b[36mPrints a SAM or BAM file to the screen\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mIntervals Manipulation:                          Tools that process genomic intervals in various formats\u001b[0m\n\u001b[32m    BedToIntervalList                            \u001b[36mConverts a BED file to a Picard Interval List.  \u001b[0m\n\u001b[32m    IntervalListToBed                            \u001b[36mConverts an Picard IntervalList file to a BED file.\u001b[0m\n\u001b[32m    IntervalListTools                            \u001b[36mA tool for performing various IntervalList manipulations\u001b[0m\n\u001b[32m    LiftOverIntervalList                         \u001b[36mLifts over an interval list from one reference build to another. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mOther:                                           Miscellaneous tools, e.g. those that aid in data streaming\u001b[0m\n\u001b[32m    FifoBuffer                                   \u001b[36mProvides a large, FIFO buffer that can be used to buffer input and output streams between programs.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mRead Data Manipulation:                          Tools that manipulate read data in SAM, BAM or CRAM format\u001b[0m\n\u001b[32m    AddCommentsToBam                             \u001b[36mAdds comments to the header of a BAM file.\u001b[0m\n\u001b[32m    AddOrReplaceReadGroups                       \u001b[36mAssigns all the reads in a file to a single new read-group.\u001b[0m\n\u001b[32m    BamToBfq                                     \u001b[36mConverts a BAM file into a BFQ (binary fastq formatted) file\u001b[0m\n\u001b[32m    BuildBamIndex                                \u001b[36mGenerates a BAM index \".bai\" file.  \u001b[0m\n\u001b[32m    CleanSam                                     \u001b[36mCleans the provided SAM/BAM, soft-clipping beyond-end-of-reference alignments and setting MAPQ to 0 for unmapped reads\u001b[0m\n\u001b[32m    DownsampleSam                                \u001b[36mDownsample a SAM or BAM file.\u001b[0m\n\u001b[32m    FastqToSam                                   \u001b[36mConverts a FASTQ file to an unaligned BAM or SAM file\u001b[0m\n\u001b[32m    FilterSamReads                               \u001b[36mSubsets reads from a SAM or BAM file by applying one of several filters.\u001b[0m\n\u001b[32m    FixMateInformation                           \u001b[36mVerify mate-pair information between mates and fix if needed.\u001b[0m\n\u001b[32m    GatherBamFiles                               \u001b[36mConcatenate efficiently BAM files that resulted from a scattered parallel analysis\u001b[0m\n\u001b[32m    MarkDuplicates                               \u001b[36mIdentifies duplicate reads.  \u001b[0m\n\u001b[32m    MarkDuplicatesWithMateCigar                  \u001b[36mIdentifies duplicate reads, accounting for mate CIGAR.  \u001b[0m\n\u001b[32m    MergeBamAlignment                            \u001b[36mMerge alignment data from a SAM or BAM with data in an unmapped BAM file.  \u001b[0m\n\u001b[32m    MergeSamFiles                                \u001b[36mMerges multiple SAM and/or BAM files into a single file.  \u001b[0m\n\u001b[32m    PositionBasedDownsampleSam                   \u001b[36mDownsample a SAM or BAM file to retain a subset of the reads based on the reads location in each tile in the flowcell.\u001b[0m\n\u001b[32m    ReorderSam                                   \u001b[36mReorders reads in a SAM or BAM file to match ordering in a second reference file.\u001b[0m\n\u001b[32m    ReplaceSamHeader                             \u001b[36mReplaces the SAMFileHeader in a SAM or BAM file.  \u001b[0m\n\u001b[32m    RevertOriginalBaseQualitiesAndAddMateCigar   \u001b[36mReverts the original base qualities and adds the mate cigar tag to read-group BAMs\u001b[0m\n\u001b[32m    RevertSam                                    \u001b[36mReverts SAM or BAM files to a previous state.  \u001b[0m\n\u001b[32m    SamFormatConverter                           \u001b[36mConvert a BAM file to a SAM file, or a SAM to a BAM\u001b[0m\n\u001b[32m    SamToFastq                                   \u001b[36mConverts a SAM or BAM file to FASTQ.\u001b[0m\n\u001b[32m    SetNmAndUqTags                               \u001b[36mDEPRECATED: Use SetNmMdAndUqTags instead.\u001b[0m\n\u001b[32m    SetNmMdAndUqTags                             \u001b[36mFixes the NM, MD, and UQ tags in a SAM file \u001b[0m\n\u001b[32m    SimpleMarkDuplicatesWithMateCigar            \u001b[36m(Experimental) Examines aligned records in the supplied SAM or BAM file to locate duplicate molecules.\u001b[0m\n\u001b[32m    SortSam                                      \u001b[36mSorts a SAM or BAM file\u001b[0m\n\u001b[32m    SplitSamByLibrary                            \u001b[36mSplits a SAM or BAM file into individual files by library\u001b[0m\n\u001b[32m    SplitSamByNumberOfReads                      \u001b[36mSplits a SAM or BAM file to multiple BAMs.\u001b[0m\n\u001b[32m    UmiAwareMarkDuplicatesWithMateCigar          \u001b[36mIdentifies duplicate reads using information from read positions and UMIs. \u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mReference:                                       Tools that analyze and manipulate FASTA format references\u001b[0m\n\u001b[32m    BaitDesigner                                 \u001b[36mDesigns oligonucleotide baits for hybrid selection reactions.\u001b[0m\n\u001b[32m    CreateSequenceDictionary                     \u001b[36mCreates a sequence dictionary for a reference sequence.  \u001b[0m\n\u001b[32m    ExtractSequences                             \u001b[36mSubsets intervals from a reference sequence to a new FASTA file.\u001b[0m\n\u001b[32m    NonNFastaSize                                \u001b[36mCounts the number of non-N bases in a fasta file.\u001b[0m\n\u001b[32m    NormalizeFasta                               \u001b[36mNormalizes lines of sequence in a FASTA file to be of the same length.\u001b[0m\n\u001b[32m    ScatterIntervalsByNs                         \u001b[36mWrites an interval list created by splitting a reference at Ns.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Evaluation and Refinement:               Tools that evaluate and refine variant calls, e.g. with annotations not offered by the engine\u001b[0m\n\u001b[32m    FindMendelianViolations                      \u001b[36mFinds mendelian violations of all types within a VCF\u001b[0m\n\u001b[32m    GenotypeConcordance                          \u001b[36mCalculates the concordance between genotype data of one samples in each of two VCFs - one  being considered the truth (or reference) the other being the call.  The concordance is broken into separate results sections for SNPs and indels.  Statistics are reported in three different files.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Filtering:                               Tools that filter variants by annotating the FILTER column\u001b[0m\n\u001b[32m    FilterVcf                                    \u001b[36mHard filters a VCF.\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\u001b[0m\u001b[31mVariant Manipulation:                            Tools that manipulate variant call format (VCF) data\u001b[0m\n\u001b[32m    FixVcfHeader                                 \u001b[36mReplaces or fixes a VCF header.\u001b[0m\n\u001b[32m    GatherVcfs                                   \u001b[36mGathers multiple VCF files from a scatter operation into a single VCF file\u001b[0m\n\u001b[32m    LiftoverVcf                                  \u001b[36mLifts over a VCF file from one reference build to another.  \u001b[0m\n\u001b[32m    MakeSitesOnlyVcf                             \u001b[36mCreates a VCF that contains all the site-level information for all records in the input VCF but no genotype information.\u001b[0m\n\u001b[32m    MergeVcfs                                    \u001b[36mCombines multiple variant files into a single variant file\u001b[0m\n\u001b[32m    RenameSampleInVcf                            \u001b[36mRenames a sample within a VCF or BCF.\u001b[0m\n\u001b[32m    SortVcf                                      \u001b[36mSorts one or more VCF files.  \u001b[0m\n\u001b[32m    SplitVcfs                                    \u001b[36mSplits SNPs and INDELs into separate files.  \u001b[0m\n\u001b[32m    UpdateVcfSequenceDictionary                  \u001b[36mTakes a VCF and a second file that contains a sequence dictionary and updates the VCF with the new sequence dictionary.\u001b[0m\n\u001b[32m    VcfFormatConverter                           \u001b[36mConverts VCF to BCF or BCF to VCF.  \u001b[0m\n\u001b[32m    VcfToIntervalList                            \u001b[36mConverts a VCF or BCF file to a Picard Interval List\u001b[0m\n\n\u001b[37m--------------------------------------------------------------------------------------\n\n\u001b[0m"}]}
{"messages": [{"role": "user", "content":   "plink-1.90b4"}, {"role": "assistant", "content":  "PLINK v1.90b4 64-bit (20 Mar 2017)             www.cog-genomics.org/plink/1.9/\n(C) 2005-2017 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * [square brackets] denote a required parameter, where the text between the\n    brackets describes its nature.\n  * <angle brackets> denote an optional modifier (or if \\'|\\'is present, a set\n    of mutually exclusive optional modifiers).  Use the EXACT text in the\n    definition, e.g. \\'--dummy acgt\\'.\n  * There\\'s one exception to the angle brackets/exact text rule: when an angle\n    bracket term ends with \\'=[value]\\', \\'[value]\\'designates a variable\n    parameter.\n  * {curly braces} denote an optional parameter, where the text between the\n    braces describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink [input flag(s)...] {command flag(s)...} {other flag(s)...}\n  plink --help {flag name(s)...}\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile {prefix} : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed [filename] : Specify full name of .bed file.\n  --bim [filename] : Specify full name of .bim file.\n  --fam [filename] : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file {prefix}  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped [filename] : Specify full name of .ped file.\n  --map [filename] : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile {prefix} : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped [fname]   : Specify full name of .tped file.\n  --tfam [fname]   : Specify full name of .tfam file.\n\n  --lfile {prefix} : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen [fname]   : Specify full name of .lgen file.\n  --reference [fn] : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf [filename] : Specify full name of .vcf or .vcf.gz file.\n  --bcf [filename] : Specify full name of BCF2 file.\n\n  --data {prefix}  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen [filename] : Specify full name of .gen or .gen.gz file.\n  --bgen [f] <snpid-chr> : Specify full name of .bgen file.\n  --sample [fname] : Specify full name of .sample file.\n\n  --23file [fname] {FID} {IID} {sex} {pheno} {pat. ID} {mat. ID} :\n    Specify 23andMe input file.\n\n  --grm-gz {prfx}  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin {prfx} : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy [sample ct] [SNP ct] {missing geno freq} {missing pheno freq}\n          <acgt | 1234 | 12> <scalar-pheno>\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate [simulation parameter file] <tags | haps> <acgt | 1234 | 12>\n  --simulate-qt [simulation parameter file] <tags | haps> <acgt | 1234 | 12>\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.{extension}\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out [prefix]   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode [output format] <01 | 12> <tab | tabx | spacex | bgz | gen-gz>\n           <include-alt> <omit-nonmale-y>\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-[chr #].recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-[chr #][.ped + .info]\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan <verbose>\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster <omit-unassigned>\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to {output prefix}.set, while --set-table writes a\n    variant-by-set membership table to {output prefix}.set.table.\n\n  --merge [.ped filename] [.map filename]\n  --merge [text fileset prefix]\n  --bmerge [.bed filename] [.bim filename] [.fam filename]\n  --bmerge [binary fileset prefix]\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to {output prefix}.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list [filename]\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars <require-same-ref> <ids-only> <suppress-first>\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq <counts | case-control> <gz>\n  --freqx <gz>\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing <gz>\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy <midp> <gz>\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel <summaries-only>\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het <small-sample> <gz>\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex {female max F} {male min F}\n  --check-sex ycount {female max F} {male min F} {female max Y obs}\n                     {male min Y obs}\n  --check-sex y-only {female max Y obs} {male min Y obs}\n  --impute-sex {female max F} {male min F}\n  --impute-sex ycount {female max F} {male min F} {female max Y obs}\n                      {male min Y obs}\n  --impute-sex y-only {female max Y obs} {male min Y obs}\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst <case-control>\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep [window size]<kb> [step size (variant ct)] [VIF threshold]\n  --indep-pairwise [window size]<kb> [step size (variant ct)] [r^2 threshold]\n  --indep-pairphase [window size]<kb> [step size (variant ct)] [r^2 threshold]\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \\'--indep-pairwise 500 kb 5 0.5\\'\n    and \\'--indep-pairwise 500kb 5 0.5\\'have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r <square | square0 | triangle | inter-chr> <gz | bin | bin4> <spaces>\n      <in-phase> <d | dprime | dprime-signed> <with-freqs> <yes-really>\n  --r2 <square | square0 | triangle | inter-chr> <gz | bin | bin4> <spaces>\n       <in-phase> <d | dprime | dprime-signed> <with-freqs> <yes-really>\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld [variant ID] [variant ID] <hwe-midp>\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags [filename]\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks <no-pheno-req> <no-small-max-span>\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \\'--recode beagle\\'to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance <square | square0 | triangle> <gz | bin | bin4> <ibs> <1-ibs>\n             <allele-ct> <flat-missing>\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to {output prefix}.dist, and a list of the\n    corresponding sample IDs to {output prefix}.dist.id.  The first row of the\n    .dist file contains a single {genome 1-genome 2} distance, the second row\n    has the {genome 1-genome 3} and {genome 2-genome 3} distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to {output prefix}.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to {output prefix}.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to {output prefix}.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \\'--distance 1-ibs flat-missing\n    square\\'and \\'--distance ibs flat-missing square\\', respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel <square | square0 | triangle> <gz | bin | bin4>\n             <cov | ibc2 | ibc3>\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to {output prefix}.rel, and corresponding IDs to {output prefix}.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz <no-gz> <cov | ibc2 | ibc3>\n  --make-grm-bin <cov | ibc2 | ibc3>\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff {val}\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    {output prefix}.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test {permutation count}\n  --groupdist {iters} {d}\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance {iters} {d}\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to {number of people}^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel {iters} {d}\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome <gz> <rel-check> <full> <unbounded> <nudge>\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg <group | group-verbose> <consensus-match> <extend>\n            <subtract-1-from-lengths>\n  --homozyg-snp [min var count]\n  --homozyg-kb [min length]\n  --homozyg-density [max inverse density (kb/var)]\n  --homozyg-gap [max internal gap kb length]\n  --homozyg-het [max hets]\n  --homozyg-window-snp [scanning window size]\n  --homozyg-window-het [max hets in scanning window hit]\n  --homozyg-window-missing [max missing calls in scanning window hit]\n  --homozyg-window-threshold [min scanning window hit rate]\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group{-verbose}\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group{-verbose}\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as [end bp position] -\n        [start bp position] + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster <cc> <group-avg | old-tiebreaks> <missing> <only2>\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca {count} <header> <tabs> <var-wts>\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour [n1] [n2]\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc <perm | mperm=[value]> <perm-count> <fisher | fisher-midp> <counts>\n          <set-test>\n  --assoc <perm | mperm=[value]> <perm-count> <qt-means> <lin> <set-test>\n  --model <perm | mperm=[value]> <perm-count>\n          <fisher | fisher-midp | trend-only> <set-test>\n          <dom | rec | gen | trend>\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=[value]\\'causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh <perm | mperm=[value]> <perm-count> <set-test>\n    (alias: --cmh)\n  --bd <perm | perm-bd | mperm=[value]> <perm-count> <set-test>\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe {covariate index}\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \\'--gxe 3\\'to base them on the third covariate instead.\n\n  --linear <perm | mperm=[value]> <perm-count> <set-test>\n           <genotypic | hethom | dominant | recessive | no-snp> <hide-covar>\n           <sex | no-x-sex> <interaction> <beta> <standard-beta> <intercept>\n  --logistic <perm | mperm=[value]> <perm-count> <set-test>\n             <genotypic | hethom | dominant | recessive | no-snp> <hide-covar>\n             <sex | no-x-sex> <interaction> <beta> <intercept>\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=[value]\\'starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage [allele dosage file] <noheader> <skip0=[i]> <skip1=[j]> <skip2=[k]>\n           <dose1> <format=[m]> <Zout> <occur | standard-beta> <sex>\n           <case-control-freqs>\n  --dosage [list file] list <sepheader | noheader> <skip0=[i]> <skip1=[j]>\n           <skip2=[k]> <dose1> <format=[m]> <Zout> <occur | standard-beta>\n           <sex> <case-control-freqs>\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modiifer.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso [h2 estimate] {min lambda} <report-zeroes>\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing <perm | mperm=[value]> <perm-count> <midp>\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  The \\'midp\\'modifier causes Lancaster\\'s mid-p\n    adjustment to be applied.\n\n  --make-perm-pheno [ct]\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt <exact | exact-midp | poo> <perm | mperm=[value]> <perm-count>\n        <parentdt1 | parentdt2 | pat | mat> <set-test>\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=[value]\\'requests a family-based adaptive or max(T)\n      permutation test.  By default, the permutation test statistic is the\n      basic TDT p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined\n      test p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam <perm | mperm=[value]> <perm-count> <emp-se>\n  --qfam-parents <perm | mperm=[value]> <perm-count> <emp-se>\n  --qfam-between <perm | mperm=[value]> <perm-count> <emp-se>\n  --qfam-total <perm | mperm=[value]> <perm-count> <emp-se>\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=[value]\\'just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate [PLINK report] <attrib=[file]> <ranges=[file]> <filter=[file]>\n             <snps=[file]> <NA | prune> <block> <subset=[file]> <minimal>\n             <distance>\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=[file]\\'specifies a (possibly gzipped) attribute file.\n    * \\'ranges=[file]\\'specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=[file]\\'causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=[file]\\'causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=[file]\\'causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump [PLINK report filename(s)...]\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report [PLINK report] [gene range file]\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis [PLINK report filenames...]\n  --meta-analysis [PLINK report filenames...] + <logscale | qt>\n                  <no-map | no-allele> <study> <report-all> <weighted-z>\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis <boost | joint-effects | no-ueki> <case-only>\n                   <set-by-set | set-by-all> <nop>\n  --epistasis <set-by-set | set-by-all>\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge [common file prefix] [ct]\n    When a --{fast-}epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus [variant ID] [variant ID]\n    Two-locus joint genotype count report.\n\n  --score [filename] {i} {j} {k} <header> <sum | no-sum>\n          <no-mean-imputation | center> <include-cnt> <double-dosage>\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --write-var-ranges [block ct]\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink2/order .)\n  --script [fname] : Include command-line options from file.\n  --rerun {log}    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype [char] : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid {ID}     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim {d}       : Parse sample IDs as [FID][d][IID] (default delim \\'_\\').\n  --vcf-idspace-to [c] : Convert spaces in sample IDs to the given character.\n  --biallelic-only <strict> <list> : Skip VCF variants with 2+ alt. alleles.\n  --vcf-min-qual [val]             : Skip VCF variants with low/missing QUAL.\n  --vcf-filter {exception(s)...}   : Skip variants which have FILTER failures.\n  --vcf-require-gt                 : Skip variants with no GT field.\n  --vcf-min-gq [val]               : No-call a genotype when GQ is below the\n                                     given threshold.\n  --vcf-min-gp [val]               : No-call a genotype when 0-1 scaled GP is\n                                     below the given threshold.\n  --vcf-half-call [m]  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr [chr nm] : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name [col nm] : Import named phenotype from the .sample file.\n  --hard-call-threshold [val]  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code {string list} : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases [num]   : Set --simulate case count (default 1000).\n  --simulate-ncontrols [n]  : Set --simulate control count (default 1000).\n  --simulate-prevalence [p] : Set --simulate disease prevalence (default 0.01).\n  --simulate-n [num]        : Set --simulate-qt sample count (default 1000).\n  --simulate-label [prefix] : Set --simulate{-qt} FID/IID name prefix.\n  --simulate-missing [freq] : Set --simulate{-qt} missing genotype frequency.\n  --allow-extra-chr <0>     : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set [autosome ct] <no-x> <no-y> <no-xy> <no-mt> :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num [value]    : Alias for \\'--chr-set [value] no-y no-xy no-mt\\'.\n  --cm-map [fname pattern] {chr} : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --pheno [fname]  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno [n]     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name [c] : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype [v] : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.\n  --make-pheno [fn] [val] : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno [Lt] {Hbt} : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar [filename] <keep-pheno-on-missing-cov> : Specify covariate file.\n  --covar-name [...]      : Specify covariate(s) in --covar file by name.\n                            Separate multiple names with spaces or commas, and\n                            use dashes to designate ranges.\n  --covar-number [...]    : Specify covariate(s) in --covar file by index.\n  --no-const-covar        : Exclude constant covariates.\n  --within [f] <keep-NA>  : Specify initial cluster assignments.\n  --mwithin [n]           : Load cluster assignments from column n+2.\n  --family                : Create a cluster for each family ID.\n  --loop-assoc [f] <keep-NA>    : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set [filename]              : Load sets from a .set file.\n  --set-names [name(s)...]      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset [filename]           : Load only sets named in the given text file.\n  --set-collapse-all [set name] : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all [s] : --set-collapse-all + inversion.\n  --make-set [filename]         : Define sets from a list of named bp ranges.\n  --make-set-border [kbs]       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep [filename]     : Exclude all samples not named in the file.\n  --remove [filename]   : Exclude all samples named in the file.\n  --keep-fam [filename] : Exclude all families not named in the file.\n  --remove-fam [fname]  : Exclude all families named in the file.\n  --extract <range> [f] : Exclude all variants not named in the file.\n  --exclude <range> [f] : Exclude all variants named in the file.\n  --keep-clusters [filename]          : These can be used individually or in\n  --keep-cluster-names [name(s)...]     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters [filename]        : Exclude all clusters named in the file.\n  --remove-cluster-names [name(s)...] : Exclude the named clusters.\n  --gene [sets...] : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib [f] {att lst} : Given a file assigning attributes to variants, and a\n  --attrib-indiv [f] {a}   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \\'negative match conditions\\'instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr [chrs...]  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \\'--chr 1-4, 22, xy\\'.\n  --not-chr [...]  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only <just-acgt> : Exclude non-SNP variants.  By default, SNP = both\n                            allele codes are single-character; \\'just-acgt\\'\n                            restricts SNP codes to {A,C,G,T,a,c,g,t,[missing]}.\n  --from [var ID]  : Use ID(s) to specify a variant range to load.  When used\n  --to   [var ID]    together, both variants must be on the same chromosome.\n  --snp  [var ID]  : Specify a single variant to load.\n  --exclude-snp [] : Specify a single variant to exclude.\n  --window  [kbs]  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp [pos]  : Use physical position(s) to define a variant range to\n  --to-bp   [pos]    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb [pos]    values.  You must also specify a single chromosome (using\n  --to-kb   [pos]    e.g. --chr) when using these flags.\n  --from-mb [pos]\n  --to-mb   [pos]\n  --snps [var IDs...]  : Use IDs to specify variant range(s) to load or\n  --exclude-snps [...]   exclude.  E.g. \\'--snps rs1111-rs2222, rs3333, rs4444\\'.\n  --thin [p]       : Randomly remove variants, retaining each with prob. p.\n  --thin-count [n] : Randomly remove variants until n of them remain.\n  --bp-space [bps] : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv [p]         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count [n]   : Randomly remove samples until n of them remain.\n  --filter [f] [val(s)...] : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter [n]            : Match against (n+2)th column instead.\n  --geno {val}     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind {val}     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing [f1] [f2] : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf {freq}        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf [freq]    : Exclude variants with MAF greater than the threshold.\n  --mac [ct]          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac [ct]      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq [fn] : Estimate MAFs and heterozygote frequencies from the given\n                     --freq{x} report, instead of the input fileset.\n  --hwe [p] <midp> <include-nonctrl> : Exclude variants with Hardy-Weinberg\n                                       equilibrium exact test p-values below a\n                                       threshold.\n  --me [t] [v] <var-first> : Filter out trios and variants with Mendel error\n                             rates exceeding the given thresholds.\n  --me-exclude-one {ratio} : Make --me exclude only one sample per trio.\n  --qual-scores [f] {qcol} {IDcol} {skip} : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold [min qual score]       : Set --qual-scores range floor.\n  --qual-max-threshold [max qual score]   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders <require-2-missing> <first> : Clear parental IDs for those\n                                                with 1+ missing parent(s).\n  --recode-allele [fn] : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr [MT code] : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype [ch] : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype [s] : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster [f] : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x [bp1] [bp2] <no-fail> : Changes chromosome code of all X chromosome\n  --split-x [build] <no-fail>       variants with bp position <= bp1 or >= bp2\n                                    to XY.  The following build codes are\n                                    supported as shorthand:\n                                    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n                                    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n                                    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n                                    By default, PLINK errors out when no\n                                    variants would be affected by --split-x;\n                                    the \\'no-fail\\'modifier (useful in scripts)\n                                    overrides this.\n  --merge-x <no-fail>             : Merge XY chromosome back with X.\n  --set-me-missing  : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2 : Cause --make-bed to replace all missing calls with\n                      homozygous A2 calls.\n  --set-missing-var-ids [t]   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len [n] : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code [string] : Change unnamed variant code (default \\'.\\').\n  --update-chr  [f] {chrcol} {IDcol}  {skip} : Update variant chromosome codes.\n  --update-cm   [f] {cmcol}  {IDcol}  {skip} : Update centimorgan positions.\n  --update-map  [f] {bpcol}  {IDcol}  {skip} : Update variant bp positions.\n  --update-name [f] {newcol} {oldcol} {skip} : Update variant IDs.\n  --update-alleles [fname] : Update variant allele codes.\n  --allele1234 <multichar> : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                             With \\'multichar\\', converts all A/C/G/Ts in allele\n                             names to 1/2/3/4s.\n  --alleleACGT <multichar> : Reverse of --allele1234.\n  --update-ids [f]     : Update sample IDs.\n  --update-parents [f] : Update parental IDs.\n  --update-sex [f] {n} : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip [filename]    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset [fn]   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window [ct+1] : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb [x] : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold [x] : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{-fid/-iid}.\n  --a1-allele [f] {a1col} {IDcol} {skip} : Force alleles in the file to A1.\n  --a2-allele [filename] {a2col} {IDcol} {skip} :\n    Force alleles in the file to A2.  (\"--a2-allele [VCF filename] 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort [m] {f} : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype <no-parents> <no-sex | female-2> : Include more sample info\n                                                      in new .cov file.\n  --dummy-coding {N} <no-round> : Split categorical variables (n categories,\n                                  2 < n <= N, default N is 49) into n-1 binary\n                                  dummy variables when writing covariate file.\n  --merge-mode [n]   : Adjust --{b}merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window [ct+1] : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb [x] : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm [x] : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 [x] : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp [var ID]  : Set first variant in all --r/--r2 pairs.\n  --ld-snps [vID...] : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list [f]  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb [kbs]     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 [val]     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr [code]   : Set Xchr model for --indep{-pairwise}, --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb [kbs]      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf [cutoff]  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci [x]  : Set --blocks \\'strong LD\\'CI thresholds (defaults\n  --blocks-strong-highci [x]   0.70 and 0.98).\n  --blocks-recomb-highci [x] : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac [x]   : Force haploblock [strong LD pairs]:[total\n                               informative pairs] ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=[x]        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists [dist file] {id file} : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap [val]    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min [cutoff]     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max [cutoff]     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match [] : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size [ct]   : Set minimum size of pools in \\'--homozyg group\\'report.\n  --read-genome [fn] : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc [p-val]    : Specify minimum PPC test p-value within a cluster.\n  --mc [max size]  : Specify maximum cluster size.\n  --mcc [c1] [c2]  : Specify maximum case and control counts per cluster.\n  --K [min count]  : Specify minimum cluster count.\n  --ibm [val]      : Specify minimum identity-by-missingness.\n  --match [f] {mv} : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type [f] : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \\'negative matches\\'(i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \\'positive matches\\'(default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch [f] {m} : Force all members of a cluster to have similar\n  --qt [fname]       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names [...] : These can be used individually or in combination\n  --pca-clusters [fname]      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot [dims] <by-cluster> <eigendecomp> <eigvals> :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell [thresh]  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition [var ID] <dominant | recessive> : Add one variant as a --linear\n                                                or --logistic covariate.\n  --condition-list [f] <dominant | recessive> : Add variants named in the file\n                                                as --linear/--logistic covs.\n  --parameters [...]  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> {...} : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \\'--tests all\\'to include all terms.\n  --vif [max VIF]     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model [code] : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars {cov(s)...} : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust <gc> <log10> <qq-plot>   : Report some multiple-testing corrections.\n  --lambda [val]   : Set genomic control lambda for --adjust.\n  --ci [size]      : Report confidence intervals for odds ratios.\n  --pfilter [val]  : Filter out association test results with higher p-values.\n  --aperm [min perms - 1] {max perms} {alpha} {beta} {init interval} {slope} :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      [slope]p + [init interval] more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p [p-val]       : Adjust set test significant variant p-value ceiling\n                          (default 0.05).\n  --set-r2 {v} <write>  : Adjust set test significant variant pairwise r^2\n                          ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to {output prefix}.ldset.\n  --set-max [ct]        : Adjust set test maximum # of significant variants\n                          considered per set (default 5).\n  --set-test-lambda [v] : Specify genomic control correction for set test.\n  --border [kbs]            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field [nm] : Set --annotate variant ID field name.\n  --clump-p1 [pval] : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 [pval] : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 [r^2]  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb [kbs]  : Set --clump kb radius (default 250).\n  --clump-snp-field [n...]  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field [name...]   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate [hdr...] : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range [filename]  : Report overlaps between clumps and regions.\n  --clump-range-border [kb] : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-snp-field [n...] : Set --meta-analysis variant ID, A1/A2\n  --meta-analysis-a1-field [n...]    allele, p-value, and/or effective sample\n  --meta-analysis-a2-field [n...]    size field names.  Defauls are \\'SNP\\',\n  --meta-analysis-p-field [n...]     \\'A1\\', \\'A2\\', \\'P\\', and \\'NMISS\\',\n  --meta-analysis-ess-field [n...]   respectively.  When multiple parameters\n                                     are given to these flags, earlier names\n                                     take precedence over later ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/[# cases] + 1/[# controls]).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border [kbs]   : Extend --gene-report regions by given # of kbs.\n  --gene-subset [filename]   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field [] : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap [kbs]      : Set \\'--fast-epistasis case-only\\'min. gap (default 1000).\n  --epi1 [p-value] : Set --{fast-}epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 [p-value] : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin [n] : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range [range file] [data file] {i} {j} <header> :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --parallel [k] [n] : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory [val]     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads [val]    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d [char]         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed [val...]    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size [val] : Set number of permutations per batch for some\n                            permutation tests.\n  --output-min-p [p] : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink2 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"messages": [{"role": "user", "content":   "plink-1.90b5"}, {"role": "assistant", "content":  "PLINK v1.90b5 64-bit (14 Nov 2017)             www.cog-genomics.org/plink/1.9/\n(C) 2005-2017 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * [square brackets] denote a required parameter, where the text between the\n    brackets describes its nature.\n  * <angle brackets> denote an optional modifier (or if \\'|\\'is present, a set\n    of mutually exclusive optional modifiers).  Use the EXACT text in the\n    definition, e.g. \\'--dummy acgt\\'.\n  * There\\'s one exception to the angle brackets/exact text rule: when an angle\n    bracket term ends with \\'=[value]\\', \\'[value]\\'designates a variable\n    parameter.\n  * {curly braces} denote an optional parameter, where the text between the\n    braces describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink [input flag(s)...] {command flag(s)...} {other flag(s)...}\n  plink --help {flag name(s)...}\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile {prefix} : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed [filename] : Specify full name of .bed file.\n  --bim [filename] : Specify full name of .bim file.\n  --fam [filename] : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file {prefix}  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped [filename] : Specify full name of .ped file.\n  --map [filename] : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile {prefix} : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped [fname]   : Specify full name of .tped file.\n  --tfam [fname]   : Specify full name of .tfam file.\n\n  --lfile {prefix} : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen [fname]   : Specify full name of .lgen file.\n  --reference [fn] : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf [filename] : Specify full name of .vcf or .vcf.gz file.\n  --bcf [filename] : Specify full name of BCF2 file.\n\n  --data {prefix}  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen [filename] : Specify full name of .gen or .gen.gz file.\n  --bgen [f] <snpid-chr> : Specify full name of .bgen file.\n  --sample [fname] : Specify full name of .sample file.\n\n  --23file [fname] {FID} {IID} {sex} {pheno} {pat. ID} {mat. ID} :\n    Specify 23andMe input file.\n\n  --grm-gz {prfx}  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin {prfx} : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy [sample ct] [SNP ct] {missing geno freq} {missing pheno freq}\n          <acgt | 1234 | 12> <scalar-pheno>\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate [simulation parameter file] <tags | haps> <acgt | 1234 | 12>\n  --simulate-qt [simulation parameter file] <tags | haps> <acgt | 1234 | 12>\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.{extension}\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out [prefix]   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode [output format] <01 | 12> <tab | tabx | spacex | bgz | gen-gz>\n           <include-alt> <omit-nonmale-y>\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-[chr #].recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-[chr #][.ped + .info]\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan <verbose>\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster <omit-unassigned>\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to {output prefix}.set, while --set-table writes a\n    variant-by-set membership table to {output prefix}.set.table.\n\n  --merge [.ped filename] [.map filename]\n  --merge [text fileset prefix]\n  --bmerge [.bed filename] [.bim filename] [.fam filename]\n  --bmerge [binary fileset prefix]\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to {output prefix}.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list [filename]\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars <require-same-ref> <ids-only> <suppress-first>\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq <counts | case-control> <gz>\n  --freqx <gz>\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing <gz>\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy <midp> <gz>\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel <summaries-only>\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het <small-sample> <gz>\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex {female max F} {male min F}\n  --check-sex ycount {female max F} {male min F} {female max Y obs}\n                     {male min Y obs}\n  --check-sex y-only {female max Y obs} {male min Y obs}\n  --impute-sex {female max F} {male min F}\n  --impute-sex ycount {female max F} {male min F} {female max Y obs}\n                      {male min Y obs}\n  --impute-sex y-only {female max Y obs} {male min Y obs}\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst <case-control>\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep [window size]<kb> [step size (variant ct)] [VIF threshold]\n  --indep-pairwise [window size]<kb> [step size (variant ct)] [r^2 threshold]\n  --indep-pairphase [window size]<kb> [step size (variant ct)] [r^2 threshold]\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \\'--indep-pairwise 500 kb 5 0.5\\'\n    and \\'--indep-pairwise 500kb 5 0.5\\'have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r <square | square0 | triangle | inter-chr> <gz | bin | bin4> <spaces>\n      <in-phase> <d | dprime | dprime-signed> <with-freqs> <yes-really>\n  --r2 <square | square0 | triangle | inter-chr> <gz | bin | bin4> <spaces>\n       <in-phase> <d | dprime | dprime-signed> <with-freqs> <yes-really>\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld [variant ID] [variant ID] <hwe-midp>\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags [filename]\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks <no-pheno-req> <no-small-max-span>\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \\'--recode beagle\\'to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance <square | square0 | triangle> <gz | bin | bin4> <ibs> <1-ibs>\n             <allele-ct> <flat-missing>\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to {output prefix}.dist, and a list of the\n    corresponding sample IDs to {output prefix}.dist.id.  The first row of the\n    .dist file contains a single {genome 1-genome 2} distance, the second row\n    has the {genome 1-genome 3} and {genome 2-genome 3} distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to {output prefix}.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to {output prefix}.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to {output prefix}.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \\'--distance 1-ibs flat-missing\n    square\\'and \\'--distance ibs flat-missing square\\', respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel <square | square0 | triangle> <gz | bin | bin4>\n             <cov | ibc2 | ibc3>\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to {output prefix}.rel, and corresponding IDs to {output prefix}.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz <no-gz> <cov | ibc2 | ibc3>\n  --make-grm-bin <cov | ibc2 | ibc3>\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff {val}\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    {output prefix}.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test {permutation count}\n  --groupdist {iters} {d}\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance {iters} {d}\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to {number of people}^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel {iters} {d}\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome <gz> <rel-check> <full> <unbounded> <nudge>\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg <group | group-verbose> <consensus-match> <extend>\n            <subtract-1-from-lengths>\n  --homozyg-snp [min var count]\n  --homozyg-kb [min length]\n  --homozyg-density [max inverse density (kb/var)]\n  --homozyg-gap [max internal gap kb length]\n  --homozyg-het [max hets]\n  --homozyg-window-snp [scanning window size]\n  --homozyg-window-het [max hets in scanning window hit]\n  --homozyg-window-missing [max missing calls in scanning window hit]\n  --homozyg-window-threshold [min scanning window hit rate]\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group{-verbose}\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group{-verbose}\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as [end bp position] -\n        [start bp position] + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster <cc> <group-avg | old-tiebreaks> <missing> <only2>\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca {count} <header> <tabs> <var-wts>\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour [n1] [n2]\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc <perm | mperm=[value]> <perm-count> <fisher | fisher-midp> <counts>\n          <set-test>\n  --assoc <perm | mperm=[value]> <perm-count> <qt-means> <lin> <set-test>\n  --model <perm | mperm=[value]> <perm-count>\n          <fisher | fisher-midp | trend-only> <set-test>\n          <dom | rec | gen | trend>\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=[value]\\'causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh <perm | mperm=[value]> <perm-count> <set-test>\n    (alias: --cmh)\n  --bd <perm | perm-bd | mperm=[value]> <perm-count> <set-test>\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe {covariate index}\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \\'--gxe 3\\'to base them on the third covariate instead.\n\n  --linear <perm | mperm=[value]> <perm-count> <set-test>\n           <genotypic | hethom | dominant | recessive | no-snp> <hide-covar>\n           <sex | no-x-sex> <interaction> <beta> <standard-beta> <intercept>\n  --logistic <perm | mperm=[value]> <perm-count> <set-test>\n             <genotypic | hethom | dominant | recessive | no-snp> <hide-covar>\n             <sex | no-x-sex> <interaction> <beta> <intercept>\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=[value]\\'starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage [allele dosage file] <noheader> <skip0=[i]> <skip1=[j]> <skip2=[k]>\n           <dose1> <format=[m]> <Zout> <occur | standard-beta> <sex>\n           <case-control-freqs>\n  --dosage [list file] list <sepheader | noheader> <skip0=[i]> <skip1=[j]>\n           <skip2=[k]> <dose1> <format=[m]> <Zout> <occur | standard-beta>\n           <sex> <case-control-freqs>\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modifier.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso [h2 estimate] {min lambda} <report-zeroes>\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing <perm | mperm=[value]> <perm-count> <midp>\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  The \\'midp\\'modifier causes Lancaster\\'s mid-p\n    adjustment to be applied.\n\n  --make-perm-pheno [ct]\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt <exact | exact-midp | poo> <perm | mperm=[value]> <perm-count>\n        <parentdt1 | parentdt2 | pat | mat> <set-test>\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=[value]\\'requests a family-based adaptive or max(T)\n      permutation test.  By default, the permutation test statistic is the\n      basic TDT p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined\n      test p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam <perm | mperm=[value]> <perm-count> <emp-se>\n  --qfam-parents <perm | mperm=[value]> <perm-count> <emp-se>\n  --qfam-between <perm | mperm=[value]> <perm-count> <emp-se>\n  --qfam-total <perm | mperm=[value]> <perm-count> <emp-se>\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=[value]\\'just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate [PLINK report] <attrib=[file]> <ranges=[file]> <filter=[file]>\n             <snps=[file]> <NA | prune> <block> <subset=[file]> <minimal>\n             <distance>\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=[file]\\'specifies a (possibly gzipped) attribute file.\n    * \\'ranges=[file]\\'specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=[file]\\'causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=[file]\\'causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=[file]\\'causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump [PLINK report filename(s)...]\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report [PLINK report] [gene range file]\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis [PLINK report filenames...]\n  --meta-analysis [PLINK report filenames...] + <logscale | qt>\n                  <no-map | no-allele> <study> <report-all> <weighted-z>\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis <boost | joint-effects | no-ueki> <case-only>\n                   <set-by-set | set-by-all> <nop>\n  --epistasis <set-by-set | set-by-all>\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge [common file prefix] [ct]\n    When a --{fast-}epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus [variant ID] [variant ID]\n    Two-locus joint genotype count report.\n\n  --score [filename] {i} {j} {k} <header> <sum | no-sum>\n          <no-mean-imputation | center> <include-cnt> <double-dosage>\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --R [R script file] <debug>\n    Connect to a Rserve (preferably version 1.7 or later) background process,\n    and execute the Rplink function defined in the input file.  (Unless the\n    \\'debug\\'modifier is present; in that case, the R commands that PLINK would\n    have tried to execute are logged to a file.)\n\n  --write-var-ranges [block ct]\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink2/order .)\n  --script [fname] : Include command-line options from file.\n  --rerun {log}    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype [char] : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid {ID}     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim {d}       : Parse sample IDs as [FID][d][IID] (default delim \\'_\\').\n  --vcf-idspace-to [c] : Convert spaces in sample IDs to the given character.\n  --biallelic-only <strict> <list> : Skip VCF variants with 2+ alt. alleles.\n  --vcf-min-qual [val]             : Skip VCF variants with low/missing QUAL.\n  --vcf-filter {exception(s)...}   : Skip variants which have FILTER failures.\n  --vcf-require-gt                 : Skip variants with no GT field.\n  --vcf-min-gq [val]               : No-call a genotype when GQ is below the\n                                     given threshold.\n  --vcf-min-gp [val]               : No-call a genotype when 0-1 scaled GP is\n                                     below the given threshold.\n  --vcf-half-call [m]  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr [chr nm] : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name [col nm] : Import named phenotype from the .sample file.\n  --hard-call-threshold [val]  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code {string list} : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases [num]   : Set --simulate case count (default 1000).\n  --simulate-ncontrols [n]  : Set --simulate control count (default 1000).\n  --simulate-prevalence [p] : Set --simulate disease prevalence (default 0.01).\n  --simulate-n [num]        : Set --simulate-qt sample count (default 1000).\n  --simulate-label [prefix] : Set --simulate{-qt} FID/IID name prefix.\n  --simulate-missing [freq] : Set --simulate{-qt} missing genotype frequency.\n  --allow-extra-chr <0>     : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set [autosome ct] <no-x> <no-y> <no-xy> <no-mt> :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num [value]    : Alias for \\'--chr-set [value] no-y no-xy no-mt\\'.\n  --cm-map [fname pattern] {chr} : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --pheno [fname]  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno [n]     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name [c] : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype [v] : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.\n  --make-pheno [fn] [val] : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno [Lt] {Hbt} : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar [filename] <keep-pheno-on-missing-cov> : Specify covariate file.\n  --covar-name [...]      : Specify covariate(s) in --covar file by name.\n                            Separate multiple names with spaces or commas, and\n                            use dashes to designate ranges.\n  --covar-number [...]    : Specify covariate(s) in --covar file by index.\n  --no-const-covar        : Exclude constant covariates.\n  --within [f] <keep-NA>  : Specify initial cluster assignments.\n  --mwithin [n]           : Load cluster assignments from column n+2.\n  --family                : Create a cluster for each family ID.\n  --loop-assoc [f] <keep-NA>    : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set [filename]              : Load sets from a .set file.\n  --set-names [name(s)...]      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset [filename]           : Load only sets named in the given text file.\n  --set-collapse-all [set name] : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all [s] : --set-collapse-all + inversion.\n  --make-set [filename]         : Define sets from a list of named bp ranges.\n  --make-set-border [kbs]       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep [filename]     : Exclude all samples not named in the file.\n  --remove [filename]   : Exclude all samples named in the file.\n  --keep-fam [filename] : Exclude all families not named in the file.\n  --remove-fam [fname]  : Exclude all families named in the file.\n  --extract <range> [f] : Exclude all variants not named in the file.\n  --exclude <range> [f] : Exclude all variants named in the file.\n  --keep-clusters [filename]          : These can be used individually or in\n  --keep-cluster-names [name(s)...]     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters [filename]        : Exclude all clusters named in the file.\n  --remove-cluster-names [name(s)...] : Exclude the named clusters.\n  --gene [sets...] : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib [f] {att lst} : Given a file assigning attributes to variants, and a\n  --attrib-indiv [f] {a}   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \\'negative match conditions\\'instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr [chrs...]  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \\'--chr 1-4, 22, xy\\'.\n  --not-chr [...]  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only <just-acgt> : Exclude non-SNP variants.  By default, SNP = both\n                            allele codes are single-character; \\'just-acgt\\'\n                            restricts SNP codes to {A,C,G,T,a,c,g,t,[missing]}.\n  --from [var ID]  : Use ID(s) to specify a variant range to load.  When used\n  --to   [var ID]    together, both variants must be on the same chromosome.\n  --snp  [var ID]  : Specify a single variant to load.\n  --exclude-snp [] : Specify a single variant to exclude.\n  --window  [kbs]  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp [pos]  : Use physical position(s) to define a variant range to\n  --to-bp   [pos]    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb [pos]    values.  You must also specify a single chromosome (using\n  --to-kb   [pos]    e.g. --chr) when using these flags.\n  --from-mb [pos]\n  --to-mb   [pos]\n  --snps [var IDs...]  : Use IDs to specify variant range(s) to load or\n  --exclude-snps [...]   exclude.  E.g. \\'--snps rs1111-rs2222, rs3333, rs4444\\'.\n  --thin [p]       : Randomly remove variants, retaining each with prob. p.\n  --thin-count [n] : Randomly remove variants until n of them remain.\n  --bp-space [bps] : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv [p]         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count [n]   : Randomly remove samples until n of them remain.\n  --filter [f] [val(s)...] : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter [n]            : Match against (n+2)th column instead.\n  --geno {val}     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind {val}     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing [f1] [f2] : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf {freq}        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf [freq]    : Exclude variants with MAF greater than the threshold.\n  --mac [ct]          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac [ct]      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq [fn] : Estimate MAFs and heterozygote frequencies from the given\n                     --freq{x} report, instead of the input fileset.\n  --hwe [p] <midp> <include-nonctrl> : Exclude variants with Hardy-Weinberg\n                                       equilibrium exact test p-values below a\n                                       threshold.\n  --me [t] [v] <var-first> : Filter out trios and variants with Mendel error\n                             rates exceeding the given thresholds.\n  --me-exclude-one {ratio} : Make --me exclude only one sample per trio.\n  --qual-scores [f] {qcol} {IDcol} {skip} : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold [min qual score]       : Set --qual-scores range floor.\n  --qual-max-threshold [max qual score]   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders <require-2-missing> <first> : Clear parental IDs for those\n                                                with 1+ missing parent(s).\n  --recode-allele [fn] : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr [MT code] : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype [ch] : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype [s] : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster [f] : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x [bp1] [bp2] <no-fail> : Changes chromosome code of all X chromosome\n  --split-x [build] <no-fail>       variants with bp position <= bp1 or >= bp2\n                                    to XY.  The following build codes are\n                                    supported as shorthand:\n                                    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n                                    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n                                    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n                                    By default, PLINK errors out when no\n                                    variants would be affected by --split-x;\n                                    the \\'no-fail\\'modifier (useful in scripts)\n                                    overrides this.\n  --merge-x <no-fail>             : Merge XY chromosome back with X.\n  --set-me-missing  : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2 : Cause --make-bed to replace all missing calls with\n                      homozygous A2 calls.\n  --set-missing-var-ids [t]   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len [n] : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code [string] : Change unnamed variant code (default \\'.\\').\n  --update-chr  [f] {chrcol} {IDcol}  {skip} : Update variant chromosome codes.\n  --update-cm   [f] {cmcol}  {IDcol}  {skip} : Update centimorgan positions.\n  --update-map  [f] {bpcol}  {IDcol}  {skip} : Update variant bp positions.\n  --update-name [f] {newcol} {oldcol} {skip} : Update variant IDs.\n  --update-alleles [fname] : Update variant allele codes.\n  --allele1234 <multichar> : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                             With \\'multichar\\', converts all A/C/G/Ts in allele\n                             names to 1/2/3/4s.\n  --alleleACGT <multichar> : Reverse of --allele1234.\n  --update-ids [f]     : Update sample IDs.\n  --update-parents [f] : Update parental IDs.\n  --update-sex [f] {n} : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip [filename]    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset [fn]   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window [ct+1] : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb [x] : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold [x] : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{-fid/-iid}.\n  --a1-allele [f] {a1col} {IDcol} {skip} : Force alleles in the file to A1.\n  --a2-allele [filename] {a2col} {IDcol} {skip} :\n    Force alleles in the file to A2.  (\"--a2-allele [VCF filename] 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort [m] {f} : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype <no-parents> <no-sex | female-2> : Include more sample info\n                                                      in new .cov file.\n  --dummy-coding {N} <no-round> : Split categorical variables (n categories,\n                                  2 < n <= N, default N is 49) into n-1 binary\n                                  dummy variables when writing covariate file.\n  --merge-mode [n]   : Adjust --{b}merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window [ct+1] : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb [x] : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm [x] : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 [x] : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp [var ID]  : Set first variant in all --r/--r2 pairs.\n  --ld-snps [vID...] : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list [f]  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb [kbs]     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 [val]     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr [code]   : Set Xchr model for --indep{-pairwise}, --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb [kbs]      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf [cutoff]  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci [x]  : Set --blocks \\'strong LD\\'CI thresholds (defaults\n  --blocks-strong-highci [x]   0.70 and 0.98).\n  --blocks-recomb-highci [x] : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac [x]   : Force haploblock [strong LD pairs]:[total\n                               informative pairs] ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=[x]        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists [dist file] {id file} : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap [val]    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min [cutoff]     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max [cutoff]     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match [] : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size [ct]   : Set minimum size of pools in \\'--homozyg group\\'report.\n  --read-genome [fn] : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc [p-val]    : Specify minimum PPC test p-value within a cluster.\n  --mc [max size]  : Specify maximum cluster size.\n  --mcc [c1] [c2]  : Specify maximum case and control counts per cluster.\n  --K [min count]  : Specify minimum cluster count.\n  --ibm [val]      : Specify minimum identity-by-missingness.\n  --match [f] {mv} : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type [f] : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \\'negative matches\\'(i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \\'positive matches\\'(default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch [f] {m} : Force all members of a cluster to have similar\n  --qt [fname]       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names [...] : These can be used individually or in combination\n  --pca-clusters [fname]      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot [dims] <by-cluster> <eigendecomp> <eigvals> :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell [thresh]  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition [var ID] <dominant | recessive> : Add one variant as a --linear\n                                                or --logistic covariate.\n  --condition-list [f] <dominant | recessive> : Add variants named in the file\n                                                as --linear/--logistic covs.\n  --parameters [...]  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> {...} : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \\'--tests all\\'to include all terms.\n  --vif [max VIF]     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model [code] : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars {cov(s)...} : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust <gc> <log10> <qq-plot>   : Report some multiple-testing corrections.\n  --lambda [val]   : Set genomic control lambda for --adjust.\n  --ci [size]      : Report confidence intervals for odds ratios.\n  --pfilter [val]  : Filter out association test results with higher p-values.\n  --aperm [min perms - 1] {max perms} {alpha} {beta} {init interval} {slope} :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      [slope]p + [init interval] more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p [p-val]       : Adjust set test significant variant p-value ceiling\n                          (default 0.05).\n  --set-r2 {v} <write>  : Adjust set test significant variant pairwise r^2\n                          ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to {output prefix}.ldset.\n  --set-max [ct]        : Adjust set test maximum # of significant variants\n                          considered per set (default 5).\n  --set-test-lambda [v] : Specify genomic control correction for set test.\n  --border [kbs]            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field [nm] : Set --annotate variant ID field name.\n  --clump-p1 [pval] : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 [pval] : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 [r^2]  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb [kbs]  : Set --clump kb radius (default 250).\n  --clump-snp-field [n...]  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field [name...]   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate [hdr...] : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range [filename]  : Report overlaps between clumps and regions.\n  --clump-range-border [kb] : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-snp-field [n...] : Set --meta-analysis variant ID, A1/A2\n  --meta-analysis-a1-field [n...]    allele, p-value, and/or effective sample\n  --meta-analysis-a2-field [n...]    size field names.  Defauls are \\'SNP\\',\n  --meta-analysis-p-field [n...]     \\'A1\\', \\'A2\\', \\'P\\', and \\'NMISS\\',\n  --meta-analysis-ess-field [n...]   respectively.  When multiple parameters\n                                     are given to these flags, earlier names\n                                     take precedence over later ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/[# cases] + 1/[# controls]).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border [kbs]   : Extend --gene-report regions by given # of kbs.\n  --gene-subset [filename]   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field [] : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap [kbs]      : Set \\'--fast-epistasis case-only\\'min. gap (default 1000).\n  --epi1 [p-value] : Set --{fast-}epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 [p-value] : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin [n] : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range [range file] [data file] {i} {j} <header> :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --R-port [port #]  : Connect to Rserve on a port other than 6311.\n  --R-host [host]    : Connect to Rserve host.\n  --R-socket [sock]  : Connect to Rserve socket.\n  --parallel [k] [n] : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory [val]     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads [val]    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d [char]         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed [val...]    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size [val] : Set number of permutations per batch for some\n                            permutation tests.\n  --output-min-p [p] : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink2 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"messages": [{"role": "user", "content":   "plink-1.90b6.12"}, {"role": "assistant", "content":  "PLINK v1.90b6.12 64-bit (28 Oct 2019)          www.cog-genomics.org/plink/1.9/\n(C) 2005-2019 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * <angle brackets> denote a required parameter, where the text between the\n    angle brackets describes its nature.\n  * [\\'square brackets + single-quotes\\'] denotes an optional modifier.  Use the\n    EXACT text in the quotes.\n  * [{bar|separated|braced|bracketed|values}] denotes a collection of mutually\n    exclusive optional modifiers (again, the exact text must be used).  When\n    there are no outer square brackets, one of the choices must be selected.\n  * [\\'quoted_text=\\'<description of value>] denotes an optional modifier that\n    must begin with the quoted text, and be followed by a value with no\n    whitespace in between.  \\'|\\'may also be used here to indicate mutually\n    exclusive options.\n  * [square brackets without quotes or braces] denote an optional parameter,\n    where the text between the brackets describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]\n  plink --help [flag name(s)...]\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile [prefix] : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed <filename> : Specify full name of .bed file.\n  --bim <filename> : Specify full name of .bim file.\n  --fam <filename> : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file [prefix]  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped <filename> : Specify full name of .ped file.\n  --map <filename> : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile [prefix] : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped <fname>   : Specify full name of .tped file.\n  --tfam <fname>   : Specify full name of .tfam file.\n\n  --lfile [prefix] : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen <fname>   : Specify full name of .lgen file.\n  --reference <fn> : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf <filename> : Specify full name of .vcf or .vcf.gz file.\n  --bcf <filename> : Specify full name of BCF2 file.\n\n  --data [prefix]  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen <filename> : Specify full name of .gen or .gen.gz file.\n  --bgen <f> [\\'snpid-chr\\'] : Specify full name of .bgen file.\n  --sample <fname> : Specify full name of .sample file.\n\n  --23file <fname> [FID] [IID] [sex] [pheno] [pat. ID] [mat. ID] :\n    Specify 23andMe input file.\n\n  --grm-gz [prfx]  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin [prfx] : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy <sample ct> <SNP ct> [missing geno freq] [missing pheno freq]\n          [{acgt | 1234 | 12}] [\\'scalar-pheno\\']\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate <simulation parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n  --simulate-qt <sim. parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.<extension>\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out <prefix>   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode <output format> [{01 | 12}] [{tab | tabx | spacex | bgz | gen-gz}]\n           [\\'include-alt\\'] [\\'omit-nonmale-y\\']\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-<chr #>.recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-<chr #>{.ped,.info}\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan [\\'verbose\\']\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster [\\'omit-unassigned\\']\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to <output prefix>.set, while --set-table writes a\n    variant-by-set membership table to <output prefix>.set.table.\n\n  --merge <.ped filename> <.map filename>\n  --merge <text fileset prefix>\n  --bmerge <.bed filename> <.bim filename> <.fam filename>\n  --bmerge <binary fileset prefix>\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to <output prefix>.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list <filename>\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars [\\'require-same-ref\\'] [\\'ids-only\\'] [\\'suppress-first\\']\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq [{counts | case-control}] [\\'gz\\']\n  --freqx [\\'gz\\']\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing [\\'gz\\']\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n    Unlike most other commands, this doesn\\'t treat het. haploids as missing.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy [\\'midp\\'] [\\'gz\\']\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel [\\'summaries-only\\']\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het [\\'small-sample\\'] [\\'gz\\']\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex [female max F] [male min F]\n  --check-sex ycount [female max F] [male min F] [female max Y obs]\n                     [male min Y obs]\n  --check-sex y-only [female max Y obs] [male min Y obs]\n  --impute-sex [female max F] [male min F]\n  --impute-sex ycount [female max F] [male min F] [female max Y obs]\n                      [male min Y obs]\n  --impute-sex y-only [female max Y obs] [male min Y obs]\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst [\\'case-control\\']\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep <window size>[\\'kb\\'] <step size (variant ct)> <VIF threshold>\n  --indep-pairwise <window size>[\\'kb\\'] <step size (variant ct)> <r^2 threshold>\n  --indep-pairphase <window size>[\\'kb\\'] <step size (variant ct)> <r^2 thresh>\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \"--indep-pairwise 500 kb 5 0.5\"\n    and \"--indep-pairwise 500kb 5 0.5\" have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n      [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n      [\\'yes-really\\']\n  --r2 [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n       [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n       [\\'yes-really\\']\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld <variant ID> <variant ID> [\\'hwe-midp\\']\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags <filename>\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks [\\'no-pheno-req\\'] [\\'no-small-max-span\\']\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \"--recode beagle\" to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance [{square | square0 | triangle}] [{gz | bin | bin4}] [\\'ibs\\']\n             [\\'1-ibs\\'] [\\'allele-ct\\'] [\\'flat-missing\\']\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to <output prefix>.dist, and a list of the\n    corresponding sample IDs to <output prefix>.dist.id.  The first row of the\n    .dist file contains a single <genome 1-genome 2> distance, the second row\n    has the <genome 1-genome 3> and <genome 2-genome 3> distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to <output prefix>.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to <output prefix>.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to <output prefix>.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \"--distance 1-ibs flat-missing\n    square\" and \"--distance ibs flat-missing square\", respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel [{square | square0 | triangle}] [{gz | bin | bin4}]\n             [{cov | ibc2 | ibc3}]\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to <output prefix>.rel, and corresponding IDs to <output prefix>.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz [\\'no-gz\\'] [{cov | ibc2 | ibc3}]\n  --make-grm-bin [{cov | ibc2 | ibc3}]\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff [val]\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    <output prefix>.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test [permutation count]\n  --groupdist [iters] [d]\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance [iters] [d]\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to <number of people>^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel [iters] [d]\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome [\\'gz\\'] [\\'rel-check\\'] [\\'full\\'] [\\'unbounded\\'] [\\'nudge\\']\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg [{group | group-verbose}] [\\'consensus-match\\'] [\\'extend\\']\n            [\\'subtract-1-from-lengths\\']\n  --homozyg-snp <min var count>\n  --homozyg-kb <min length>\n  --homozyg-density <max inverse density (kb/var)>\n  --homozyg-gap <max internal gap kb length>\n  --homozyg-het <max hets>\n  --homozyg-window-snp <scanning window size>\n  --homozyg-window-het <max hets in scanning window hit>\n  --homozyg-window-missing <max missing calls in scanning window hit>\n  --homozyg-window-threshold <min scanning window hit rate>\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group{,-verbose}\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group{,-verbose}\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as <end bp position> -\n        <start bp position> + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster [\\'cc\\'] [{group-avg | old-tiebreaks}] [\\'missing\\'] [\\'only2\\']\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca [count] [\\'header\\'] [\\'tabs\\'] [\\'var-wts\\']\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour <n1> <n2>\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [{fisher | fisher-midp}]\n          [\\'counts\\'] [\\'set-test\\']\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'qt-means\\'] [\\'lin\\']\n          [\\'set-test\\']\n  --model [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n          [{fisher | fisher-midp | trend-only}] [\\'set-test\\']\n          [{dom | rec | gen | trend}]\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=\\'<value> causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n    (alias: --cmh)\n  --bd [\\'perm\\'| \\'perm-bd\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe [covariate index]\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \"--gxe 3\" to base them on the third covariate instead.\n\n  --linear [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n           [{genotypic | hethom | dominant | recessive | no-snp}]\n           [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n           [\\'standard-beta\\'] [\\'intercept\\']\n  --logistic [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n             [{genotypic | hethom | dominant | recessive | no-snp}]\n             [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n             [\\'intercept\\']\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=\\'<value> starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage <allele dosage file> [\\'noheader\\'] [\\'skip0=\\'<i>] [\\'skip1=\\'<j>]\n           [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --dosage <list file> list [{sepheader | noheader}] [\\'skip0=\\'<i>]\n           [\\'skip1=\\'<j>] [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modifier.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format=\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso <h2 estimate> [min lambda] [\\'report-zeroes\\']\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'midp\\']\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  (Het. haploids are treated as missing.)\n    The \\'midp\\'modifier causes Lancaster\\'s mid-p adjustment to be applied.\n\n  --make-perm-pheno <ct>\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt [{exact | exact-midp | poo}] [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n        [{parentdt1 | parentdt2 | pat | mat}] [\\'set-test\\']\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=\\'requests a family-based adaptive or max(T) permutation\n      test.  By default, the permutation test statistic is the basic TDT\n      p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined test\n      p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-parents [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-between [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-total [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=\\'<value> just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate <PLINK report> [\\'attrib=\\'<file>] [\\'ranges=\\'<file>]\n             [\\'filter=\\'<file>] [\\'snps=\\'<file>] [{NA | prune}] [\\'block\\']\n             [\\'subset=\\'<file>] [\\'minimal\\'] [\\'distance\\']\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=\\'<file> specifies a (possibly gzipped) attribute file.\n    * \\'ranges=\\'<file> specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=\\'<file> causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=\\'<file> causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=\\'<file> causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump <PLINK report filename(s)...>\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report <PLINK report> <gene range file>\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis <PLINK report filenames...>\n  --meta-analysis <PLINK report filenames...> + [{logscale | qt}]\n                  [{no-map | no-allele}] [\\'study\\'] [\\'report-all\\']\n                  [\\'weighted-z\\']\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis [{boost | joint-effects | no-ueki}] [\\'case-only\\']\n                   [{set-by-set | set-by-all}] [\\'nop\\']\n  --epistasis [{set-by-set | set-by-all}]\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge <common file prefix> <ct>\n    When a --{,fast-}epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus <variant ID> <variant ID>\n    Two-locus joint genotype count report.\n\n  --score <filename> [i] [j] [k] [\\'header\\'] [{sum | no-sum}]\n          [{no-mean-imputation | center}] [\\'include-cnt\\'] [\\'double-dosage\\']\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --R <R script file> [\\'debug\\']\n    Connect to a Rserve (preferably version 1.7 or later) background process,\n    and execute the Rplink function defined in the input file.  (Unless the\n    \\'debug\\'modifier is present; in that case, the R commands that PLINK would\n    have tried to execute are logged to a file.)\n\n  --write-var-ranges <block ct>\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink/1.9/order .)\n  --script <fname> : Include command-line options from file.\n  --rerun [log]    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype <char> : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid [ID]     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim [d]       : Parse sample IDs as <FID><d><IID> (default delim \\'_\\').\n  --vcf-idspace-to <c> : Convert spaces in sample IDs to the given character.\n  --biallelic-only [\\'strict\\'] [\\'list\\'] : Skip VCF variants with 2+ ALT alleles.\n  --vcf-min-qual <val>           : Skip VCF variants with low/missing QUAL.\n  --vcf-filter [exception(s)...] : Skip variants which have FILTER failures.\n  --vcf-require-gt               : Skip variants with no GT field.\n  --vcf-min-gq <val>             : No-call a genotype when GQ is below the\n                                   given threshold.\n  --vcf-min-gp <val>             : No-call a genotype when 0-1 scaled GP is\n                                   below the given threshold.\n  --vcf-half-call <m>  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr <chr nm> : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name <col nm> : Import named phenotype from the .sample file.\n  --hard-call-threshold <val>  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code [string list] : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases <num>   : Set --simulate case count (default 1000).\n  --simulate-ncontrols <n>  : Set --simulate control count (default 1000).\n  --simulate-prevalence <p> : Set --simulate disease prevalence (default 0.01).\n  --simulate-n <num>        : Set --simulate-qt sample count (default 1000).\n  --simulate-label <prefix> : Set --simulate{,-qt} FID/IID name prefix.\n  --simulate-missing <freq> : Set --simulate{,-qt} missing genotype frequency.\n  --allow-extra-chr [\\'0\\']   : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set <autosome ct> [\\'no-x\\'] [\\'no-y\\'] [\\'no-xy\\'] [\\'no-mt\\'] :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num <value>    : Alias for \"--chr-set <value> no-y no-xy no-mt\".\n  --cm-map <fname pattern> [chr] : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --allow-no-samples : Allow the input fileset to contain no samples.\n  --allow-no-vars    : Allow the input fileset to contain no variants.\n  --pheno <fname>  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno <n>     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name <c> : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype <v> : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.  This also\n                            forces phenotypes to be interpreted as case/ctrl.\n  --make-pheno <fn> <val> : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno <Lt> [Hbt] : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar <filename> [\\'keep-pheno-on-missing-cov\\'] : Specify covariate file.\n  --covar-name <...>       : Specify covariate(s) in --covar file by name.\n                             Separate multiple names with spaces or commas, and\n                             use dashes to designate ranges.\n  --covar-number <...>     : Specify covariate(s) in --covar file by index.\n  --no-const-covar         : Exclude constant covariates.\n  --allow-no-covars        : Allow no covariates to be loaded from --covar\n                             file.\n  --within <f> [\\'keep-NA\\'] : Specify initial cluster assignments.\n  --mwithin <n>            : Load cluster assignments from column n+2.\n  --family                 : Create a cluster for each family ID.\n  --loop-assoc <f> [\\'keep-NA\\']  : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set <filename>              : Load sets from a .set file.\n  --set-names <name(s)...>      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset <filename>           : Load only sets named in the given text file.\n  --set-collapse-all <set name> : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all <s> : --set-collapse-all + inversion.\n  --make-set <filename>         : Define sets from a list of named bp ranges.\n  --make-set-border <kbs>       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep <filename>       : Exclude all samples not named in the file.\n  --remove <filename>     : Exclude all samples named in the file.\n  --keep-fam <filename>   : Exclude all families not named in the file.\n  --remove-fam <filename> : Exclude all families named in the file.\n  --extract [\\'range\\'] <f> : Exclude all variants not named in the file.\n  --exclude [\\'range\\'] <f> : Exclude all variants named in the file.\n  --keep-clusters <filename>          : These can be used individually or in\n  --keep-cluster-names <name(s)...>     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters <filename>        : Exclude all clusters named in the file.\n  --remove-cluster-names <name(s)...> : Exclude the named clusters.\n  --gene <sets...> : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib <f> [att lst] : Given a file assigning attributes to variants, and a\n  --attrib-indiv <f> [a]   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \"negative match conditions\" instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr <chrs...>  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \"--chr 1-4, 22, xy\".\n  --not-chr <...>  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only [\\'just-acgt\\'] : Exclude non-SNP variants.  By default, SNP = both\n                              allele codes are single-character; \\'just-acgt\\'\n                              restricts codes to {A,C,G,T,a,c,g,t,<missing>}.\n  --from <var ID>  : Use ID(s) to specify a variant range to load.  When used\n  --to   <var ID>    together, both variants must be on the same chromosome.\n  --snp  <var ID>  : Specify a single variant to load.\n  --exclude-snp <> : Specify a single variant to exclude.\n  --window  <kbs>  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp <pos>  : Use physical position(s) to define a variant range to\n  --to-bp   <pos>    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb <pos>    values.  You must also specify a single chromosome (using\n  --to-kb   <pos>    e.g. --chr) when using these flags.\n  --from-mb <pos>\n  --to-mb   <pos>\n  --snps <var IDs...>  : Use IDs to specify variant range(s) to load or\n  --exclude-snps <...>   exclude.  E.g. \"--snps rs1111-rs2222, rs3333, rs4444\".\n  --thin <p>       : Randomly remove variants, retaining each with prob. p.\n  --thin-count <n> : Randomly remove variants until n of them remain.\n  --bp-space <bps> : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv <p>         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count <n>   : Randomly remove samples until n of them remain.\n  --filter <f> <val(s)...> : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter <n>            : Match against (n+2)th column instead.\n  --geno [val]     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind [val]     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing <f1> <f2> : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf [freq]        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf <freq>    : Exclude variants with MAF greater than the threshold.\n  --mac <ct>          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac <ct>      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq <fn> : Estimate MAFs and heterozygote frequencies from the given\n                     --freq{,x} report, instead of the input fileset.\n  --hwe <p> [\\'midp\\'] [\\'include-nonctrl\\'] : Exclude variants with Hardy-Weinberg\n                                           equilibrium exact test p-values\n                                           below a threshold.\n  --me <t> <v> [\\'var-first\\'] : Filter out trios and variants with Mendel error\n                               rates exceeding the given thresholds.\n  --me-exclude-one [ratio]   : Make --me exclude only one sample per trio.\n  --qual-scores <f> [qcol] [IDcol] [skip] : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold <min qual score>       : Set --qual-scores range floor.\n  --qual-max-threshold <max qual score>   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders [\\'require-2-missing\\'] [\\'first\\'] :\n    Clear parental IDs for those with 1+ missing parent(s).\n  --recode-allele <fn> : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr <MT code> : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype <ch> : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype <s> : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster <f> : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x <bp1> <bp2> [\\'no-fail\\']\n  --split-x <build> [\\'no-fail\\'] :\n    Changes chromosome code of all chrX variants with bp position <= bp1 or >=\n    bp2 to XY.  The following build codes are supported as shorthand:\n    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n    By default, PLINK errors out when no variants would be affected by\n    --split-x; the \\'no-fail\\'modifier (useful in scripts) overrides this.\n  --merge-x [\\'no-fail\\'] : Merge XY chromosome back with X.\n  --set-me-missing      : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2     : Cause --make-bed to replace all missing calls with\n                          homozygous A2 calls.\n  --set-missing-var-ids <t>   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len <n> : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code <string> : Change unnamed variant code (default \\'.\\').\n  --update-chr  <f> [chrcol] [IDcol]  [skip] : Update variant chromosome codes.\n  --update-cm   <f> [cmcol]  [IDcol]  [skip] : Update centimorgan positions.\n  --update-map  <f> [bpcol]  [IDcol]  [skip] : Update variant bp positions.\n  --update-name <f> [newcol] [oldcol] [skip] : Update variant IDs.\n  --update-alleles <fname> : Update variant allele codes.\n  --allele1234 [\\'multichar\\'] : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                               With \\'multichar\\', converts all A/C/G/Ts in\n                               allele names to 1/2/3/4s.\n  --alleleACGT [\\'multichar\\'] : Reverse of --allele1234.\n  --update-ids <f>     : Update sample IDs.\n  --update-parents <f> : Update parental IDs.\n  --update-sex <f> [n] : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip <filename>    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset <fn>   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window <ct+1> : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb <x> : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold <x> : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{,-fid,-iid}.\n  --a1-allele <f> [a1col] [IDcol] [skip] : Force alleles in the file to A1.\n  --a2-allele <filename> [a2col] [IDcol] [skip] :\n    Force alleles in the file to A2.  (\"--a2-allele <VCF filename> 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort <m> [f] : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype [\\'no-parents\\'] [{no-sex | female-2}] :\n    Include more sample info in new .cov file.\n  --dummy-coding [N] [\\'no-round\\'] : Split categorical variables (n categories,\n                                    2 < n <= N, default N is 49) into n-1\n                                    binary dummy variables when writing\n                                    covariate file.\n  --merge-mode <n>   : Adjust --{,b}merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window <ct+1> : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb <x> : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm <x> : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 <x> : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp <var ID>  : Set first variant in all --r/--r2 pairs.\n  --ld-snps <vID...> : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list <f>  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb <kbs>     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 <val>     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr <code>   : Set chrX model for --indep{,-pairwise}, --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb <kbs>      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf <cutoff>  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci <x>  : Set --blocks \"strong LD\" CI thresholds (defaults\n  --blocks-strong-highci <x>   0.70 and 0.98).\n  --blocks-recomb-highci <x> : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac <x>   : Force haploblock <strong LD pairs>:<total\n                               informative pairs> ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=<x>        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists <dist file> [id file] : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap <val>    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min <cutoff>     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max <cutoff>     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match <> : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size <ct>   : Set minimum size of pools in \"--homozyg group\" report.\n  --read-genome <fn> : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc <p-val>    : Specify minimum PPC test p-value within a cluster.\n  --mc <max size>  : Specify maximum cluster size.\n  --mcc <c1> <c2>  : Specify maximum case and control counts per cluster.\n  --K <min count>  : Specify minimum cluster count.\n  --ibm <val>      : Specify minimum identity-by-missingness.\n  --match <f> [mv] : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type <f> : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \"negative matches\" (i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \"positive matches\" (default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch <f> [m] : Force all members of a cluster to have similar\n  --qt <fname>       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names <...> : These can be used individually or in combination\n  --pca-clusters <fname>      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot <dims> [\\'by-cluster\\'] [\\'eigendecomp\\'] [\\'eigvals\\'] :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell <thresh>  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition <var ID> [{dominant | recessive}] : Add one variant as a --linear\n                                                  or --logistic covariate.\n  --condition-list <f> [{dominant | recessive}] : Add variants named in the\n                                                  file as --linear/--logistic\n                                                  covariates.\n  --parameters <...>  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> [...] : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \"--tests all\" to include all terms.\n  --vif <max VIF>     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model <code> : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars [cov(s)...] : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust [\\'gc\\'] [\\'log10\\'] [\\'qq-plot\\'] : Report some multiple-testing\n                                          corrections.\n  --lambda <val>   : Set genomic control lambda for --adjust.\n  --ci <size>      : Report confidence intervals for odds ratios.\n  --pfilter <val>  : Filter out association test results with higher p-values.\n  --aperm <min perms - 1> [max perms] [alpha] [beta] [init interval] [slope] :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      <slope>p + <init interval> more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p <p-val>        : Adjust set test significant variant p-value ceiling\n                           (default 0.05).\n  --set-r2 [v] [\\'write\\'] : Adjust set test significant variant pairwise r^2\n                           ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to <output prefix>.ldset.\n  --set-max <ct>         : Adjust set test maximum # of significant variants\n                           considered per set (default 5).\n  --set-test-lambda <v>  : Specify genomic control correction for set test.\n  --border <kbs>            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field <nm> : Set --annotate variant ID field name.\n  --clump-p1 <pval> : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 <pval> : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 <r^2>  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb <kbs>  : Set --clump kb radius (default 250).\n  --clump-snp-field <n...>  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field <name...>   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate <hdr...> : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range <filename>  : Report overlaps between clumps and regions.\n  --clump-range-border <kb> : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-snp-field <n...> : Set --meta-analysis variant ID, A1/A2\n  --meta-analysis-a1-field <n...>    allele, p-value, standard error, and/or\n  --meta-analysis-a2-field <n...>    effective sample size field names.\n  --meta-analysis-p-field <n...>     Defaults are \\'SNP\\', \\'A1\\', \\'A2\\', \\'P\\', \\'SE\\',\n  --meta-analysis-se-field <n...>    and \\'NMISS\\', respectively.  When multiple\n  --meta-analysis-ess-field <n...>   parameters are given to these flags,\n                                     earlier names take precedence over later\n                                     ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/<# cases> + 1/<# controls>).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border <kbs>   : Extend --gene-report regions by given # of kbs.\n  --gene-subset <filename>   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field <> : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap <kbs>      : Set \"--fast-epistasis case-only\" min. gap (default 1000).\n  --epi1 <p-value> : Set --{,fast-}epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 <p-value> : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin <n> : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range <range file> <data file> [i] [j] [\\'header\\'] :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --R-port <port #>  : Connect to Rserve on a port other than 6311.\n  --R-host <host>    : Connect to Rserve host.\n  --R-socket <sock>  : Connect to Rserve socket.\n  --parallel <k> <n> : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory <val>     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads <val>    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d <char>         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed <val...>    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size <val> : Set number of permutations per batch for some\n                            permutation tests.\n  --output-min-p <p> : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink/1.9 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"messages": [{"role": "user", "content":   "plink-1.90b6.18"}, {"role": "assistant", "content":  "PLINK v1.90b6.18 64-bit (16 Jun 2020)          www.cog-genomics.org/plink/1.9/\n(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * <angle brackets> denote a required parameter, where the text between the\n    angle brackets describes its nature.\n  * [\\'square brackets + single-quotes\\'] denotes an optional modifier.  Use the\n    EXACT text in the quotes.\n  * [{bar|separated|braced|bracketed|values}] denotes a collection of mutually\n    exclusive optional modifiers (again, the exact text must be used).  When\n    there are no outer square brackets, one of the choices must be selected.\n  * [\\'quoted_text=\\'<description of value>] denotes an optional modifier that\n    must begin with the quoted text, and be followed by a value with no\n    whitespace in between.  \\'|\\'may also be used here to indicate mutually\n    exclusive options.\n  * [square brackets without quotes or braces] denote an optional parameter,\n    where the text between the brackets describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]\n  plink --help [flag name(s)...]\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile [prefix] : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed <filename> : Specify full name of .bed file.\n  --bim <filename> : Specify full name of .bim file.\n  --fam <filename> : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file [prefix]  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped <filename> : Specify full name of .ped file.\n  --map <filename> : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile [prefix] : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped <fname>   : Specify full name of .tped file.\n  --tfam <fname>   : Specify full name of .tfam file.\n\n  --lfile [prefix] : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen <fname>   : Specify full name of .lgen file.\n  --reference <fn> : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf <filename> : Specify full name of .vcf or .vcf.gz file.\n  --bcf <filename> : Specify full name of BCF2 file.\n\n  --data [prefix]  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen <filename> : Specify full name of .gen or .gen.gz file.\n  --bgen <f> [\\'snpid-chr\\'] : Specify full name of .bgen file.\n  --sample <fname> : Specify full name of .sample file.\n\n  --23file <fname> [FID] [IID] [sex] [pheno] [pat. ID] [mat. ID] :\n    Specify 23andMe input file.\n\n  --grm-gz [prfx]  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin [prfx] : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy <sample ct> <SNP ct> [missing geno freq] [missing pheno freq]\n          [{acgt | 1234 | 12}] [\\'scalar-pheno\\']\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate <simulation parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n  --simulate-qt <sim. parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.<extension>\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out <prefix>   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode <output format> [{01 | 12}] [{tab | tabx | spacex | bgz | gen-gz}]\n           [\\'include-alt\\'] [\\'omit-nonmale-y\\']\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-<chr #>.recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-<chr #>{.ped,.info}\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan [\\'verbose\\']\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster [\\'omit-unassigned\\']\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to <output prefix>.set, while --set-table writes a\n    variant-by-set membership table to <output prefix>.set.table.\n\n  --merge <.ped filename> <.map filename>\n  --merge <text fileset prefix>\n  --bmerge <.bed filename> <.bim filename> <.fam filename>\n  --bmerge <binary fileset prefix>\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to <output prefix>.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list <filename>\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars [\\'require-same-ref\\'] [\\'ids-only\\'] [\\'suppress-first\\']\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq [{counts | case-control}] [\\'gz\\']\n  --freqx [\\'gz\\']\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing [\\'gz\\']\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n    Unlike most other commands, this doesn\\'t treat het. haploids as missing.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy [\\'midp\\'] [\\'gz\\']\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel [\\'summaries-only\\']\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het [\\'small-sample\\'] [\\'gz\\']\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex [female max F] [male min F]\n  --check-sex ycount [female max F] [male min F] [female max Y obs]\n                     [male min Y obs]\n  --check-sex y-only [female max Y obs] [male min Y obs]\n  --impute-sex [female max F] [male min F]\n  --impute-sex ycount [female max F] [male min F] [female max Y obs]\n                      [male min Y obs]\n  --impute-sex y-only [female max Y obs] [male min Y obs]\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst [\\'case-control\\']\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep <window size>[\\'kb\\'] <step size (variant ct)> <VIF threshold>\n  --indep-pairwise <window size>[\\'kb\\'] <step size (variant ct)> <r^2 threshold>\n  --indep-pairphase <window size>[\\'kb\\'] <step size (variant ct)> <r^2 thresh>\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \"--indep-pairwise 500 kb 5 0.5\"\n    and \"--indep-pairwise 500kb 5 0.5\" have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n      [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n      [\\'yes-really\\']\n  --r2 [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n       [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n       [\\'yes-really\\']\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld <variant ID> <variant ID> [\\'hwe-midp\\']\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags <filename>\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks [\\'no-pheno-req\\'] [\\'no-small-max-span\\']\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \"--recode beagle\" to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance [{square | square0 | triangle}] [{gz | bin | bin4}] [\\'ibs\\']\n             [\\'1-ibs\\'] [\\'allele-ct\\'] [\\'flat-missing\\']\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to <output prefix>.dist, and a list of the\n    corresponding sample IDs to <output prefix>.dist.id.  The first row of the\n    .dist file contains a single <genome 1-genome 2> distance, the second row\n    has the <genome 1-genome 3> and <genome 2-genome 3> distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to <output prefix>.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to <output prefix>.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to <output prefix>.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \"--distance 1-ibs flat-missing\n    square\" and \"--distance ibs flat-missing square\", respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel [{square | square0 | triangle}] [{gz | bin | bin4}]\n             [{cov | ibc2 | ibc3}]\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to <output prefix>.rel, and corresponding IDs to <output prefix>.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz [\\'no-gz\\'] [{cov | ibc2 | ibc3}]\n  --make-grm-bin [{cov | ibc2 | ibc3}]\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff [val]\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    <output prefix>.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test [permutation count]\n  --groupdist [iters] [d]\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance [iters] [d]\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to <number of people>^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel [iters] [d]\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome [\\'gz\\'] [\\'rel-check\\'] [\\'full\\'] [\\'unbounded\\'] [\\'nudge\\']\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg [{group | group-verbose}] [\\'consensus-match\\'] [\\'extend\\']\n            [\\'subtract-1-from-lengths\\']\n  --homozyg-snp <min var count>\n  --homozyg-kb <min length>\n  --homozyg-density <max inverse density (kb/var)>\n  --homozyg-gap <max internal gap kb length>\n  --homozyg-het <max hets>\n  --homozyg-window-snp <scanning window size>\n  --homozyg-window-het <max hets in scanning window hit>\n  --homozyg-window-missing <max missing calls in scanning window hit>\n  --homozyg-window-threshold <min scanning window hit rate>\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group[-verbose]\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group[-verbose]\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as <end bp position> -\n        <start bp position> + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster [\\'cc\\'] [{group-avg | old-tiebreaks}] [\\'missing\\'] [\\'only2\\']\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca [count] [\\'header\\'] [\\'tabs\\'] [\\'var-wts\\']\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour <n1> <n2>\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [{fisher | fisher-midp}]\n          [\\'counts\\'] [\\'set-test\\']\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'qt-means\\'] [\\'lin\\']\n          [\\'set-test\\']\n  --model [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n          [{fisher | fisher-midp | trend-only}] [\\'set-test\\']\n          [{dom | rec | gen | trend}]\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=\\'<value> causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n    (alias: --cmh)\n  --bd [\\'perm\\'| \\'perm-bd\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe [covariate index]\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \"--gxe 3\" to base them on the third covariate instead.\n\n  --linear [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n           [{genotypic | hethom | dominant | recessive | no-snp}]\n           [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n           [\\'standard-beta\\'] [\\'intercept\\']\n  --logistic [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n             [{genotypic | hethom | dominant | recessive | no-snp}]\n             [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n             [\\'intercept\\']\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=\\'<value> starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage <allele dosage file> [\\'noheader\\'] [\\'skip0=\\'<i>] [\\'skip1=\\'<j>]\n           [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --dosage <list file> list [{sepheader | noheader}] [\\'skip0=\\'<i>]\n           [\\'skip1=\\'<j>] [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modifier.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format=\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso <h2 estimate> [min lambda] [\\'report-zeroes\\']\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'midp\\']\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  (Het. haploids are treated as missing.)\n    The \\'midp\\'modifier causes Lancaster\\'s mid-p adjustment to be applied.\n\n  --make-perm-pheno <ct>\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt [{exact | exact-midp | poo}] [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n        [{parentdt1 | parentdt2 | pat | mat}] [\\'set-test\\']\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=\\'requests a family-based adaptive or max(T) permutation\n      test.  By default, the permutation test statistic is the basic TDT\n      p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined test\n      p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-parents [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-between [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-total [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=\\'<value> just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate <PLINK report> [\\'attrib=\\'<file>] [\\'ranges=\\'<file>]\n             [\\'filter=\\'<file>] [\\'snps=\\'<file>] [{NA | prune}] [\\'block\\']\n             [\\'subset=\\'<file>] [\\'minimal\\'] [\\'distance\\']\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=\\'<file> specifies a (possibly gzipped) attribute file.\n    * \\'ranges=\\'<file> specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=\\'<file> causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=\\'<file> causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=\\'<file> causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump <PLINK report filename(s)...>\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report <PLINK report> <gene range file>\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis <PLINK report filenames...>\n  --meta-analysis <PLINK report filenames...> + [{logscale | qt}]\n                  [{no-map | no-allele}] [\\'study\\'] [\\'report-all\\']\n                  [\\'weighted-z\\']\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis [{boost | joint-effects | no-ueki}] [\\'case-only\\']\n                   [{set-by-set | set-by-all}] [\\'nop\\']\n  --epistasis [{set-by-set | set-by-all}]\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge <common file prefix> <ct>\n    When a --[fast-]epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus <variant ID> <variant ID>\n    Two-locus joint genotype count report.\n\n  --score <filename> [i] [j] [k] [\\'header\\'] [{sum | no-sum}]\n          [{no-mean-imputation | center}] [\\'include-cnt\\'] [\\'double-dosage\\']\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --R <R script file> [\\'debug\\']\n    Connect to a Rserve (preferably version 1.7 or later) background process,\n    and execute the Rplink function defined in the input file.  (Unless the\n    \\'debug\\'modifier is present; in that case, the R commands that PLINK would\n    have tried to execute are logged to a file.)\n\n  --write-var-ranges <block ct>\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink/1.9/order .)\n  --script <fname> : Include command-line options from file.\n  --rerun [log]    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype <char> : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid [ID]     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim [d]       : Parse sample IDs as <FID><d><IID> (default delim \\'_\\').\n  --vcf-idspace-to <c> : Convert spaces in sample IDs to the given character.\n  --biallelic-only [\\'strict\\'] [\\'list\\'] : Skip VCF variants with 2+ ALT alleles.\n  --vcf-min-qual <val>           : Skip VCF variants with low/missing QUAL.\n  --vcf-filter [exception(s)...] : Skip variants which have FILTER failures.\n  --vcf-require-gt               : Skip variants with no GT field.\n  --vcf-min-gq <val>             : No-call a genotype when GQ is below the\n                                   given threshold.\n  --vcf-min-gp <val>             : No-call a genotype when 0-1 scaled GP is\n                                   below the given threshold.\n  --vcf-half-call <m>  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr <chr nm> : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name <col nm> : Import named phenotype from the .sample file.\n  --hard-call-threshold <val>  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code [string list] : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases <num>   : Set --simulate case count (default 1000).\n  --simulate-ncontrols <n>  : Set --simulate control count (default 1000).\n  --simulate-prevalence <p> : Set --simulate disease prevalence (default 0.01).\n  --simulate-n <num>        : Set --simulate-qt sample count (default 1000).\n  --simulate-label <prefix> : Set --simulate[-qt] FID/IID name prefix.\n  --simulate-missing <freq> : Set --simulate[-qt] missing genotype frequency.\n  --allow-extra-chr [\\'0\\']   : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set <autosome ct> [\\'no-x\\'] [\\'no-y\\'] [\\'no-xy\\'] [\\'no-mt\\'] :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num <value>    : Alias for \"--chr-set <value> no-y no-xy no-mt\".\n  --cm-map <fname pattern> [chr] : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --allow-no-samples : Allow the input fileset to contain no samples.\n  --allow-no-vars    : Allow the input fileset to contain no variants.\n  --pheno <fname>  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno <n>     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name <c> : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype <v> : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.  This also\n                            forces phenotypes to be interpreted as case/ctrl.\n  --make-pheno <fn> <val> : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno <Lt> [Hbt] : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar <filename> [\\'keep-pheno-on-missing-cov\\'] : Specify covariate file.\n  --covar-name <...>       : Specify covariate(s) in --covar file by name.\n                             Separate multiple names with spaces or commas, and\n                             use dashes to designate ranges.\n  --covar-number <...>     : Specify covariate(s) in --covar file by index.\n  --no-const-covar         : Exclude constant covariates.\n  --allow-no-covars        : Allow no covariates to be loaded from --covar\n                             file.\n  --within <f> [\\'keep-NA\\'] : Specify initial cluster assignments.\n  --mwithin <n>            : Load cluster assignments from column n+2.\n  --family                 : Create a cluster for each family ID.\n  --loop-assoc <f> [\\'keep-NA\\']  : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set <filename>              : Load sets from a .set file.\n  --set-names <name(s)...>      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset <filename>           : Load only sets named in the given text file.\n  --set-collapse-all <set name> : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all <s> : --set-collapse-all + inversion.\n  --make-set <filename>         : Define sets from a list of named bp ranges.\n  --make-set-border <kbs>       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep <filename>       : Exclude all samples not named in the file.\n  --remove <filename>     : Exclude all samples named in the file.\n  --keep-fam <filename>   : Exclude all families not named in the file.\n  --remove-fam <filename> : Exclude all families named in the file.\n  --extract [\\'range\\'] <f> : Exclude all variants not named in the file.\n  --exclude [\\'range\\'] <f> : Exclude all variants named in the file.\n  --keep-clusters <filename>          : These can be used individually or in\n  --keep-cluster-names <name(s)...>     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters <filename>        : Exclude all clusters named in the file.\n  --remove-cluster-names <name(s)...> : Exclude the named clusters.\n  --gene <sets...> : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib <f> [att lst] : Given a file assigning attributes to variants, and a\n  --attrib-indiv <f> [a]   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \"negative match conditions\" instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr <chrs...>  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \"--chr 1-4, 22, xy\".\n  --not-chr <...>  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only [\\'just-acgt\\'] : Exclude non-SNP variants.  By default, SNP = both\n                              allele codes are single-character; \\'just-acgt\\'\n                              restricts codes to {A,C,G,T,a,c,g,t,<missing>}.\n  --from <var ID>  : Use ID(s) to specify a variant range to load.  When used\n  --to   <var ID>    together, both variants must be on the same chromosome.\n  --snp  <var ID>  : Specify a single variant to load.\n  --exclude-snp <> : Specify a single variant to exclude.\n  --window  <kbs>  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp <pos>  : Use physical position(s) to define a variant range to\n  --to-bp   <pos>    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb <pos>    values.  You must also specify a single chromosome (using\n  --to-kb   <pos>    e.g. --chr) when using these flags.\n  --from-mb <pos>\n  --to-mb   <pos>\n  --snps <var IDs...>  : Use IDs to specify variant range(s) to load or\n  --exclude-snps <...>   exclude.  E.g. \"--snps rs1111-rs2222, rs3333, rs4444\".\n  --thin <p>       : Randomly remove variants, retaining each with prob. p.\n  --thin-count <n> : Randomly remove variants until n of them remain.\n  --bp-space <bps> : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv <p>         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count <n>   : Randomly remove samples until n of them remain.\n  --filter <f> <val(s)...> : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter <n>            : Match against (n+2)th column instead.\n  --geno [val]     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind [val]     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing <f1> <f2> : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf [freq]        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf <freq>    : Exclude variants with MAF greater than the threshold.\n  --mac <ct>          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac <ct>      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq <fn> : Estimate MAFs and heterozygote frequencies from the given\n                     --freq[x] report, instead of the input fileset.\n  --hwe <p> [\\'midp\\'] [\\'include-nonctrl\\'] : Exclude variants with Hardy-Weinberg\n                                           equilibrium exact test p-values\n                                           below a threshold.\n  --me <t> <v> [\\'var-first\\'] : Filter out trios and variants with Mendel error\n                               rates exceeding the given thresholds.\n  --me-exclude-one [ratio]   : Make --me exclude only one sample per trio.\n  --qual-scores <f> [qcol] [IDcol] [skip] : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold <min qual score>       : Set --qual-scores range floor.\n  --qual-max-threshold <max qual score>   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders [\\'require-2-missing\\'] [\\'first\\'] :\n    Clear parental IDs for those with 1+ missing parent(s).\n  --recode-allele <fn> : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr <MT code> : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype <ch> : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype <s> : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster <f> : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x <bp1> <bp2> [\\'no-fail\\']\n  --split-x <build> [\\'no-fail\\'] :\n    Changes chromosome code of all chrX variants with bp position <= bp1 or >=\n    bp2 to XY.  The following build codes are supported as shorthand:\n    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n    By default, PLINK errors out when no variants would be affected by\n    --split-x; the \\'no-fail\\'modifier (useful in scripts) overrides this.\n  --merge-x [\\'no-fail\\'] : Merge XY chromosome back with X.\n  --set-me-missing      : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2     : Cause --make-bed to replace all missing calls with\n                          homozygous A2 calls.\n  --set-missing-var-ids <t>   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len <n> : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code <string> : Change unnamed variant code (default \\'.\\').\n  --update-chr  <f> [chrcol] [IDcol]  [skip] : Update variant chromosome codes.\n  --update-cm   <f> [cmcol]  [IDcol]  [skip] : Update centimorgan positions.\n  --update-map  <f> [bpcol]  [IDcol]  [skip] : Update variant bp positions.\n  --update-name <f> [newcol] [oldcol] [skip] : Update variant IDs.\n  --update-alleles <fname> : Update variant allele codes.\n  --allele1234 [\\'multichar\\'] : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                               With \\'multichar\\', converts all A/C/G/Ts in\n                               allele names to 1/2/3/4s.\n  --alleleACGT [\\'multichar\\'] : Reverse of --allele1234.\n  --update-ids <f>     : Update sample IDs.\n  --update-parents <f> : Update parental IDs.\n  --update-sex <f> [n] : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip <filename>    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset <fn>   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window <ct+1> : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb <x> : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold <x> : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{,-fid,-iid}.\n  --a1-allele <f> [a1col] [IDcol] [skip] : Force alleles in the file to A1.\n  --a2-allele <filename> [a2col] [IDcol] [skip] :\n    Force alleles in the file to A2.  (\"--a2-allele <VCF filename> 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort <m> [f] : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype [\\'no-parents\\'] [{no-sex | female-2}] :\n    Include more sample info in new .cov file.\n  --dummy-coding [N] [\\'no-round\\'] : Split categorical variables (n categories,\n                                    2 < n <= N, default N is 49) into n-1\n                                    binary dummy variables when writing\n                                    covariate file.\n  --merge-mode <n>   : Adjust --[b]merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window <ct+1> : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb <x> : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm <x> : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 <x> : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp <var ID>  : Set first variant in all --r/--r2 pairs.\n  --ld-snps <vID...> : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list <f>  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb <kbs>     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 <val>     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr <code>   : Set chrX model for --indep[-pairwise], --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb <kbs>      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf <cutoff>  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci <x>  : Set --blocks \"strong LD\" CI thresholds (defaults\n  --blocks-strong-highci <x>   0.70 and 0.98).\n  --blocks-recomb-highci <x> : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac <x>   : Force haploblock <strong LD pairs>:<total\n                               informative pairs> ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=<x>        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists <dist file> [id file] : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap <val>    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min <cutoff>     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max <cutoff>     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match <> : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size <ct>   : Set minimum size of pools in \"--homozyg group\" report.\n  --read-genome <fn> : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc <p-val>    : Specify minimum PPC test p-value within a cluster.\n  --mc <max size>  : Specify maximum cluster size.\n  --mcc <c1> <c2>  : Specify maximum case and control counts per cluster.\n  --K <min count>  : Specify minimum cluster count.\n  --ibm <val>      : Specify minimum identity-by-missingness.\n  --match <f> [mv] : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type <f> : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \"negative matches\" (i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \"positive matches\" (default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch <f> [m] : Force all members of a cluster to have similar\n  --qt <fname>       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names <...> : These can be used individually or in combination\n  --pca-clusters <fname>      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot <dims> [\\'by-cluster\\'] [\\'eigendecomp\\'] [\\'eigvals\\'] :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell <thresh>  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition <var ID> [{dominant | recessive}] : Add one variant as a --linear\n                                                  or --logistic covariate.\n  --condition-list <f> [{dominant | recessive}] : Add variants named in the\n                                                  file as --linear/--logistic\n                                                  covariates.\n  --parameters <...>  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> [...] : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \"--tests all\" to include all terms.\n  --vif <max VIF>     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model <code> : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars [cov(s)...] : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust [\\'gc\\'] [\\'log10\\'] [\\'qq-plot\\'] : Report some multiple-testing\n                                          corrections.\n  --lambda <val>   : Set genomic control lambda for --adjust.\n  --ci <size>      : Report confidence intervals for odds ratios.\n  --pfilter <val>  : Filter out association test results with higher p-values.\n  --aperm <min perms - 1> [max perms] [alpha] [beta] [init interval] [slope] :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      <slope>p + <init interval> more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p <p-val>        : Adjust set test significant variant p-value ceiling\n                           (default 0.05).\n  --set-r2 [v] [\\'write\\'] : Adjust set test significant variant pairwise r^2\n                           ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to <output prefix>.ldset.\n  --set-max <ct>         : Adjust set test maximum # of significant variants\n                           considered per set (default 5).\n  --set-test-lambda <v>  : Specify genomic control correction for set test.\n  --border <kbs>            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field <nm> : Set --annotate variant ID field name.\n  --clump-p1 <pval> : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 <pval> : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 <r^2>  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb <kbs>  : Set --clump kb radius (default 250).\n  --clump-snp-field <n...>  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field <name...>   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate <hdr...> : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range <filename>  : Report overlaps between clumps and regions.\n  --clump-range-border <kb> : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-chr-field <n...> : Set --meta-analysis chromosome, variant\n  --meta-analysis-snp-field <n...>   ID, position, A1/A2 allele, p-value,\n  --meta-analysis-bp-field <n...>    standard error, and/or effective sample\n  --meta-analysis-a1-field <n...>    size field names.\n  --meta-analysis-a2-field <n...>    Defaults are \\'CHR\\', \\'SNP\\', \\'BP\\', \\'A1\\',\n  --meta-analysis-p-field <n...>     \\'A2\\', \\'P\\', \\'SE\\', and \\'NMISS\\',\n  --meta-analysis-se-field <n...>    respectively.  When multiple parameters\n  --meta-analysis-ess-field <n...>   are given to these flags, earlier names\n                                     take precedence over later ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/<# cases> + 1/<# controls>).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border <kbs>   : Extend --gene-report regions by given # of kbs.\n  --gene-subset <filename>   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field <> : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap <kbs>      : Set \"--fast-epistasis case-only\" min. gap (default 1000).\n  --epi1 <p-value> : Set --[fast-]epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 <p-value> : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin <n> : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range <range file> <data file> [i] [j] [\\'header\\'] :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --R-port <port #>  : Connect to Rserve on a port other than 6311.\n  --R-host <host>    : Connect to Rserve host.\n  --R-socket <sock>  : Connect to Rserve socket.\n  --parallel <k> <n> : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory <val>     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads <val>    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d <char>         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed <val...>    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size <val> : Set number of permutations per batch for some\n                            permutation tests.\n  --output-min-p <p> : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink/1.9 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"messages": [{"role": "user", "content":   "plink-1.90b6.21"}, {"role": "assistant", "content":  "PLINK v1.90b6.21 64-bit (19 Oct 2020)          www.cog-genomics.org/plink/1.9/\n(C) 2005-2020 Shaun Purcell, Christopher Chang   GNU General Public License v3\n\nIn the command line flag definitions that follow,\n  * <angle brackets> denote a required parameter, where the text between the\n    angle brackets describes its nature.\n  * [\\'square brackets + single-quotes\\'] denotes an optional modifier.  Use the\n    EXACT text in the quotes.\n  * [{bar|separated|braced|bracketed|values}] denotes a collection of mutually\n    exclusive optional modifiers (again, the exact text must be used).  When\n    there are no outer square brackets, one of the choices must be selected.\n  * [\\'quoted_text=\\'<description of value>] denotes an optional modifier that\n    must begin with the quoted text, and be followed by a value with no\n    whitespace in between.  \\'|\\'may also be used here to indicate mutually\n    exclusive options.\n  * [square brackets without quotes or braces] denote an optional parameter,\n    where the text between the brackets describes its nature.\n  * An ellipsis (...) indicates that you may enter multiple parameters of the\n    specified type.\n\n  plink <input flag(s)...> [command flag(s)...] [other flag(s)...]\n  plink --help [flag name(s)...]\n\nMost PLINK runs require exactly one main input fileset.  The following flags\nare available for defining its form and location:\n\n  --bfile [prefix] : Specify .bed + .bim + .fam prefix (default \\'plink\\').\n  --bed <filename> : Specify full name of .bed file.\n  --bim <filename> : Specify full name of .bim file.\n  --fam <filename> : Specify full name of .fam file.\n\n  --keep-autoconv  : With --file/--tfile/--lfile/--vcf/--bcf/--data/--23file,\n                     don\\'t delete autogenerated binary fileset at end of run.\n\n  --file [prefix]  : Specify .ped + .map filename prefix (default \\'plink\\').\n  --ped <filename> : Specify full name of .ped file.\n  --map <filename> : Specify full name of .map file.\n\n  --no-fid         : .fam/.ped file does not contain column 1 (family ID).\n  --no-parents     : .fam/.ped file does not contain columns 3-4 (parents).\n  --no-sex         : .fam/.ped file does not contain column 5 (sex).\n  --no-pheno       : .fam/.ped file does not contain column 6 (phenotype).\n\n  --tfile [prefix] : Specify .tped + .tfam filename prefix (default \\'plink\\').\n  --tped <fname>   : Specify full name of .tped file.\n  --tfam <fname>   : Specify full name of .tfam file.\n\n  --lfile [prefix] : Specify .lgen + .map + .fam (long-format fileset) prefix.\n  --lgen <fname>   : Specify full name of .lgen file.\n  --reference <fn> : Specify default allele file accompanying .lgen input.\n  --allele-count   : When used with --lfile/--lgen + --reference, specifies\n                     that the .lgen file contains reference allele counts.\n\n  --vcf <filename> : Specify full name of .vcf or .vcf.gz file.\n  --bcf <filename> : Specify full name of BCF2 file.\n\n  --data [prefix]  : Specify Oxford .gen + .sample prefix (default \\'plink\\').\n  --gen <filename> : Specify full name of .gen or .gen.gz file.\n  --bgen <f> [\\'snpid-chr\\'] : Specify full name of .bgen file.\n  --sample <fname> : Specify full name of .sample file.\n\n  --23file <fname> [FID] [IID] [sex] [pheno] [pat. ID] [mat. ID] :\n    Specify 23andMe input file.\n\n  --grm-gz [prfx]  : Specify .grm.gz + .grm.id (GCTA rel. matrix) prefix.\n  --grm-bin [prfx] : Specify .grm.bin + .grm.N.bin + .grm.id (GCTA triangular\n                     binary relationship matrix) filename prefix.\n\n  --dummy <sample ct> <SNP ct> [missing geno freq] [missing pheno freq]\n          [{acgt | 1234 | 12}] [\\'scalar-pheno\\']\n    This generates a fake input dataset with the specified number of samples\n    and SNPs.  By default, the missing genotype and phenotype frequencies are\n    zero, and genotypes are As and Bs (change the latter with\n    \\'acgt\\'/\\'1234\\'/\\'12\\').  The \\'scalar-pheno\\'modifier causes a normally\n    distributed scalar phenotype to be generated instead of a binary one.\n\n  --simulate <simulation parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n  --simulate-qt <sim. parameter file> [{tags | haps}] [{acgt | 1234 | 12}]\n    --simulate generates a fake input dataset with disease-associated SNPs,\n    while --simulate-qt generates a dataset with quantitative trait loci.\n\nOutput files have names of the form \\'plink.<extension>\\'by default.  You can\nchange the \\'plink\\'prefix with\n\n  --out <prefix>   : Specify prefix for output files.\n\nMost runs also require at least one of the following commands:\n\n  --make-bed\n    Create a new binary fileset.  Unlike the automatic text-to-binary\n    converters (which only heed chromosome filters), this supports all of\n    PLINK\\'s filtering flags.\n  --make-just-bim\n  --make-just-fam\n    Variants of --make-bed which only write a new .bim or .fam file.  Can be\n    used with only .bim/.fam input.\n    USE THESE CAUTIOUSLY.  It is very easy to desynchronize your binary\n    genotype data and your .bim/.fam indexes if you use these commands\n    improperly.  If you have any doubt, stick with --make-bed.\n\n  --recode <output format> [{01 | 12}] [{tab | tabx | spacex | bgz | gen-gz}]\n           [\\'include-alt\\'] [\\'omit-nonmale-y\\']\n    Create a new text fileset with all filters applied.  The following output\n    formats are supported:\n    * \\'23\\': 23andMe 4-column format.  This can only be used on a single\n      sample\\'s data (--keep may be handy), and does not support multicharacter\n      allele codes.\n    * \\'A\\': Sample-major additive (0/1/2) coding, suitable for loading from R.\n      If you need uncounted alleles to be named in the header line, add the\n      \\'include-alt\\'modifier.\n    * \\'AD\\': Sample-major additive (0/1/2) + dominant (het=1/hom=0) coding.\n      Also supports \\'include-alt\\'.\n    * \\'A-transpose\\': Variant-major 0/1/2.\n    * \\'beagle\\': Unphased per-autosome .dat and .map files, readable by early\n      BEAGLE versions.\n    * \\'beagle-nomap\\': Single .beagle.dat file.\n    * \\'bimbam\\': Regular BIMBAM format.\n    * \\'bimbam-1chr\\': BIMBAM format, with a two-column .pos.txt file.  Does not\n      support multiple chromosomes.\n    * \\'fastphase\\': Per-chromosome fastPHASE files, with\n      .chr-<chr #>.recode.phase.inp filename extensions.\n    * \\'fastphase-1chr\\': Single .recode.phase.inp file.  Does not support\n      multiple chromosomes.\n    * \\'HV\\': Per-chromosome Haploview files, with .chr-<chr #>{.ped,.info}\n      filename extensions.\n    * \\'HV-1chr\\': Single Haploview .ped + .info file pair.  Does not support\n      multiple chromosomes.\n    * \\'lgen\\': PLINK 1 long-format (.lgen + .fam + .map), loadable with --lfile.\n    * \\'lgen-ref\\': .lgen + .fam + .map + .ref, loadable with --lfile +\n       --reference.\n    * \\'list\\': Single genotype-based list, up to 4 lines per variant.  To omit\n      nonmale genotypes on the Y chromosome, add the \\'omit-nonmale-y\\'modifier.\n    * \\'rlist\\': .rlist + .fam + .map fileset, where the .rlist file is a\n      genotype-based list which omits the most common genotype for each\n      variant.  Also supports \\'omit-nonmale-y\\'.\n    * \\'oxford\\': Oxford-format .gen + .sample.  With the \\'gen-gz\\'modifier, the\n      .gen file is gzipped.\n    * \\'ped\\': PLINK 1 sample-major (.ped + .map), loadable with --file.\n    * \\'compound-genotypes\\': Same as \\'ped\\', except that the space between each\n      pair of same-variant allele codes is removed.\n    * \\'structure\\': Structure-format.\n    * \\'transpose\\': PLINK 1 variant-major (.tped + .tfam), loadable with\n      --tfile.\n    * \\'vcf\\', \\'vcf-fid\\', \\'vcf-iid\\': VCFv4.2.  \\'vcf-fid\\'and \\'vcf-iid\\'cause\n      family IDs or within-family IDs respectively to be used for the sample\n      IDs in the last header row, while \\'vcf\\'merges both IDs and puts an\n      underscore between them.  If the \\'bgz\\'modifier is added, the VCF file is\n      block-gzipped.\n      The A2 allele is saved as the reference and normally flagged as not based\n      on a real reference genome (INFO:PR).  When it is important for reference\n      alleles to be correct, you\\'ll also want to include --a2-allele and\n      --real-ref-alleles in your command.\n    In addition,\n    * The \\'12\\'modifier causes A1 (usually minor) alleles to be coded as \\'1\\'\n      and A2 alleles to be coded as \\'2\\', while \\'01\\'maps A1 -> 0 and A2 -> 1.\n    * The \\'tab\\'modifier makes the output mostly tab-delimited instead of\n      mostly space-delimited.  \\'tabx\\'and \\'spacex\\'force all tabs and all\n      spaces, respectively.\n\n  --flip-scan [\\'verbose\\']\n    (alias: --flipscan)\n    LD-based scan for case/control strand inconsistency.\n\n  --write-covar\n    If a --covar file is loaded, --make-bed/--make-just-fam and --recode\n    automatically generate an updated version (with all filters applied).\n    However, if you do not wish to simultaneously generate a new genotype file,\n    you can use --write-covar to just produce a pruned covariate file.\n\n  --write-cluster [\\'omit-unassigned\\']\n    If clusters are specified with --within/--family, this generates a new\n    cluster file (with all filters applied).  The \\'omit-unassigned\\'modifier\n    causes unclustered samples to be omitted from the file; otherwise their\n    cluster is \\'NA\\'.\n\n  --write-set\n  --set-table\n    If sets have been defined, --write-set dumps \\'END\\'-terminated set\n    membership lists to <output prefix>.set, while --set-table writes a\n    variant-by-set membership table to <output prefix>.set.table.\n\n  --merge <.ped filename> <.map filename>\n  --merge <text fileset prefix>\n  --bmerge <.bed filename> <.bim filename> <.fam filename>\n  --bmerge <binary fileset prefix>\n    Merge the given fileset with the initially loaded fileset, writing the\n    result to <output prefix>.bed + .bim + .fam.  (It is no longer necessary to\n    simultaneously specify --make-bed.)\n  --merge-list <filename>\n    Merge all filesets named in the text file with the reference fileset, if\n    one was specified.  (However, this can also be used *without* a reference;\n    in that case, the newly created fileset is then treated as the reference by\n    most other PLINK operations.)  The text file is interpreted as follows:\n    * If a line contains only one name, it is assumed to be the prefix for a\n      binary fileset.\n    * If a line contains exactly two names, they are assumed to be the full\n      filenames for a text fileset (.ped first, then .map).\n    * If a line contains exactly three names, they are assumed to be the full\n      filenames for a binary fileset (.bed, then .bim, then .fam).\n\n  --write-snplist\n  --list-23-indels\n    --write-snplist writes a .snplist file listing the names of all variants\n    which pass the filters and inclusion thresholds you\\'ve specified, while\n    --list-23-indels writes the subset with 23andMe-style indel calls (D/I\n    allele codes).\n\n  --list-duplicate-vars [\\'require-same-ref\\'] [\\'ids-only\\'] [\\'suppress-first\\']\n    --list-duplicate-vars writes a .dupvar file describing all groups of\n    variants with matching positions and allele codes.\n    * By default, A1/A2 allele assignments are ignored; use \\'require-same-ref\\'\n      to override this.\n    * Normally, the report contains position and allele codes.  To remove them\n      (and produce a file directly usable with e.g. --extract/--exclude), use\n      \\'ids-only\\'.  Note that this command will fail in \\'ids-only\\'mode if any\n      of the reported IDs are not unique.\n    * \\'suppress-first\\'causes the first variant ID in each group to be omitted\n      from the report.\n\n  --freq [{counts | case-control}] [\\'gz\\']\n  --freqx [\\'gz\\']\n    --freq generates a basic allele frequency (or count, if the \\'counts\\'\n    modifier is present) report.  This can be combined with --within/--family\n    to produce a cluster-stratified allele frequency/count report instead, or\n    the \\'case-control\\'modifier to report case and control allele frequencies\n    separately.\n    --freqx generates a more detailed genotype count report, designed for use\n    with --read-freq.\n\n  --missing [\\'gz\\']\n    Generate sample- and variant-based missing data reports.  If clusters are\n    defined, the variant-based report is cluster-stratified.  \\'gz\\'causes the\n    output files to be gzipped.\n    Unlike most other commands, this doesn\\'t treat het. haploids as missing.\n\n  --test-mishap\n    Check for association between missing calls and flanking haplotypes.\n\n  --hardy [\\'midp\\'] [\\'gz\\']\n    Generate a Hardy-Weinberg exact test p-value report.  (This does NOT\n    simultaneously filter on the p-value any more; use --hwe for that.)  With\n    the \\'midp\\'modifier, the test applies the mid-p adjustment described in\n    Graffelman J, Moreno V (2013) The mid p-value in exact tests for\n    Hardy-Weinberg Equilibrium.\n\n  --mendel [\\'summaries-only\\']\n    Generate a Mendel error report.  The \\'summaries-only\\'modifier causes the\n    .mendel file (listing every single error) to be skipped.\n\n  --het [\\'small-sample\\'] [\\'gz\\']\n  --ibc\n    Estimate inbreeding coefficients.  --het reports method-of-moments\n    estimates, while --ibc calculates all three values described in Yang J, Lee\n    SH, Goddard ME and Visscher PM (2011) GCTA: A Tool for Genome-wide Complex\n    Trait Analysis.  (That paper also describes the relationship matrix\n    computation we reimplement.)\n    * These functions require decent MAF estimates.  If there are very few\n      samples in your immediate fileset, --read-freq is practically mandatory\n      since imputed MAFs are wildly inaccurate in that case.\n    * They also assume the marker set is in approximate linkage equilibrium.\n    * By default, --het omits the n/(n-1) multiplier in Nei\\'s expected\n      homozygosity formula.  The \\'small-sample\\'modifier causes it to be\n      included, while forcing --het to use MAFs imputed from founders in the\n      immediate dataset.\n\n  --check-sex [female max F] [male min F]\n  --check-sex ycount [female max F] [male min F] [female max Y obs]\n                     [male min Y obs]\n  --check-sex y-only [female max Y obs] [male min Y obs]\n  --impute-sex [female max F] [male min F]\n  --impute-sex ycount [female max F] [male min F] [female max Y obs]\n                      [male min Y obs]\n  --impute-sex y-only [female max Y obs] [male min Y obs]\n    --check-sex normally compares sex assignments in the input dataset with\n    those imputed from X chromosome inbreeding coefficients.\n    * Make sure that the X chromosome pseudo-autosomal region has been split\n      off (with e.g. --split-x) before using this.\n    * You also need decent MAF estimates (so, with very few samples in your\n      immediate fileset, use --read-freq), and your marker set should be in\n      approximate linkage equilibrium.\n    * By default, F estimates smaller than 0.2 yield female calls, and values\n      larger than 0.8 yield male calls.  If you pass numeric parameter(s) to\n      --check-sex, the first two control these thresholds.\n    There are now two modes which consider Y chromosome data.\n    * In \\'ycount\\'mode, gender is still imputed from the X chromosome, but\n      female calls are downgraded to ambiguous whenever more than 0 nonmissing\n      Y genotypes are present, and male calls are downgraded when fewer than 0\n      are present.  (Note that these are counts, not rates.)  These thresholds\n      are controllable with --check-sex ycount\\'s optional 3rd and 4th numeric\n      parameters.\n    * In \\'y-only\\'mode, gender is imputed from nonmissing Y genotype counts.\n      The male minimum threshold defaults to 1 instead of zero in this case.\n    --impute-sex changes sex assignments to the imputed values, and is\n    otherwise identical to --check-sex.  It must be used with\n    --make-bed/--recode/--write-covar.\n\n  --fst [\\'case-control\\']\n    (alias: --Fst)\n    Estimate Wright\\'s Fst for each autosomal diploid variant using the method\n    introduced in Weir BS, Cockerham CC (1984) Estimating F-statistics for the\n    analysis of population structure, given a set of subpopulations defined via\n    --within.  Raw and weighted global means are also reported.\n    * If you\\'re interested in the global means, it is usually best to perform\n      this calculation on a marker set in approximate linkage equilibrium.\n    * If you have only two subpopulations, you can represent them with\n      case/control status and use the \\'case-control\\'modifier.\n\n  --indep <window size>[\\'kb\\'] <step size (variant ct)> <VIF threshold>\n  --indep-pairwise <window size>[\\'kb\\'] <step size (variant ct)> <r^2 threshold>\n  --indep-pairphase <window size>[\\'kb\\'] <step size (variant ct)> <r^2 thresh>\n    Generate a list of markers in approximate linkage equilibrium.  With the\n    \\'kb\\'modifier, the window size is in kilobase instead of variant count\n    units.  (Pre-\\'kb\\'space is optional, i.e. \"--indep-pairwise 500 kb 5 0.5\"\n    and \"--indep-pairwise 500kb 5 0.5\" have the same effect.)\n    Note that you need to rerun PLINK using --extract or --exclude on the\n    .prune.in/.prune.out file to apply the list to another computation.\n\n  --r [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n      [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n      [\\'yes-really\\']\n  --r2 [{square | square0 | triangle | inter-chr}] [{gz | bin | bin4}]\n       [\\'spaces\\'] [\\'in-phase\\'] [{d | dprime | dprime-signed}] [\\'with-freqs\\']\n       [\\'yes-really\\']\n    LD statistic reports.  --r yields raw inter-variant correlations, while\n    --r2 reports their squares.  You can request results for all pairs in\n    matrix format (if you specify \\'bin\\'or one of the shape modifiers), all\n    pairs in table format (\\'inter-chr\\'), or a limited window in table format\n    (default).\n    * The \\'gz\\'modifier causes the output text file to be gzipped.\n    * \\'bin\\'causes the output matrix to be written in double-precision binary\n      format, while \\'bin4\\'specifics single-precision binary.  The matrix is\n      square if no shape is explicitly specified.\n    * By default, text matrices are tab-delimited; \\'spaces\\'switches this.\n    * \\'in-phase\\'adds a column with in-phase allele pairs to table-formatted\n      reports.  (This cannot be used with very long allele codes.)\n    * \\'dprime\\'adds the absolute value of Lewontin\\'s D-prime statistic to\n      table-formatted reports, and forces both r/r^2 and D-prime to be based on\n      the maximum likelihood solution to the cubic equation discussed in Gaunt\n      T, Rodriguez S, Day I (2007) Cubic exact solutions for the estimation of\n      pairwise haplotype frequencies.\n      \\'dprime-signed\\'keeps the sign, while \\'d\\'skips division by D_{max}.\n    * \\'with-freqs\\'adds MAF columns to table-formatted reports.\n    * Since the resulting file can easily be huge, you\\'re required to add the\n      \\'yes-really\\'modifier when requesting an unfiltered, non-distributed all\n      pairs computation on more than 400k variants.\n    * These computations can be subdivided with --parallel (even when the\n      \\'square\\'modifier is active).\n  --ld <variant ID> <variant ID> [\\'hwe-midp\\']\n    This displays haplotype frequencies, r^2, and D\\'for a single pair of\n    variants.  When there are multiple biologically possible solutions to the\n    haplotype frequency cubic equation, all are displayed (instead of just the\n    maximum likelihood solution identified by --r/--r2), along with HWE exact\n    test statistics.\n\n  --show-tags <filename>\n  --show-tags all\n    * If a file is specified, list all variants which tag at least one variant\n      named in the file.  (This will normally be a superset of the original\n      list, since a variant is considered to tag itself here.)\n    * If \\'all\\'mode is specified, for each variant, each *other* variant which\n      tags it is reported.\n\n  --blocks [\\'no-pheno-req\\'] [\\'no-small-max-span\\']\n    Estimate haplotype blocks, via Haploview\\'s interpretation of the block\n    definition suggested by Gabriel S et al. (2002) The Structure of Haplotype\n    Blocks in the Human Genome.\n    * Normally, samples with missing phenotypes are not considered by this\n      computation; the \\'no-pheno-req\\'modifier lifts this restriction.\n    * Normally, size-2 blocks may not span more than 20kb, and size-3 blocks\n      are limited to 30kb.  The \\'no-small-max-span\\'modifier removes these\n      limits.\n    The .blocks file is valid input for PLINK 1.07\\'s --hap command.  However,\n    the --hap... family of flags has not been reimplemented in PLINK 1.9 due to\n    poor phasing accuracy relative to other software; for now, we recommend\n    using BEAGLE instead of PLINK for case/control haplotype association\n    analysis.  (You can use \"--recode beagle\" to export data to BEAGLE 3.3.)\n    We apologize for the inconvenience, and plan to develop variants of the\n    --hap... flags which handle pre-phased data effectively.\n\n  --distance [{square | square0 | triangle}] [{gz | bin | bin4}] [\\'ibs\\']\n             [\\'1-ibs\\'] [\\'allele-ct\\'] [\\'flat-missing\\']\n    Write a lower-triangular tab-delimited table of (weighted) genomic\n    distances in allele count units to <output prefix>.dist, and a list of the\n    corresponding sample IDs to <output prefix>.dist.id.  The first row of the\n    .dist file contains a single <genome 1-genome 2> distance, the second row\n    has the <genome 1-genome 3> and <genome 2-genome 3> distances in that\n    order, etc.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * If the \\'square\\'or \\'square0\\'modifier is present, a square matrix is\n      written instead; \\'square0\\'fills the upper right triangle with zeroes.\n    * If the \\'gz\\'modifier is present, a compressed .dist.gz file is written\n      instead of a plain text file.\n    * If the \\'bin\\'modifier is present, a binary (square) matrix of\n      double-precision floating point values, suitable for loading from R, is\n      instead written to <output prefix>.dist.bin.  (\\'bin4\\'specifies\n      single-precision numbers instead.)  This can be combined with \\'square0\\'\n      if you still want the upper right zeroed out, or \\'triangle\\'if you don\\'t\n      want to pad the upper right at all.\n    * If the \\'ibs\\'modifier is present, an identity-by-state matrix is written\n      to <output prefix>.mibs.  \\'1-ibs\\'causes distances expressed as genomic\n      proportions (i.e. 1 - IBS) to be written to <output prefix>.mdist.\n      Combine with \\'allele-ct\\'if you want to generate the usual .dist file as\n      well.\n    * By default, distance rescaling in the presence of missing genotype calls\n      is sensitive to allele count distributions: if variant A contributes, on\n      average, twice as much to other pairwise distances as variant B, a\n      missing call at variant A will result in twice as large of a missingness\n      correction.  To turn this off (because e.g. your missing calls are highly\n      nonrandom), use the \\'flat-missing\\'modifier.\n    * The computation can be subdivided with --parallel.\n  --distance-matrix\n  --ibs-matrix\n    These deprecated commands are equivalent to \"--distance 1-ibs flat-missing\n    square\" and \"--distance ibs flat-missing square\", respectively, except that\n    they generate space- instead of tab-delimited text matrices.\n\n  --make-rel [{square | square0 | triangle}] [{gz | bin | bin4}]\n             [{cov | ibc2 | ibc3}]\n    Write a lower-triangular variance-standardized realized relationship matrix\n    to <output prefix>.rel, and corresponding IDs to <output prefix>.rel.id.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * \\'square\\', \\'square0\\', \\'triangle\\', \\'gz\\', \\'bin\\', and \\'bin4\\'act as they do\n      on --distance.\n    * The \\'cov\\'modifier removes the variance standardization step, causing a\n      covariance matrix to be calculated instead.\n    * By default, the diagonal elements in the relationship matrix are based on\n      --ibc\\'s Fhat1; use the \\'ibc2\\'or \\'ibc3\\'modifiers to base them on Fhat2\n      or Fhat3 instead.\n    * The computation can be subdivided with --parallel.\n  --make-grm-gz [\\'no-gz\\'] [{cov | ibc2 | ibc3}]\n  --make-grm-bin [{cov | ibc2 | ibc3}]\n    --make-grm-gz writes the relationships in GCTA\\'s original gzipped list\n    format, which describes one pair per line, while --make-grm-bin writes them\n    in GCTA 1.1+\\'s single-precision triangular binary format.  Note that these\n    formats explicitly report the number of valid observations (where neither\n    sample has a missing call) for each pair, which is useful input for some\n    scripts.\n    These computations can be subdivided with --parallel.\n\n  --rel-cutoff [val]\n    (alias: --grm-cutoff)\n    Exclude one member of each pair of samples with relatedness greater than\n    the given cutoff value (default 0.025).  If no later operation will cause\n    the list of remaining samples to be written to disk, this will save it to\n    <output prefix>.rel.id.\n    Note that maximizing the remaining sample size is equivalent to the NP-hard\n    maximum independent set problem, so we use a greedy algorithm instead of\n    guaranteeing optimality.  (Use the --make-rel and --keep/--remove flags if\n    you want to try to do better.)\n\n  --ibs-test [permutation count]\n  --groupdist [iters] [d]\n    Given case/control phenotype data, these commands consider three subsets of\n    the distance matrix: pairs of affected samples, affected-unaffected pairs,\n    and pairs of unaffected samples.  Each of these subsets has a distribution\n    of pairwise genomic distances; --ibs-test uses permutation to estimate\n    p-values re: which types of pairs are most similar, while --groupdist\n    focuses on the differences between the centers of these distributions and\n    estimates standard errors via delete-d jackknife.\n\n  --regress-distance [iters] [d]\n    Linear regression of pairwise genomic distances on pairwise average\n    phenotypes and vice versa, using delete-d jackknife for standard errors.  A\n    scalar phenotype is required.\n    * With less than two parameters, d is set to <number of people>^0.6 rounded\n      down.  With no parameters, 100k iterations are run.\n  --regress-rel [iters] [d]\n    Linear regression of pairwise genomic relationships on pairwise average\n    phenotypes, and vice versa.  Defaults for iters and d are the same as for\n    --regress-distance.\n\n  --genome [\\'gz\\'] [\\'rel-check\\'] [\\'full\\'] [\\'unbounded\\'] [\\'nudge\\']\n    Generate an identity-by-descent report.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * The \\'rel-check\\'modifier excludes pairs of samples with different FIDs\n      from the final report.\n    * \\'full\\'adds raw pairwise comparison data to the report.\n    * The P(IBD=0/1/2) estimator employed by this command sometimes yields\n      numbers outside the range [0,1]; by default, these are clipped.  The\n      \\'unbounded\\'modifier turns off this clipping.\n    * Then, when PI_HAT^2 < P(IBD=2), \\'nudge\\'adjusts the final P(IBD=0/1/2)\n      estimates to a theoretically possible configuration.\n    * The computation can be subdivided with --parallel.\n\n  --homozyg [{group | group-verbose}] [\\'consensus-match\\'] [\\'extend\\']\n            [\\'subtract-1-from-lengths\\']\n  --homozyg-snp <min var count>\n  --homozyg-kb <min length>\n  --homozyg-density <max inverse density (kb/var)>\n  --homozyg-gap <max internal gap kb length>\n  --homozyg-het <max hets>\n  --homozyg-window-snp <scanning window size>\n  --homozyg-window-het <max hets in scanning window hit>\n  --homozyg-window-missing <max missing calls in scanning window hit>\n  --homozyg-window-threshold <min scanning window hit rate>\n    These commands request a set of run-of-homozygosity reports, and allow you\n    to customize how they are generated.\n    * If you\\'re satisfied with all the default settings described below, just\n      use --homozyg with no modifiers.  Otherwise, --homozyg lets you change a\n      few binary settings:\n      * \\'group[-verbose]\\'adds a report on pools of overlapping runs of\n        homozygosity.  (Automatically set when --homozyg-match is present.)\n      * With \\'group[-verbose]\\', \\'consensus-match\\'causes pairwise segmental\n        matches to be called based on the variants in the pool\\'s consensus\n        segment, rather than the variants in the pairwise intersection.\n      * Due to how the scanning window algorithm works, it is possible for a\n        reported ROH to be adjacent to a few homozygous variants.  The \\'extend\\'\n        modifier causes them to be included in the reported ROH if that\n        wouldn\\'t cause a violation of the --homozyg-density bound.\n      * By default, segment bp lengths are calculated as <end bp position> -\n        <start bp position> + 1.  Therefore, reports normally differ slightly\n        from PLINK 1.07, which does not add 1 at the end.  For testing\n        purposes, you can use the \\'subtract-1-from-lengths\\'modifier to apply\n        the old formula.\n    * By default, only runs of homozygosity containing at least 100 variants,\n      and of total length >= 1000 kilobases, are noted.  You can change these\n      minimums with --homozyg-snp and --homozyg-kb, respectively.\n    * By default, a ROH must have at least one variant per 50 kb on average;\n      change this bound with --homozyg-density.\n    * By default, if two consecutive variants are more than 1000 kb apart, they\n      cannot be in the same ROH; change this bound with --homozyg-gap.\n    * By default, a ROH can contain an unlimited number of heterozygous calls;\n      you can impose a limit with --homozyg-het.\n    * By default, the scanning window contains 50 variants; change this with\n      --homozyg-window-snp.\n    * By default, a scanning window hit can contain at most 1 heterozygous\n      call and 5 missing calls; change these limits with --homozyg-window-het\n      and --homozyg-window-missing, respectively.\n    * By default, for a variant to be eligible for inclusion in a ROH, the hit\n      rate of all scanning windows containing the variant must be at least\n      0.05; change this threshold with --homozyg-window-threshold.\n\n  --cluster [\\'cc\\'] [{group-avg | old-tiebreaks}] [\\'missing\\'] [\\'only2\\']\n    Cluster samples using a pairwise similarity statistic (normally IBS).\n    * The \\'cc\\'modifier forces every cluster to have at least one case and one\n      control.\n    * The \\'group-avg\\'modifier causes clusters to be joined based on average\n      instead of minimum pairwise similarity.\n    * The \\'missing\\'modifier causes clustering to be based on\n      identity-by-missingness instead of identity-by-state, and writes a\n      space-delimited identity-by-missingness matrix to disk.\n    * The \\'only2\\'modifier causes only a .cluster2 file (which is valid input\n      for --within) to be written; otherwise 2 other files will be produced.\n    * By default, IBS ties are not broken in the same manner as PLINK 1.07, so\n      final cluster solutions tend to differ.  This is generally harmless.\n      However, to simplify testing, you can use the \\'old-tiebreaks\\'modifier to\n      force emulation of the old algorithm.\n\n  --pca [count] [\\'header\\'] [\\'tabs\\'] [\\'var-wts\\']\n    Calculates a variance-standardized relationship matrix (use\n    --make-rel/--make-grm-gz/--make-grm-bin to dump it), and extracts the top\n    20 principal components.\n    * It is usually best to perform this calculation on a marker set in\n      approximate linkage equilibrium.\n    * You can change the number of PCs by passing a numeric parameter.\n    * The \\'header\\'modifier adds a header line to the .eigenvec output file.\n      (For compatibility with the GCTA flag of the same name, the default is no\n      header line.)\n    * The \\'tabs\\'modifier causes the .eigenvec file(s) to be tab-delimited.\n    * The \\'var-wts\\'modifier requests an additional .eigenvec.var file with PCs\n      expressed as variant weights instead of sample weights.\n\n  --neighbour <n1> <n2>\n    (alias: --neighbor)\n    Report IBS distances from each sample to their n1th- to n2th-nearest\n    neighbors, associated Z-scores, and the identities of those neighbors.\n    Useful for outlier detection.\n\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [{fisher | fisher-midp}]\n          [\\'counts\\'] [\\'set-test\\']\n  --assoc [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'qt-means\\'] [\\'lin\\']\n          [\\'set-test\\']\n  --model [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n          [{fisher | fisher-midp | trend-only}] [\\'set-test\\']\n          [{dom | rec | gen | trend}]\n    Basic association analysis report.\n    Given a case/control phenotype, --assoc performs a 1df chi-square allelic\n    test, while --model performs 4 other tests as well (1df dominant gene\n    action, 1df recessive gene action, 2df genotypic, Cochran-Armitage trend).\n    * With \\'fisher\\'/\\'fisher-midp\\', Fisher\\'s exact test is used to generate\n      p-values.  \\'fisher-midp\\'also applies Lancaster\\'s mid-p adjustment.\n    * \\'perm\\'causes an adaptive permutation test to be performed.\n    * \\'mperm=\\'<value> causes a max(T) permutation test with the specified\n      number of replications to be performed.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'counts\\'causes --assoc to report allele counts instead of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * \\'dom\\', \\'rec\\', \\'gen\\', and \\'trend\\'force the corresponding test to be used\n      as the basis for --model permutation.  (By default, the most significant\n      result among the allelic, dominant, and recessive tests is used.)\n    * \\'trend-only\\'causes only the trend test to be performed.\n    Given a quantitative phenotype, --assoc normally performs a Wald test.\n    * In this case, the \\'qt-means\\'modifier causes trait means and standard\n      deviations stratified by genotype to be reported as well.\n    * \\'lin\\'causes the Lin statistic to be computed, and makes it the basis for\n      multiple-testing corrections and permutation tests.\n    Several other flags (most notably, --aperm) can be used to customize the\n    permutation test.\n\n  --mh [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n    (alias: --cmh)\n  --bd [\\'perm\\'| \\'perm-bd\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n  --mh2\n  --homog\n    Given a case/control phenotype and a set of clusters, --mh computes 2x2xK\n    Cochran-Mantel-Haenszel statistics for each variant, while --bd also\n    performs the Breslow-Day test for odds ratio homogeneity.  Permutation and\n    variant set testing based on the CMH (default) or Breslow-Day (when\n    \\'perm-bd\\'is present) statistic are supported.\n    The following similar analyses are also available:\n    * --mh2 swaps the roles of case/control status and cluster membership,\n      performing a phenotype-stratified IxJxK Cochran-Mantel-Haenszel test on\n      association between cluster assignments and genotypes.\n    * --homog executes an alternative to the Breslow-Day test, based on\n      partitioning of the chi-square statistic.\n\n  --gxe [covariate index]\n    Given both a quantitative phenotype and a case/control covariate loaded\n    with --covar defining two groups, --gxe compares the regression coefficient\n    derived from considering only members of one group to the regression\n    coefficient derived from considering only members of the other.  By\n    default, the first covariate in the --covar file defines the groups; use\n    e.g. \"--gxe 3\" to base them on the third covariate instead.\n\n  --linear [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n           [{genotypic | hethom | dominant | recessive | no-snp}]\n           [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n           [\\'standard-beta\\'] [\\'intercept\\']\n  --logistic [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'set-test\\']\n             [{genotypic | hethom | dominant | recessive | no-snp}]\n             [\\'hide-covar\\'] [{sex | no-x-sex}] [\\'interaction\\'] [\\'beta\\']\n             [\\'intercept\\']\n    Multi-covariate association analysis on a quantitative (--linear) or\n    case/control (--logistic) phenotype.  Normally used with --covar.\n    * \\'perm\\'normally causes an adaptive permutation test to be performed on\n      the main effect, while \\'mperm=\\'<value> starts a max(T) permutation test.\n    * \\'perm-count\\'causes the permutation test report to include counts instead\n      of frequencies.\n    * \\'set-test\\'tests the significance of variant sets.  Requires permutation;\n      can be customized with --set-p/--set-r2/--set-max.\n    * The \\'genotypic\\'modifier adds an additive effect/dominance deviation 2df\n      joint test (0/1/2 and 0/1/0 coding), while \\'hethom\\'uses 0/0/1 and 0/1/0\n      coding instead.  If permutation is also requested, these modifiers cause\n      permutation to be based on the joint test.\n    * \\'dominant\\'and \\'recessive\\'specify a model assuming full dominance or\n      recessiveness, respectively, for the A1 allele.\n    * \\'no-snp\\'causes regression to be performed only on the phenotype and the\n      covariates, without reference to genomic data.  If permutation is also\n      requested, results are reported for all covariates.\n    * \\'hide-covar\\'removes covariate-specific lines from the report.\n    * By default, sex (male = 1, female = 0) is automatically added as a\n      covariate on X chromosome variants, and nowhere else.  The \\'sex\\'modifier\n      causes it to be added everywhere, while \\'no-x-sex\\'excludes it.\n    * \\'interaction\\'adds genotype x covariate interactions to the model.  This\n      cannot be used with the usual permutation tests; use --tests to define\n      the permutation test statistic instead.\n    * \\'intercept\\'causes intercepts to be included in the main report.\n    * For logistic regressions, the \\'beta\\'modifier causes regression\n      coefficients instead of odds ratios to be reported.\n    * With --linear, the \\'standard-beta\\'modifier standardizes the phenotype\n      and all predictors to zero mean and unit variance before regression.\n\n  --dosage <allele dosage file> [\\'noheader\\'] [\\'skip0=\\'<i>] [\\'skip1=\\'<j>]\n           [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --dosage <list file> list [{sepheader | noheader}] [\\'skip0=\\'<i>]\n           [\\'skip1=\\'<j>] [\\'skip2=\\'<k>] [\\'dose1\\'] [\\'format=\\'<m>] [\\'Zout\\']\n           [{occur | standard-beta}] [\\'sex\\'] [\\'case-control-freqs\\']\n  --write-dosage\n    Process (possibly gzipped) text files with variant-major allelic dosage\n    data.  This cannot be used with a regular input fileset; instead, you must\n    *only* specify a .fam and possibly a .map file, and you can\\'t specify any\n    other commands.\n    * PLINK 2.0 will have first-class support for genotype probabilities.  An\n      equivalent data import flag will be provided then, and --dosage will be\n      retired.\n    * By default, --dosage assumes that only one allelic dosage file should be\n      loaded.  To specify multiple files,\n      1. create a master list with one entry per line.  There are normally two\n         supported formats for this list: just a filename per line, or variant\n         batch numbers in the first column and filenames in the second.\n      2. Provide the name of that list as the first --dosage parameter.\n      3. Add the \\'list\\'modifier.\n    * By default, --dosage assumes the allelic dosage file(s) contain a header\n      line, which has \\'SNP\\'in column i+1, \\'A1\\'in column i+j+2, \\'A2\\'in column\n      i+j+3, and sample FID/IIDs starting from column i+j+k+4.  (i/j/k are\n      normally zero, but can be changed with \\'skip0\\', \\'skip1\\', and \\'skip2\\'\n      respectively.)  If such a header line is not present,\n      * when all samples appear in the same order as they do in the .fam file,\n        you can use the \\'noheader\\'modifier.\n      * Otherwise, use the \\'sepheader\\'modifier, and append sample ID filenames\n        to your \\'list\\'file entries.\n    * The \\'format=\\'modifier lets you specify the number of values used to\n      represent each dosage.  \\'format=1\\'normally indicates a single 0..2 A1\n      expected count; \\'dose1\\'modifies this to a 0..1 frequency.  \\'format=2\\'\n      (the default) indicates a 0..1 homozygous A1 likelihood followed by a\n      0..1 het likelihood, while \\'format=3\\'indicates 0..1 hom A1, 0..1 het,\n      0..1 hom A2.\n    * \\'Zout\\'causes the output file to be gzipped.\n    * Normally, an association analysis is performed.  \\'standard-beta\\'and\n      \\'sex\\'behave as they are supposed to with --linear/--logistic.\n      \\'case-control-freqs\\'causes case and control allele frequencies to be\n      reported separately.\n    * There are three alternate modes which cause the association analysis to\n      be skipped.\n      * \\'occur\\'requests a simple variant occurrence report.\n      * --write-dosage causes a simple merged file matching the \\'format\\'\n        specification (not including \\'dose1\\') to be generated.\n      * --score applies a linear scoring system to the dosages.\n\n  --lasso <h2 estimate> [min lambda] [\\'report-zeroes\\']\n    Estimate variant effect sizes via LASSO regression.  You must provide an\n    additive heritability estimate to calibrate the regression.\n    Note that this method may require a very large sample size (e.g. hundreds\n    of thousands) to be effective on complex polygenic traits.\n\n  --test-missing [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'midp\\']\n    Check for association between missingness and case/control status, using\n    Fisher\\'s exact test.  (Het. haploids are treated as missing.)\n    The \\'midp\\'modifier causes Lancaster\\'s mid-p adjustment to be applied.\n\n  --make-perm-pheno <ct>\n    Generate phenotype permutations and write them to disk, without invoking an\n    association test.\n\n  --tdt [{exact | exact-midp | poo}] [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\']\n        [{parentdt1 | parentdt2 | pat | mat}] [\\'set-test\\']\n    Report transmission disequilibrium test statistics, given case/control\n    phenotypes and pedigree information.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * By default, the basic TDT p-value is based on a chi-square test unless\n      you request the exact binomial test with \\'exact\\'or \\'exact-midp\\'.\n    * \\'perm\\'/\\'mperm=\\'requests a family-based adaptive or max(T) permutation\n      test.  By default, the permutation test statistic is the basic TDT\n      p-value; \\'parentdt1\\'/\\'parentdt2\\'cause parenTDT or combined test\n      p-values, respectively, to be considered instead.\n    * \\'set-test\\'tests the significance of variant sets.  This cannot be used\n      with exact tests for now.\n    The \\'poo\\'modifier causes a parent-of-origin analysis to be performed\n    instead, with transmissions from heterozygous fathers and heterozygous\n    mothers considered separately.\n    * The parent-of-origin analysis does not currently support exact tests.\n    * By default, the permutation test statistic is the absolute\n      parent-of-origin test Z score; \\'pat\\'/\\'mat\\'cause paternal or maternal TDT\n      chi-square statistics, respectively, to be considered instead.\n\n  --qfam [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-parents [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-between [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n  --qfam-total [\\'perm\\'| \\'mperm=\\'<value>] [\\'perm-count\\'] [\\'emp-se\\']\n    QFAM family-based association test for quantitative traits.\n    * A Mendel error check is performed before the main tests; offending\n      genotypes are treated as missing by this analysis.\n    * This procedure requires permutation.  \\'perm\\'and \\'perm-count\\'have the\n      usual meanings.  However, \\'mperm=\\'<value> just specifies a fixed number\n      of permutations; the method does not support a proper max(T) test.\n    * The \\'emp-se\\'modifier adds BETA and EMP_SE (empirical standard error for\n      beta) fields to the .perm output file.\n\n  --annotate <PLINK report> [\\'attrib=\\'<file>] [\\'ranges=\\'<file>]\n             [\\'filter=\\'<file>] [\\'snps=\\'<file>] [{NA | prune}] [\\'block\\']\n             [\\'subset=\\'<file>] [\\'minimal\\'] [\\'distance\\']\n    Add annotations to a variant-based PLINK report.  This requires an\n    annotation source:\n    * \\'attrib=\\'<file> specifies a (possibly gzipped) attribute file.\n    * \\'ranges=\\'<file> specifies a gene/range list file.\n    (Both source types can be specified simultaneously.)  The following options\n    are also supported:\n    * \\'filter=\\'<file> causes only variants within one of the ranges in the file\n      to be included in the new report.\n    * \\'snps=\\'<file> causes only variants named in the file to be included in\n      the new report.\n    * The \\'NA\\'modifier causes unannotated variants to have \\'NA\\'instead of \\'.\\'\n      in the new report\\'s ANNOT column, while the \\'prune\\'modifier excludes\n      them entirely.\n    * The \\'block\\'modifier replaces the single ANNOT column with a 0/1-coded\n      column for each possible annotation.\n    * With \\'ranges\\',\n      * \\'subset=\\'<file> causes only intervals named in the subset file to be\n        loaded from the ranges file.\n      * interval annotations normally come with a parenthesized signed distance\n        to the interval boundary (0 if the variant is located inside the\n        interval; this is always true without --border).  They can be excluded\n        with the \\'minimal\\'modifier.\n      * the \\'distance\\'modifier adds \\'DIST\\'and \\'SGN\\'columns describing signed\n        distance to the nearest interval.\n    * When --pfilter is present, high p-values are filtered out.\n\n  --clump <PLINK report filename(s)...>\n    Process association analysis report(s) with \\'SNP\\'and p-value columns,\n    organizing results by LD-based clumps.  Multiple filenames can be separated\n    by spaces or commas.\n\n  --gene-report <PLINK report> <gene range file>\n    Generate a gene-based report from a variant-based report.\n    * When --pfilter is present, high p-values are filtered out.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n\n  --meta-analysis <PLINK report filenames...>\n  --meta-analysis <PLINK report filenames...> + [{logscale | qt}]\n                  [{no-map | no-allele}] [\\'study\\'] [\\'report-all\\']\n                  [\\'weighted-z\\']\n    Perform a meta-analysis on several variant-based reports with \\'SNP\\'and\n    \\'SE\\'fields.\n    * Normally, an \\'OR\\'odds ratio field must also be present in each input\n      file.  With \\'logscale\\', \\'BETA\\'log-odds values/regression coefficients\n      are expected instead, but the generated report will still contain odds\n      ratio estimates.  With \\'qt\\', both input and output values are regression\n      betas.\n    * \\'CHR\\', \\'BP\\', and \\'A1\\'fields are also normally required.  \\'no-map\\'causes\n      them to all be ignored, while \\'no-allele\\'causes just \\'A1\\'to be ignored.\n    * If \\'A2\\'fields are present, and neither \\'no-map\\'nor \\'no-allele\\'was\n      specified, A1/A2 allele flips are handled properly.  Otherwise, A1\n      mismatches are thrown out.\n    * \\'study\\'causes study-specific effect estimates to be collated in the\n      meta-analysis report.\n    * \\'report-all\\'causes variants present in only a single input file to be\n      included in the meta-analysis report.\n    * \\'weighted-z\\'requests weighted Z-score-based p-values (as computed by the\n      Abecasis Lab\\'s METAL software) in addition to the usual inverse\n      variance-based analysis.  This requires P and effective sample size\n      fields.\n    * When --extract (without \\'range\\') is present, only variants named in the\n      --extract file are considered.\n    * Unless \\'no-map\\'is specified, chromosome filters are also respected.\n\n  --fast-epistasis [{boost | joint-effects | no-ueki}] [\\'case-only\\']\n                   [{set-by-set | set-by-all}] [\\'nop\\']\n  --epistasis [{set-by-set | set-by-all}]\n    Scan for epistatic interactions.  --fast-epistasis inspects 3x3 joint\n    genotype count tables and only applies to case/control phenotypes, while\n    --epistasis performs linear or logistic regression.\n    * By default, --fast-epistasis uses the PLINK 1.07 allele-based test.  Two\n      newer tests are now supported: \\'boost\\'invokes the likelihood ratio test\n      introduced by Wan X et al. (2010) BOOST: A Fast Approach to Detecting\n      Gene-Gene Interactions in Genome-wide Case-Control Studies, while\n      \\'joint-effects\\'applies the joint effects test introduced in Ueki M,\n      Cordell HJ (2012) Improved statistics for genome-wide interaction\n      analysis.\n    * The original --fast-epistasis test normally applies the variance and\n      empty cell corrections suggested by Ueki and Cordell\\'s paper.  To disable\n      them, use the \\'no-ueki\\'modifier.\n    * \\'case-only\\'requests a case-only instead of a case/control test.\n    * By default, all pairs of variants across the entire genome are tested.\n      To just test pairs of variants within a single set, add the \\'set-by-set\\'\n      modifier and load exactly one set with --set/--make-set; with exactly two\n      sets loaded, all variants in one set are tested against all variants in\n      the other.  \\'set-by-all\\'tests all variants in one set against the entire\n      genome instead.\n    * \\'nop\\'strips p-values from the main report.\n    * These computations can be subdivided with --parallel; however...\n  --epistasis-summary-merge <common file prefix> <ct>\n    When a --[fast-]epistasis job is subdivided with --parallel, the main\n    report can be assembled at the end by applying Unix \\'cat\\'in the usual\n    manner, but the .summary.1, .summary.2, ... files may require a specialized\n    merge.  --epistasis-summary-merge takes care of the latter.\n\n  --twolocus <variant ID> <variant ID>\n    Two-locus joint genotype count report.\n\n  --score <filename> [i] [j] [k] [\\'header\\'] [{sum | no-sum}]\n          [{no-mean-imputation | center}] [\\'include-cnt\\'] [\\'double-dosage\\']\n    Apply a linear scoring system to each sample.\n    The input file should have one line per scored variant.  Variant IDs are\n    read from column #i, allele codes are read from column #j, and scores are\n    read from column #k, where i defaults to 1, j defaults to i+1, and k\n    defaults to j+1.\n    * The \\'header\\'modifier causes the first nonempty line of the input file to\n      be ignored; otherwise, --score assumes there is no header line.\n    * By default, final scores are averages of the valid per-variant scores.\n      The \\'sum\\'modifier causes sums to be reported instead.  (This cannot be\n      used with \\'no-mean-imputation\\'.  And for backward compatibility, \\'sum\\'is\n      automatically on with dosage data unless \\'no-sum\\'is specified.)\n    * By default, copies of the unnamed allele contribute zero to score, while\n      missing genotypes contribute an amount proportional to the loaded (via\n      --read-freq) or imputed allele frequency.  To throw out missing\n      observations instead (decreasing the denominator in the final average\n      when this happens), use the \\'no-mean-imputation\\'modifier.\n    * Alternatively, you can use the \\'center\\'modifier to shift all scores to\n      mean zero.\n    * This command can be used with dosage data.  By default, the \\'CNT\\'column\n      is omitted from the output file in this case; use \\'include-cnt\\'to keep\n      it.  Also, note that scores are multiplied by 0..1 dosages, not 0..2\n      diploid allele counts, unless the \\'double-dosage\\'modifier is present.\n\n  --R <R script file> [\\'debug\\']\n    Connect to a Rserve (preferably version 1.7 or later) background process,\n    and execute the Rplink function defined in the input file.  (Unless the\n    \\'debug\\'modifier is present; in that case, the R commands that PLINK would\n    have tried to execute are logged to a file.)\n\n  --write-var-ranges <block ct>\n    Divide the set of variants into equal-size blocks.  (Can be used with\n    --snps to split a job across multiple machines.)\n\nThe following other flags are supported.  (Order of operations is described at\nhttps://www.cog-genomics.org/plink/1.9/order .)\n  --script <fname> : Include command-line options from file.\n  --rerun [log]    : Rerun commands in log (default \\'plink.log\\').\n  --version        : Display only version number before exiting.\n  --silent         : Suppress output to console.\n  --gplink         : Reserved for interoperation with gPLINK.\n  --missing-genotype <char> : Set missing genotype code (normally \\'0\\').\n  --double-id          : Set both FIDs and IIDs to the VCF/BCF sample ID.\n  --const-fid [ID]     : Set all FIDs to the given constant (default \\'0\\').\n  --id-delim [d]       : Parse sample IDs as <FID><d><IID> (default delim \\'_\\').\n  --vcf-idspace-to <c> : Convert spaces in sample IDs to the given character.\n  --biallelic-only [\\'strict\\'] [\\'list\\'] : Skip VCF variants with 2+ ALT alleles.\n  --vcf-min-qual <val>           : Skip VCF variants with low/missing QUAL.\n  --vcf-filter [exception(s)...] : Skip variants which have FILTER failures.\n  --vcf-require-gt               : Skip variants with no GT field.\n  --vcf-min-gq <val>             : No-call a genotype when GQ is below the\n                                   given threshold.\n  --vcf-min-gp <val>             : No-call a genotype when 0-1 scaled GP is\n                                   below the given threshold.\n  --vcf-half-call <m>  : Specify how \\'0/.\\'and similar VCF GT values should be\n                         handled.  The following four modes are supported:\n                         * \\'error\\'/\\'e\\'(default) errors out and reports line #.\n                         * \\'haploid\\'/\\'h\\'treats them as haploid calls.\n                         * \\'missing\\'/\\'m\\'treats them as missing.\n                         * \\'reference\\'/\\'r\\'treats the missing value as 0.\n  --oxford-single-chr <chr nm> : Specify single-chromosome .gen file with\n                                 ignorable first column.\n  --oxford-pheno-name <col nm> : Import named phenotype from the .sample file.\n  --hard-call-threshold <val>  : When an Oxford-format fileset is loaded, calls\n  --hard-call-threshold random   with uncertainty level greater than 0.1 are\n                                 normally treated as missing.  You can adjust\n                                 this threshold by providing a numeric\n                                 parameter, or randomize all calls with\n                                 \\'random\\'.\n  --missing-code [string list] : Comma-delimited list of missing phenotype\n    (alias: --missing_code)      values for Oxford-format filesets (def. \\'NA\\').\n  --simulate-ncases <num>   : Set --simulate case count (default 1000).\n  --simulate-ncontrols <n>  : Set --simulate control count (default 1000).\n  --simulate-prevalence <p> : Set --simulate disease prevalence (default 0.01).\n  --simulate-n <num>        : Set --simulate-qt sample count (default 1000).\n  --simulate-label <prefix> : Set --simulate[-qt] FID/IID name prefix.\n  --simulate-missing <freq> : Set --simulate[-qt] missing genotype frequency.\n  --allow-extra-chr [\\'0\\']   : Permit unrecognized chromosome codes.  The \\'0\\'\n    (alias: --aec)            modifier causes them to be treated as if they had\n                              been set to zero.\n  --chr-set <autosome ct> [\\'no-x\\'] [\\'no-y\\'] [\\'no-xy\\'] [\\'no-mt\\'] :\n    Specify a nonhuman chromosome set.  The first parameter sets the number of\n    diploid autosome pairs if positive, or haploid chromosomes if negative.\n    Given diploid autosomes, the remaining modifiers indicate the absence of\n    the named non-autosomal chromosomes.\n  --cow/--dog/--horse/--mouse/--rice/--sheep : Shortcuts for those species.\n  --autosome-num <value>    : Alias for \"--chr-set <value> no-y no-xy no-mt\".\n  --cm-map <fname pattern> [chr] : Use SHAPEIT-format recombination maps to set\n                                   centimorgan positions.  To process more than\n                                   one chromosome, include a \\'@\\'in the first\n                                   parameter where the chrom. number belongs,\n                                   e.g. \\'genetic_map_chr@_combined_b37.txt\\'.\n  --zero-cms         : Zero out centimorgan positions.\n  --allow-no-samples : Allow the input fileset to contain no samples.\n  --allow-no-vars    : Allow the input fileset to contain no variants.\n  --pheno <fname>  : Load phenotype data from the specified file, instead of\n                     using the values in the main input fileset.\n  --all-pheno      : For basic association tests, loop through all phenotypes\n                     in --pheno file.\n  --mpheno <n>     : Load phenotype from column (n+2) in --pheno file.\n  --pheno-name <c> : If --pheno file has a header row, use column with the\n                     given name.\n  --pheno-merge    : When the main input fileset contains an phenotype value\n                     for a sample, but the --pheno file does not, use the\n                     original value instead of treating the phenotype as\n                     missing.\n  --missing-phenotype <v> : Set missing phenotype value (normally -9).\n  --1                     : Expect case/control phenotypes to be coded as\n                            0 = control, 1 = case, instead of the usual\n                            0 = missing, 1 = control, 2 = case.  This also\n                            forces phenotypes to be interpreted as case/ctrl.\n  --make-pheno <fn> <val> : Define a new case/control phenotype.  If the val\n                            parameter is \\'*\\', all samples listed in the given\n                            file are cases, and everyone else is a control.\n                            (Note that, in some shells, it is necessary to\n                            surround the * with quotes.)\n                            Otherwise, all samples with third column entry\n                            equal to the val parameter are cases, and all other\n                            samples mentioned in the file are controls.\n  --tail-pheno <Lt> [Hbt] : Downcode a scalar phenotype to a case/control\n                            phenotype.  All samples with phenotype values\n                            greater than Hbt are cases, and all with values\n                            less than or equal to Lt are controls.  If Hbt is\n                            unspecified, it is equal to Lt; otherwise,\n                            in-between phenotype values are set to missing.\n  --covar <filename> [\\'keep-pheno-on-missing-cov\\'] : Specify covariate file.\n  --covar-name <...>       : Specify covariate(s) in --covar file by name.\n                             Separate multiple names with spaces or commas, and\n                             use dashes to designate ranges.\n  --covar-number <...>     : Specify covariate(s) in --covar file by index.\n  --no-const-covar         : Exclude constant covariates.\n  --allow-no-covars        : Allow no covariates to be loaded from --covar\n                             file.\n  --within <f> [\\'keep-NA\\'] : Specify initial cluster assignments.\n  --mwithin <n>            : Load cluster assignments from column n+2.\n  --family                 : Create a cluster for each family ID.\n  --loop-assoc <f> [\\'keep-NA\\']  : Run specified case/control association\n                                  commands once for each cluster in the file,\n                                  using cluster membership as the phenotype.\n  --set <filename>              : Load sets from a .set file.\n  --set-names <name(s)...>      : Load only sets named on the command line.\n                                  Use spaces to separate multiple names.\n  --subset <filename>           : Load only sets named in the given text file.\n  --set-collapse-all <set name> : Merge all sets.\n  --complement-sets             : Invert all sets.  (Names gain \\'C_\\'prefixes.)\n  --make-set-complement-all <s> : --set-collapse-all + inversion.\n  --make-set <filename>         : Define sets from a list of named bp ranges.\n  --make-set-border <kbs>       : Stretch regions in --make-set file.\n  --make-set-collapse-group     : Define sets from groups instead of sets in\n                                  --make-set file.\n  --keep <filename>       : Exclude all samples not named in the file.\n  --remove <filename>     : Exclude all samples named in the file.\n  --keep-fam <filename>   : Exclude all families not named in the file.\n  --remove-fam <filename> : Exclude all families named in the file.\n  --extract [\\'range\\'] <f> : Exclude all variants not named in the file.\n  --exclude [\\'range\\'] <f> : Exclude all variants named in the file.\n  --keep-clusters <filename>          : These can be used individually or in\n  --keep-cluster-names <name(s)...>     combination to define a list of\n                                        clusters to keep; all samples not in a\n                                        cluster in that list are then excluded.\n                                        Use spaces to separate cluster names\n                                        for --keep-cluster-names.\n  --remove-clusters <filename>        : Exclude all clusters named in the file.\n  --remove-cluster-names <name(s)...> : Exclude the named clusters.\n  --gene <sets...> : Exclude variants not in a set named on the command line.\n                     (Separate multiple set names with spaces.)\n  --gene-all       : Exclude variants which aren\\'t a member of any set.  (PLINK\n                     1.07 automatically did this under some circumstances.)\n  --attrib <f> [att lst] : Given a file assigning attributes to variants, and a\n  --attrib-indiv <f> [a]   comma-delimited list (with no whitespace) of\n                           attribute names, remove variants/samples which are\n                           either missing from the file or don\\'t have any of\n                           the listed attributes.  If some attribute names in\n                           the list are preceded by \\'-\\', they are treated as\n                           \"negative match conditions\" instead: variants with\n                           at least one negative match attribute are removed.\n                           The first character in the list cannot be a \\'-\\', due\n                           to how command-line parsing works; add a comma in\n                           front to get around this.\n  --chr <chrs...>  : Exclude all variants not on the given chromosome(s).\n                     Valid choices for humans are 0 (unplaced), 1-22, X, Y, XY,\n                     and MT.  Separate multiple chromosomes with spaces and/or\n                     commas, and use a dash (no adjacent spaces permitted) to\n                     denote a range, e.g. \"--chr 1-4, 22, xy\".\n  --not-chr <...>  : Reverse of --chr (exclude variants on listed chromosomes).\n  --autosome       : Exclude all non-autosomal variants.\n  --autosome-xy    : Exclude all non-autosomal variants, except those with\n                     chromosome code XY (pseudo-autosomal region of X).\n  --snps-only [\\'just-acgt\\'] : Exclude non-SNP variants.  By default, SNP = both\n                              allele codes are single-character; \\'just-acgt\\'\n                              restricts codes to {A,C,G,T,a,c,g,t,<missing>}.\n  --from <var ID>  : Use ID(s) to specify a variant range to load.  When used\n  --to   <var ID>    together, both variants must be on the same chromosome.\n  --snp  <var ID>  : Specify a single variant to load.\n  --exclude-snp <> : Specify a single variant to exclude.\n  --window  <kbs>  : With --snp or --exclude-snp, loads/excludes all variants\n                     within half the specified kb distance of the named one.\n  --from-bp <pos>  : Use physical position(s) to define a variant range to\n  --to-bp   <pos>    load.  --from-kb/--to-kb/--from-mb/--to-mb allow decimal\n  --from-kb <pos>    values.  You must also specify a single chromosome (using\n  --to-kb   <pos>    e.g. --chr) when using these flags.\n  --from-mb <pos>\n  --to-mb   <pos>\n  --snps <var IDs...>  : Use IDs to specify variant range(s) to load or\n  --exclude-snps <...>   exclude.  E.g. \"--snps rs1111-rs2222, rs3333, rs4444\".\n  --thin <p>       : Randomly remove variants, retaining each with prob. p.\n  --thin-count <n> : Randomly remove variants until n of them remain.\n  --bp-space <bps> : Remove variants so that each pair is no closer than the\n                     given bp distance.  (Equivalent to VCFtools --thin.)\n  --thin-indiv <p>         : Randomly remove samples, retaining with prob. p.\n  --thin-indiv-count <n>   : Randomly remove samples until n of them remain.\n  --filter <f> <val(s)...> : Exclude all samples without a 3rd column entry in\n                             the given file matching one of the given\n                             space-separated value(s).\n  --mfilter <n>            : Match against (n+2)th column instead.\n  --geno [val]     : Exclude variants with missing call frequencies greater\n                     than a threshold (default 0.1).  (Note that the default\n                     threshold is only applied if --geno is invoked without a\n                     parameter; when --geno is not invoked, no per-variant\n                     missing call frequency ceiling is enforced at all.  Other\n                     inclusion/exclusion default thresholds work the same way.)\n  --mind [val]     : Exclude samples with missing call frequencies greater than\n                     a threshold (default 0.1).\n  --oblig-missing <f1> <f2> : Specify blocks of missing genotype calls for\n                              --geno/--mind to ignore.  The first file should\n                              have variant IDs in the first column and block\n                              IDs in the second, while the second file should\n                              have FIDs in the first column, IIDs in the\n                              second, and block IDs in the third.\n  --prune             : Remove samples with missing phenotypes.\n  --maf [freq]        : Exclude variants with minor allele frequency lower than\n                        a threshold (default 0.01).\n  --max-maf <freq>    : Exclude variants with MAF greater than the threshold.\n  --mac <ct>          : Exclude variants with minor allele count lower than the\n    (alias: --min-ac)   given threshold.\n  --max-mac <ct>      : Exclude variants with minor allele count greater than\n    (alias: --max-ac)   the given threshold.\n  --maf-succ       : Rule of succession MAF estimation (used in EIGENSOFT).\n                     Given j observations of one allele and k >= j observations\n                     of the other, infer a MAF of (j+1) / (j+k+2), rather than\n                     the default j / (j+k).\n  --read-freq <fn> : Estimate MAFs and heterozygote frequencies from the given\n                     --freq[x] report, instead of the input fileset.\n  --hwe <p> [\\'midp\\'] [\\'include-nonctrl\\'] : Exclude variants with Hardy-Weinberg\n                                           equilibrium exact test p-values\n                                           below a threshold.\n  --me <t> <v> [\\'var-first\\'] : Filter out trios and variants with Mendel error\n                               rates exceeding the given thresholds.\n  --me-exclude-one [ratio]   : Make --me exclude only one sample per trio.\n  --qual-scores <f> [qcol] [IDcol] [skip] : Filter out variants with\n                                            out-of-range quality scores.\n                                            Default range is now [0, \\infty ).\n  --qual-threshold <min qual score>       : Set --qual-scores range floor.\n  --qual-max-threshold <max qual score>   : Set --qual-scores range ceiling.\n  --allow-no-sex   : Do not treat ambiguous-sex samples as having missing\n                     phenotypes in analysis commands.  (Automatic /w --no-sex.)\n  --must-have-sex  : Force ambiguous-sex phenotypes to missing on\n                     --make-bed/--make-just-fam/--recode/--write-covar.\n  --filter-cases       : Include only cases in the current analysis.\n  --filter-controls    : Include only controls.\n  --filter-males       : Include only males.\n  --filter-females     : Include only females.\n  --filter-founders    : Include only founders.\n  --filter-nonfounders : Include only nonfounders.\n  --nonfounders        : Include nonfounders in allele freq/HWE calculations.\n  --make-founders [\\'require-2-missing\\'] [\\'first\\'] :\n    Clear parental IDs for those with 1+ missing parent(s).\n  --recode-allele <fn> : With --recode A/A-transpose/AD, count alleles named in\n                         the file (otherwise A1 alleles are always counted).\n  --output-chr <MT code> : Set chromosome coding scheme in output files by\n                           providing the desired human mitochondrial code.\n                           (Options are \\'26\\', \\'M\\', \\'MT\\', \\'0M\\', \\'chr26\\', \\'chrM\\',\n                           and \\'chrMT\\'.)\n  --output-missing-genotype <ch> : Set the code used to represent missing\n                                   genotypes in output files (normally the\n                                   --missing-genotype value).\n  --output-missing-phenotype <s> : Set the string used to represent missing\n                                   phenotypes in output files (normally the\n                                   --missing-phenotype value).\n  --zero-cluster <f> : In combination with --within/--family, set blocks of\n                       genotype calls to missing.  The input file should have\n                       variant IDs in the first column and cluster IDs in the\n                       second.  This must now be used with --make-bed and no\n                       other output commands.\n  --set-hh-missing       : Cause --make-bed and --recode to set heterozygous\n                           haploid genotypes to missing.\n  --set-mixed-mt-missing : Cause --make-bed and --recode to set mixed MT\n                           genotypes to missing.\n  --split-x <bp1> <bp2> [\\'no-fail\\']\n  --split-x <build> [\\'no-fail\\'] :\n    Changes chromosome code of all chrX variants with bp position <= bp1 or >=\n    bp2 to XY.  The following build codes are supported as shorthand:\n    * \\'b36\\'/\\'hg18\\'= NCBI 36, 2709521/154584237\n    * \\'b37\\'/\\'hg19\\'= GRCh37, 2699520/154931044\n    * \\'b38\\'/\\'hg38\\'= GRCh38, 2781479/155701383\n    By default, PLINK errors out when no variants would be affected by\n    --split-x; the \\'no-fail\\'modifier (useful in scripts) overrides this.\n  --merge-x [\\'no-fail\\'] : Merge XY chromosome back with X.\n  --set-me-missing      : Cause --make-bed to set Mendel errors to missing.\n  --fill-missing-a2     : Cause --make-bed to replace all missing calls with\n                          homozygous A2 calls.\n  --set-missing-var-ids <t>   : Given a template string with a \\'@\\'where the\n                                chromosome code should go and \\'#\\'where the bp\n                                coordinate belongs, --set-missing-var-ids\n                                assigns chromosome-and-bp-based IDs to unnamed\n                                variants.\n                                You may also use \\'$1\\'and \\'$2\\'to refer to\n                                allele names in the template string, and in\n                                fact this becomes essential when multiple\n                                variants share the same coordinate.\n  --new-id-max-allele-len <n> : Specify maximum number of leading characters\n                                from allele names to include in new variant IDs\n                                (default 23).\n  --missing-var-code <string> : Change unnamed variant code (default \\'.\\').\n  --update-chr  <f> [chrcol] [IDcol]  [skip] : Update variant chromosome codes.\n  --update-cm   <f> [cmcol]  [IDcol]  [skip] : Update centimorgan positions.\n  --update-map  <f> [bpcol]  [IDcol]  [skip] : Update variant bp positions.\n  --update-name <f> [newcol] [oldcol] [skip] : Update variant IDs.\n  --update-alleles <fname> : Update variant allele codes.\n  --allele1234 [\\'multichar\\'] : Interpret/recode A/C/G/T alleles as 1/2/3/4.\n                               With \\'multichar\\', converts all A/C/G/Ts in\n                               allele names to 1/2/3/4s.\n  --alleleACGT [\\'multichar\\'] : Reverse of --allele1234.\n  --update-ids <f>     : Update sample IDs.\n  --update-parents <f> : Update parental IDs.\n  --update-sex <f> [n] : Update sexes.  Sex (1 or M = male, 2 or F = female, 0\n                         = missing) is loaded from column n+2 (default n is 1).\n  --flip <filename>    : Flip alleles (A<->T, C<->G) for SNP IDs in the file.\n  --flip-subset <fn>   : Only apply --flip to samples in --flip-subset file.\n  --flip-scan-window <ct+1> : Set --flip-scan max variant ct dist. (def. 10).\n  --flip-scan-window-kb <x> : Set --flip-scan max kb distance (default 1000).\n  --flip-scan-threshold <x> : Set --flip-scan min correlation (default 0.5).\n  --keep-allele-order  : Keep the allele order defined in the .bim file,\n  --real-ref-alleles     instead of forcing A2 to be the major allele.\n                         --real-ref-alleles also removes \\'PR\\'from the INFO\n                         values emitted by --recode vcf{,-fid,-iid}.\n  --a1-allele <f> [a1col] [IDcol] [skip] : Force alleles in the file to A1.\n  --a2-allele <filename> [a2col] [IDcol] [skip] :\n    Force alleles in the file to A2.  (\"--a2-allele <VCF filename> 4 3 \\'#\\'\",\n    which scrapes reference allele assignments from a VCF file, is especially\n    useful.)\n  --indiv-sort <m> [f] : Specify FID/IID sort order.  The following four modes\n                         are supported:\n                         * \\'none\\'/\\'0\\'keeps samples in the order they were\n                           loaded.  Default for non-merge operations.\n                         * \\'natural\\'/\\'n\\'invokes \\'natural sort\\', e.g.\n                           \\'id2\\'< \\'ID3\\'< \\'id10\\'.  Default when merging.\n                         * \\'ascii\\'/\\'a\\'sorts in ASCII order, e.g.\n                           \\'ID3\\'< \\'id10\\'< \\'id2\\'.\n                         * \\'file\\'/\\'f\\'uses the order in the given file (named\n                           in the second parameter).\n                         For now, only --merge/--bmerge/--merge-list and\n                         --make-bed/--make-just-fam respect this flag.\n  --with-phenotype [\\'no-parents\\'] [{no-sex | female-2}] :\n    Include more sample info in new .cov file.\n  --dummy-coding [N] [\\'no-round\\'] : Split categorical variables (n categories,\n                                    2 < n <= N, default N is 49) into n-1\n                                    binary dummy variables when writing\n                                    covariate file.\n  --merge-mode <n>   : Adjust --[b]merge/--merge-list behavior based on a\n                       numeric code.\n                       1 (default) = ignore missing calls, otherwise difference\n                                     -> missing\n                       2 = only overwrite originally missing calls\n                       3 = only overwrite when nonmissing in new file\n                       4/5 = never overwrite and always overwrite, respectively\n                       6 = report all mismatching calls without merging\n                       7 = report mismatching nonmissing calls without merging\n  --merge-equal-pos  : With --merge/--bmerge/--merge-list, merge variants with\n                       different names but identical positions.  (Exception:\n                       same-position chromosome code 0 variants aren\\'t merged.)\n  --mendel-duos      : Make Mendel error checks consider samples with only one\n                       parent in the dataset.\n  --mendel-multigen  : Make Mendel error checks consider (great-)grandparental\n                       genotypes when parental genotype data is missing.\n  --ld-window <ct+1> : Set --r/--r2 max variant ct pairwise distance (usu. 10).\n  --ld-window-kb <x> : Set --r/--r2 max kb pairwise distance (usually 1000).\n  --ld-window-cm <x> : Set --r/--r2 max centimorgan pairwise distance.\n  --ld-window-r2 <x> : Set threshold for --r2 report inclusion (usually 0.2).\n  --ld-snp <var ID>  : Set first variant in all --r/--r2 pairs.\n  --ld-snps <vID...> : Restrict first --r/--r2 variant to the given ranges.\n  --ld-snp-list <f>  : Restrict first --r/--r2 var. to those named in the file.\n  --list-all         : Generate the \\'all\\'mode report when using --show-tags in\n                       file mode.\n  --tag-kb <kbs>     : Set --show-tags max tag kb distance (default 250).\n  --tag-r2 <val>     : Set --show-tags min tag r-squared (default 0.8)\n  --tag-mode2        : Use two-column --show-tags (file mode) I/O format.\n  --ld-xchr <code>   : Set chrX model for --indep[-pairwise], --r/--r2,\n                       --flip-scan, and --show-tags.\n                       1 (default) = males coded 0/1, females 0/1/2 (A1 dosage)\n                       2 = males coded 0/2\n                       3 = males coded 0/2, but females given double weighting\n  --blocks-max-kb <kbs>      : Set --blocks maximum haploblock span (def. 200).\n  --blocks-min-maf <cutoff>  : Adjust --blocks MAF minimum (default 0.05).\n  --blocks-strong-lowci <x>  : Set --blocks \"strong LD\" CI thresholds (defaults\n  --blocks-strong-highci <x>   0.70 and 0.98).\n  --blocks-recomb-highci <x> : Set \\'recombination\\'CI threshold (default 0.90).\n  --blocks-inform-frac <x>   : Force haploblock <strong LD pairs>:<total\n                               informative pairs> ratios to be larger than this\n                               value (default 0.95).\n  --distance-wts exp=<x>        : When computing genomic distances, assign each\n                                  variant a weight of (2q(1-q))^{-x}, where q\n                                  is the loaded or inferred MAF.\n  --read-dists <dist file> [id file] : Load a triangular binary distance matrix\n                                       instead of recalculating from scratch.\n  --ppc-gap <val>    : Minimum number of base pairs, in thousands, between\n                       informative pairs of markers used in --genome PPC test.\n                       500 if unspecified.\n  --min <cutoff>     : Specify minimum PI_HAT for inclusion in --genome report.\n  --max <cutoff>     : Specify maximum PI_HAT for inclusion in --genome report.\n  --homozyg-match <> : Set minimum concordance across jointly homozygous\n                       variants for a pairwise allelic match to be declared.\n  --pool-size <ct>   : Set minimum size of pools in \"--homozyg group\" report.\n  --read-genome <fn> : Load --genome report for --cluster/--neighbour, instead\n                       of recalculating IBS and PPC test p-values from scratch.\n  --ppc <p-val>    : Specify minimum PPC test p-value within a cluster.\n  --mc <max size>  : Specify maximum cluster size.\n  --mcc <c1> <c2>  : Specify maximum case and control counts per cluster.\n  --K <min count>  : Specify minimum cluster count.\n  --ibm <val>      : Specify minimum identity-by-missingness.\n  --match <f> [mv] : Use covariate values to restrict clustering.  Without\n                     --match-type, two samples can only be in the same cluster\n                     if all covariates match.  The optional second parameter\n                     specifies a covariate value to treat as missing.\n  --match-type <f> : Refine interpretation of --match file.  The --match-type\n                     file is expected to be a single line with as many entries\n                     as the --match file has covariates; \\'0\\'entries specify\n                     \"negative matches\" (i.e. samples with equal covariate\n                     values cannot be in the same cluster), \\'1\\'entries specify\n                     \"positive matches\" (default), and \\'-1\\'causes the\n                     corresponding covariate to be ignored.\n  --qmatch <f> [m] : Force all members of a cluster to have similar\n  --qt <fname>       quantitative covariate values.  The --qmatch file contains\n                     the covariate values, while the --qt file is a list of\n                     nonnegative tolerances (and \\'-1\\'s marking covariates to\n                     skip).\n  --pca-cluster-names <...> : These can be used individually or in combination\n  --pca-clusters <fname>      to define a list of clusters to use in the basic\n                              --pca computation.  (--pca-cluster-names expects\n                              a space-delimited sequence of cluster names,\n                              while --pca-clusters expects a file with one\n                              cluster name per line.)  All samples outside\n                              those clusters will then be projected on to the\n                              calculated PCs.\n  --mds-plot <dims> [\\'by-cluster\\'] [\\'eigendecomp\\'] [\\'eigvals\\'] :\n    Multidimensional scaling analysis.  Requires --cluster.\n  --cell <thresh>  : Skip some --model tests when a contingency table entry is\n                     smaller than the given threshold.\n  --condition <var ID> [{dominant | recessive}] : Add one variant as a --linear\n                                                  or --logistic covariate.\n  --condition-list <f> [{dominant | recessive}] : Add variants named in the\n                                                  file as --linear/--logistic\n                                                  covariates.\n  --parameters <...>  : Include only the given covariates/interactions in the\n                        --linear/--logistic models, identified by a list of\n                        1-based indices and/or ranges of them.\n  --tests <all> [...] : Perform a (joint) test on the specified term(s) in the\n                        --linear/--logistic model, identified by 1-based\n                        indices and/or ranges of them.  If permutation was\n                        requested, it is based on this test.\n                        * Note that, when --parameters is also present, the\n                          indices refer to the terms remaining AFTER pruning by\n                          --parameters.\n                        * You can use \"--tests all\" to include all terms.\n  --vif <max VIF>     : Set VIF threshold for --linear multicollinearity check\n                        (default 50).\n  --xchr-model <code> : Set the X chromosome --linear/--logistic model.\n                        0 = skip sex and haploid chromosomes\n                        1 (default) = add sex as a covariate on X chromosome\n                        2 = code male genotypes 0/2 instead of 0/1\n                        3 = test for interaction between genotype and sex\n  --lasso-select-covars [cov(s)...] : Subject some or all covariates to LASSO\n                                      model selection.\n  --adjust [\\'gc\\'] [\\'log10\\'] [\\'qq-plot\\'] : Report some multiple-testing\n                                          corrections.\n  --lambda <val>   : Set genomic control lambda for --adjust.\n  --ci <size>      : Report confidence intervals for odds ratios.\n  --pfilter <val>  : Filter out association test results with higher p-values.\n  --aperm <min perms - 1> [max perms] [alpha] [beta] [init interval] [slope] :\n    Set up to six parameters controlling adaptive permutation tests.\n    * The first two control the minimum and maximum number of permutations that\n      may be run for each variant; default values are 5 and 1000000.\n    * The next two control the early termination condition.  A\n      100% * (1 - beta/2T) confidence interval is calculated for each empirical\n      p-value, where T is the total number of variants; whenever this\n      confidence interval doesn\\'t contain alpha, the variant is exempted from\n      further permutation testing.  Default values are 0 and 1e-4.\n    * The last two control when the early termination condition is checked.  If\n      a check occurs at permutation #p, the next check occurs after\n      <slope>p + <init interval> more permutations (rounded down).  Default\n      initial interval is 1, and default slope is 0.001.\n  --mperm-save     : Save best max(T) permutation test statistics.\n  --mperm-save-all : Save all max(T) permutation test statistics.\n  --set-p <p-val>        : Adjust set test significant variant p-value ceiling\n                           (default 0.05).\n  --set-r2 [v] [\\'write\\'] : Adjust set test significant variant pairwise r^2\n                           ceiling (default 0.5).  \\'write\\'causes violating\n                          pairs to be dumped to <output prefix>.ldset.\n  --set-max <ct>         : Adjust set test maximum # of significant variants\n                           considered per set (default 5).\n  --set-test-lambda <v>  : Specify genomic control correction for set test.\n  --border <kbs>            : Extend --annotate range intervals by given # kbs.\n  --annotate-snp-field <nm> : Set --annotate variant ID field name.\n  --clump-p1 <pval> : Set --clump index var. p-value ceiling (default 1e-4).\n  --clump-p2 <pval> : Set --clump secondary p-value threshold (default 0.01).\n  --clump-r2 <r^2>  : Set --clump r^2 threshold (default 0.5).\n  --clump-kb <kbs>  : Set --clump kb radius (default 250).\n  --clump-snp-field <n...>  : Set --clump variant ID field name (default\n                              \\'SNP\\').  With multiple field names, earlier names\n                              take precedence over later ones.\n  --clump-field <name...>   : Set --clump p-value field name (default \\'P\\').\n  --clump-allow-overlap     : Let --clump non-index vars. join multiple clumps.\n  --clump-verbose           : Request extended --clump report.\n  --clump-annotate <hdr...> : Include named extra fields in --clump-verbose and\n                              --clump-best reports.  (Field names can be\n                              separated with spaces or commas.)\n  --clump-range <filename>  : Report overlaps between clumps and regions.\n  --clump-range-border <kb> : Stretch regions in --clump-range file.\n  --clump-index-first       : Extract --clump index vars. from only first file.\n  --clump-replicate         : Exclude clumps which contain secondary results\n                              from only one file.\n  --clump-best              : Report best proxy for each --clump index var.\n  --meta-analysis-chr-field <n...> : Set --meta-analysis chromosome, variant\n  --meta-analysis-snp-field <n...>   ID, position, A1/A2 allele, p-value,\n  --meta-analysis-bp-field <n...>    standard error, and/or effective sample\n  --meta-analysis-a1-field <n...>    size field names.\n  --meta-analysis-a2-field <n...>    Defaults are \\'CHR\\', \\'SNP\\', \\'BP\\', \\'A1\\',\n  --meta-analysis-p-field <n...>     \\'A2\\', \\'P\\', \\'SE\\', and \\'NMISS\\',\n  --meta-analysis-se-field <n...>    respectively.  When multiple parameters\n  --meta-analysis-ess-field <n...>   are given to these flags, earlier names\n                                     take precedence over later ones.\n                                     Note that, if the numbers of cases and\n                                     controls are unequal, effective sample\n                                     size should be\n                                       4 / (1/<# cases> + 1/<# controls>).\n  --meta-analysis-report-dups      : When a variant appears multiple times in\n                                     in the same file, report that.\n  --gene-list-border <kbs>   : Extend --gene-report regions by given # of kbs.\n  --gene-subset <filename>   : Specify gene name subset for --gene-report.\n  --gene-report-snp-field <> : Set --gene-report variant ID field name (default\n                               \\'SNP\\').  Only relevant with --extract.\n  --gap <kbs>      : Set \"--fast-epistasis case-only\" min. gap (default 1000).\n  --epi1 <p-value> : Set --[fast-]epistasis reporting threshold (default\n                     5e-6 for \\'boost\\', 1e-4 otherwise).\n  --epi2 <p-value> : Set threshold for contributing to SIG_E count (def. 0.01).\n  --je-cellmin <n> : Set required number of observations per 3x3x2 contingency\n                     table cell for joint-effects test (default 5).\n  --q-score-range <range file> <data file> [i] [j] [\\'header\\'] :\n    Apply --score to subset(s) of variants in the primary score list based\n    on e.g. p-value ranges.\n    * The first file should have range labels in the first column, p-value\n      lower bounds in the second column, and upper bounds in the third column.\n      Lines with too few entries, or nonnumeric values in the second or third\n      column, are ignored.\n    * The second file should contain a variant ID and a p-value on each\n      nonempty line (except possibly the first).  Variant IDs are read from\n      column #i and p-values are read from column #j, where i defaults to 1 and\n      j defaults to i+1.  The \\'header\\'modifier causes the first nonempty line\n      of this file to be skipped.\n  --R-port <port #>  : Connect to Rserve on a port other than 6311.\n  --R-host <host>    : Connect to Rserve host.\n  --R-socket <sock>  : Connect to Rserve socket.\n  --parallel <k> <n> : Divide the output matrix into n pieces, and only compute\n                       the kth piece.  The primary output file will have the\n                       piece number included in its name, e.g. plink.rel.13 or\n                       plink.rel.13.gz if k is 13.  Concatenating these files\n                       in order will yield the full matrix of interest.  (Yes,\n                       this can be done before unzipping.)\n                       N.B. This generally cannot be used to directly write a\n                       symmetric square matrix.  Choose square0 or triangle\n                       shape instead, and postprocess as necessary.\n  --memory <val>     : Set size, in MB, of initial workspace malloc attempt.\n                       (Practically mandatory when using GNU parallel.)\n  --threads <val>    : Set maximum number of concurrent threads.\n                       This has one known limitation: some BLAS/LAPACK linear\n                       algebra operations are multithreaded in a way that PLINK\n                       cannot control.  If this is problematic, you should\n                       recompile against single-threaded BLAS/LAPACK.\n  --d <char>         : Change variant/covariate range delimiter (normally \\'-\\').\n  --seed <val...>    : Set random number seed(s).  Each value must be an\n                       integer between 0 and 4294967295 inclusive.\n  --perm-batch-size <val> : Set number of permutations per batch for some\n                            permutation tests.\n  --native           : Allow Intel MKL to use processor-dependent code paths.\n  --output-min-p <p> : Specify minimum p-value to write to reports.\n  --debug            : Use slower, more crash-resistant logging method.\n\nPrimary methods paper:\nChang CC, Chow CC, Tellier LCAM, Vattikuti S, Purcell SM, Lee JJ (2015)\nSecond-generation PLINK: rising to the challenge of larger and richer datasets.\nGigaScience, 4.\n\nFor further documentation and support, consult the main webpage\n(https://www.cog-genomics.org/plink/1.9 ) and/or the mailing list\n(https://groups.google.com/d/forum/plink2-users ).\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.11.3"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.11.3\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.12.0"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.12.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.10.1"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.10.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.11.2"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.11.2\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.11.0"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.11.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.10.0"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.10.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.10.2"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.10.2\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.11.1"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.11.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.13.0"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.13.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.13.1"}, {"role": "assistant", "content":  "Version Info: ### PLEASE UPGRADE SALMON ###\n### A newer version of salmon with important bug fixes and improvements is available. ####\n###\nThe newest version, available at https://github.com/COMBINE-lab/salmon/releases\ncontains new features, improvements, and bug fixes; please upgrade at your\nearliest convenience.\n###\nSign up for the salmon mailing list to hear about new versions, features and updates at:\nhttps://oceangenomics.com/subscribe\n###\nsalmon v0.13.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.14.0"}, {"role": "assistant", "content":  "salmon v0.14.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.14.1"}, {"role": "assistant", "content":  "salmon v0.14.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.14.2"}, {"role": "assistant", "content":  "salmon v0.14.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.0.0"}, {"role": "assistant", "content":  "salmon v1.0.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-0.15.0"}, {"role": "assistant", "content":  "salmon v0.15.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-v0.12.0ds1-1b1-deb"}, {"role": "assistant", "content":  "salmon v0.12.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.1.0"}, {"role": "assistant", "content":  "salmon v1.1.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.2.0"}, {"role": "assistant", "content":  "salmon v1.2.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.2.1"}, {"role": "assistant", "content":  "salmon v1.2.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index Create a salmon index\n     quant Quantify a sample\n     alevin single cell analysis\n     swim  Perform super-secret operation\n     quantmerge Merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.3.0"}, {"role": "assistant", "content":  "salmon v1.3.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.4.0"}, {"role": "assistant", "content":  "salmon v1.4.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.5.0"}, {"role": "assistant", "content":  "salmon v1.5.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.5.1"}, {"role": "assistant", "content":  "salmon v1.5.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.5.2"}, {"role": "assistant", "content":  "salmon v1.5.2\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.6.0"}, {"role": "assistant", "content":  "salmon v1.6.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.7.0"}, {"role": "assistant", "content":  "salmon v1.7.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.10.1"}, {"role": "assistant", "content":  "salmon v1.10.1\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.10.0"}, {"role": "assistant", "content":  "salmon v1.10.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.9.0"}, {"role": "assistant", "content":  "salmon v1.9.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.8.0"}, {"role": "assistant", "content":  "salmon v1.8.0\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.10.2"}, {"role": "assistant", "content":  "salmon v1.10.2\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "salmon-1.10.3"}, {"role": "assistant", "content":  "salmon v1.10.3\n\nUsage:  salmon -h|--help or \n        salmon -v|--version or \n        salmon -c|--cite or \n        salmon [--no-version-check] <COMMAND> [-h | options]\n\nCommands:\n     index      : create a salmon index\n     quant      : quantify a sample\n     alevin     : single cell analysis\n     swim       : perform super-secret operation\n     quantmerge : merge multiple quantifications into a single file\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.9"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.9 (using htslib 1.9)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.7"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.7 (using htslib 1.7)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.3.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.3.1 (using htslib 1.3.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.3"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.3 (using htslib 1.3)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.5"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.5 (using htslib 1.5)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.0"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.0 (using htslib 1.0)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.8"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.8 (using htslib 1.8)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.6"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.6 (using htslib 1.6)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.4.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.4.1 (using htslib 1.4.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.2.rglab"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.2-rglab-CRG (using htslib 1.2.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.4"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.4 (using htslib 1.4)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-v1.2"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.2 (using htslib 1.2.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-v1.3"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.3.1 (using htslib 1.3.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-v1.7.0"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.7 (using htslib 1.7)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-v1.3.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.3.1 (using htslib 1.3.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     rmdup          remove PCR duplicates\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.2"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.2-rglab-CRG (using htslib 1.2.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.1 (using htslib 1.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-v1.9-4-deb"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.9 (using htslib 1.9)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.10"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.10 (using htslib 1.10)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.2-0"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.2-rglab-CRG (using htslib 1.2.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- indexing\n         faidx       index/extract FASTA\n         index       index alignment\n  -- editing\n         calmd       recalculate MD/NM tags and \\'=\\'bases\n         fixmate     fix mate information\n         reheader    replace BAM header\n         rmdup       remove PCR duplicates\n         targetcut   cut fosmid regions (for fosmid pool only)\n  -- file operations\n         bamshuf     shuffle and group alignments by name\n         cat         concatenate BAMs\n         merge       merge sorted alignments\n         mpileup     multi-way pileup\n         sort        sort alignment file\n         split       splits a file by read group\n         bam2fq      converts a BAM to a FASTQ\n  -- stats\n         bedcov      read depth per BED region\n         depth       compute the depth\n         flagstat    simple stats\n         idxstats    BAM index stats\n         phase       phase heterozygotes\n         stats       generate stats (former bamcheck)\n  -- viewing\n         flags       explain BAM flags\n         tview       text alignment viewer\n         view        SAM<->BAM<->CRAM conversion\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.11"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.11 (using htslib 1.11)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.12"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.12 (using htslib 1.12)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.13"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.13 (using htslib 1.13)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.14"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.14 (using htslib 1.14)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.15"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.15 (using htslib 1.15)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.15.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.15.1 (using htslib 1.16)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.16.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.16.1 (using htslib 1.17)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.17"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.17 (using htslib 1.17)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.18"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.18 (using htslib 1.18)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.19"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.19 (using htslib 1.19)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.19.2"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.19.2 (using htslib 1.19.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.19.1"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.19.1 (using htslib 1.19.1)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "samtools-1.20"}, {"role": "assistant", "content":  "\nProgram: samtools (Tools for alignments in the SAM format)\nVersion: 1.20 (using htslib 1.20)\n\nUsage:   samtools <command> [options]\n\nCommands:\n  -- Indexing\n     dict           create a sequence dictionary file\n     faidx          index/extract FASTA\n     fqidx          index/extract FASTQ\n     index          index alignment\n\n  -- Editing\n     calmd          recalculate MD/NM tags and \\'=\\'bases\n     fixmate        fix mate information\n     reheader       replace BAM header\n     targetcut      cut fosmid regions (for fosmid pool only)\n     addreplacerg   adds or replaces RG tags\n     markdup        mark duplicates\n     ampliconclip   clip oligos from the end of reads\n\n  -- File operations\n     collate        shuffle and group alignments by name\n     cat            concatenate BAMs\n     consensus      produce a consensus Pileup/FASTA/FASTQ\n     merge          merge sorted alignments\n     mpileup        multi-way pileup\n     sort           sort alignment file\n     split          splits a file by read group\n     quickcheck     quickly check if SAM/BAM/CRAM file appears intact\n     fastq          converts a BAM to a FASTQ\n     fasta          converts a BAM to a FASTA\n     import         Converts FASTA or FASTQ files to SAM/BAM/CRAM\n     reference      Generates a reference from aligned data\n     reset          Reverts aligner changes in reads\n\n  -- Statistics\n     bedcov         read depth per BED region\n     coverage       alignment depth and percent coverage\n     depth          compute the depth\n     flagstat       simple stats\n     idxstats       BAM index stats\n     cram-size      list CRAM Content-ID and Data-Series sizes\n     phase          phase heterozygotes\n     stats          generate stats (former bamcheck)\n     ampliconstats  generate amplicon specific stats\n\n  -- Viewing\n     flags          explain BAM flags\n     head           header viewer\n     tview          text alignment viewer\n     view           SAM<->BAM<->CRAM conversion\n     depad          convert padded BAM to unpadded BAM\n     samples        list the samples in a set of SAM/BAM/CRAM files\n\n  -- Misc\n     help [cmd]     display this help message or help for [cmd]\n     version        detailed version information\n\n"}]}
{"messages": [{"role": "user", "content":   "tophat-2.0.13"}, {"role": "assistant", "content":  "tophat: \nTopHat maps short sequences from spliced transcripts to whole genomes.\n\nUsage:\n    tophat [options] <bowtie_index> <reads1[,reads2,...]> [reads1[,reads2,...]] \\\n                                    [quals1,[quals2,...]] [quals1[,quals2,...]]\n\nOptions:\n    -v/--version\n    -o/--output-dir                <string>    [ default: ./tophat_out         ]\n    --bowtie1                                  [ default: bowtie2              ]\n    -N/--read-mismatches           <int>       [ default: 2                    ]\n    --read-gap-length              <int>       [ default: 2                    ]\n    --read-edit-dist               <int>       [ default: 2                    ]\n    --read-realign-edit-dist       <int>       [ default: \"read-edit-dist\" + 1 ]\n    -a/--min-anchor                <int>       [ default: 8                    ]\n    -m/--splice-mismatches         <0-2>       [ default: 0                    ]\n    -i/--min-intron-length         <int>       [ default: 50                   ]\n    -I/--max-intron-length         <int>       [ default: 500000               ]\n    -g/--max-multihits             <int>       [ default: 20                   ]\n    --suppress-hits\n    -x/--transcriptome-max-hits    <int>       [ default: 60                   ]\n    -M/--prefilter-multihits                   ( for -G/--GTF option, enable\n                                                 an initial bowtie search\n                                                 against the genome )\n    --max-insertion-length         <int>       [ default: 3                    ]\n    --max-deletion-length          <int>       [ default: 3                    ]\n    --solexa-quals\n    --solexa1.3-quals                          (same as phred64-quals)\n    --phred64-quals                            (same as solexa1.3-quals)\n    -Q/--quals\n    --integer-quals\n    -C/--color                                 (Solid - color space)\n    --color-out\n    --library-type                 <string>    (fr-unstranded, fr-firststrand,\n                                                fr-secondstrand)\n    -p/--num-threads               <int>       [ default: 1                   ]\n    -R/--resume                    <out_dir>   ( try to resume execution )\n    -G/--GTF                       <filename>  (GTF/GFF with known transcripts)\n    --transcriptome-index          <bwtidx>    (transcriptome bowtie index)\n    -T/--transcriptome-only                    (map only to the transcriptome)\n    -j/--raw-juncs                 <filename>\n    --insertions                   <filename>\n    --deletions                    <filename>\n    -r/--mate-inner-dist           <int>       [ default: 50                  ]\n    --mate-std-dev                 <int>       [ default: 20                  ]\n    --no-novel-juncs\n    --no-novel-indels\n    --no-gtf-juncs\n    --no-coverage-search\n    --coverage-search\n    --microexon-search\n    --keep-tmp\n    --tmp-dir                      <dirname>   [ default: <output_dir>/tmp ]\n    -z/--zpacker                   <program>   [ default: gzip             ]\n    -X/--unmapped-fifo                         [use mkfifo to compress more temporary\n                                                 files for color space reads]\n\nAdvanced Options:\n    --report-secondary-alignments\n    --no-discordant\n    --no-mixed\n\n    --segment-mismatches           <int>       [ default: 2                ]\n    --segment-length               <int>       [ default: 25               ]\n\n    --bowtie-n                                 [ default: bowtie -v        ]\n    --min-coverage-intron          <int>       [ default: 50               ]\n    --max-coverage-intron          <int>       [ default: 20000            ]\n    --min-segment-intron           <int>       [ default: 50               ]\n    --max-segment-intron           <int>       [ default: 500000           ]\n    --no-sort-bam                              (Output BAM is not coordinate-sorted)\n    --no-convert-bam                           (Do not output bam format.\n                                                Output is <output_dir>/accepted_hit.sam)\n    --keep-fasta-order\n    --allow-partial-mapping\n\nBowtie2 related options:\n  Preset options in --end-to-end mode (local alignment is not used in TopHat2)\n    --b2-very-fast\n    --b2-fast\n    --b2-sensitive\n    --b2-very-sensitive\n\n  Alignment options\n    --b2-N                         <int>       [ default: 0                ]\n    --b2-L                         <int>       [ default: 20               ]\n    --b2-i                         <func>      [ default: S,1,1.25         ]\n    --b2-n-ceil                    <func>      [ default: L,0,0.15         ]\n    --b2-gbar                      <int>       [ default: 4                ]\n\n  Scoring options\n    --b2-mp                        <int>,<int> [ default: 6,2              ]\n    --b2-np                        <int>       [ default: 1                ]\n    --b2-rdg                       <int>,<int> [ default: 5,3              ]\n    --b2-rfg                       <int>,<int> [ default: 5,3              ]\n    --b2-score-min                 <func>      [ default: L,-0.6,-0.6      ]\n\n  Effort options\n    --b2-D                         <int>       [ default: 15               ]\n    --b2-R                         <int>       [ default: 2                ]\n\nFusion related options:\n    --fusion-search\n    --fusion-anchor-length         <int>       [ default: 20               ]\n    --fusion-min-dist              <int>       [ default: 10000000         ]\n    --fusion-read-mismatches       <int>       [ default: 2                ]\n    --fusion-multireads            <int>       [ default: 2                ]\n    --fusion-multipairs            <int>       [ default: 2                ]\n    --fusion-ignore-chromosomes    <list>      [ e.g, <chrM,chrX>          ]\n\n    --fusion-do-not-resolve-conflicts          [this is for test purposes  ]\n\nSAM Header Options (for embedding sequencing run metadata in output):\n    --rg-id                        <string>    (read group ID)\n    --rg-sample                    <string>    (sample ID)\n    --rg-library                   <string>    (library ID)\n    --rg-description               <string>    (descriptive string, no tabs allowed)\n    --rg-platform-unit             <string>    (e.g Illumina lane ID)\n    --rg-center                    <string>    (sequencing center name)\n    --rg-date                      <string>    (ISO 8601 date of the sequencing run)\n    --rg-platform                  <string>    (Sequencing platform descriptor)\n\n    for detailed help see http://tophat.cbcb.umd.edu/manual.html\n"}]}
{"messages": [{"role": "user", "content":   "tophat-2.1.0"}, {"role": "assistant", "content":  "tophat: \nTopHat maps short sequences from spliced transcripts to whole genomes.\n\nUsage:\n    tophat [options] <bowtie_index> <reads1[,reads2,...]> [reads1[,reads2,...]] \\\n                                    [quals1,[quals2,...]] [quals1[,quals2,...]]\n\nOptions:\n    -v/--version\n    -o/--output-dir                <string>    [ default: ./tophat_out         ]\n    --bowtie1                                  [ default: bowtie2              ]\n    -N/--read-mismatches           <int>       [ default: 2                    ]\n    --read-gap-length              <int>       [ default: 2                    ]\n    --read-edit-dist               <int>       [ default: 2                    ]\n    --read-realign-edit-dist       <int>       [ default: \"read-edit-dist\" + 1 ]\n    -a/--min-anchor                <int>       [ default: 8                    ]\n    -m/--splice-mismatches         <0-2>       [ default: 0                    ]\n    -i/--min-intron-length         <int>       [ default: 50                   ]\n    -I/--max-intron-length         <int>       [ default: 500000               ]\n    -g/--max-multihits             <int>       [ default: 20                   ]\n    --suppress-hits\n    -x/--transcriptome-max-hits    <int>       [ default: 60                   ]\n    -M/--prefilter-multihits                   ( for -G/--GTF option, enable\n                                                 an initial bowtie search\n                                                 against the genome )\n    --max-insertion-length         <int>       [ default: 3                    ]\n    --max-deletion-length          <int>       [ default: 3                    ]\n    --solexa-quals\n    --solexa1.3-quals                          (same as phred64-quals)\n    --phred64-quals                            (same as solexa1.3-quals)\n    -Q/--quals\n    --integer-quals\n    -C/--color                                 (Solid - color space)\n    --color-out\n    --library-type                 <string>    (fr-unstranded, fr-firststrand,\n                                                fr-secondstrand)\n    -p/--num-threads               <int>       [ default: 1                   ]\n    -R/--resume                    <out_dir>   ( try to resume execution )\n    -G/--GTF                       <filename>  (GTF/GFF with known transcripts)\n    --transcriptome-index          <bwtidx>    (transcriptome bowtie index)\n    -T/--transcriptome-only                    (map only to the transcriptome)\n    -j/--raw-juncs                 <filename>\n    --insertions                   <filename>\n    --deletions                    <filename>\n    -r/--mate-inner-dist           <int>       [ default: 50                  ]\n    --mate-std-dev                 <int>       [ default: 20                  ]\n    --no-novel-juncs\n    --no-novel-indels\n    --no-gtf-juncs\n    --no-coverage-search\n    --coverage-search\n    --microexon-search\n    --keep-tmp\n    --tmp-dir                      <dirname>   [ default: <output_dir>/tmp ]\n    -z/--zpacker                   <program>   [ default: gzip             ]\n    -X/--unmapped-fifo                         [use mkfifo to compress more temporary\n                                                 files for color space reads]\n\nAdvanced Options:\n    --report-secondary-alignments\n    --no-discordant\n    --no-mixed\n\n    --segment-mismatches           <int>       [ default: 2                ]\n    --segment-length               <int>       [ default: 25               ]\n\n    --bowtie-n                                 [ default: bowtie -v        ]\n    --min-coverage-intron          <int>       [ default: 50               ]\n    --max-coverage-intron          <int>       [ default: 20000            ]\n    --min-segment-intron           <int>       [ default: 50               ]\n    --max-segment-intron           <int>       [ default: 500000           ]\n    --no-sort-bam                              (Output BAM is not coordinate-sorted)\n    --no-convert-bam                           (Do not output bam format.\n                                                Output is <output_dir>/accepted_hits.sam)\n    --keep-fasta-order\n    --allow-partial-mapping\n\nBowtie2 related options:\n  Preset options in --end-to-end mode (local alignment is not used in TopHat2)\n    --b2-very-fast\n    --b2-fast\n    --b2-sensitive\n    --b2-very-sensitive\n\n  Alignment options\n    --b2-N                         <int>       [ default: 0                ]\n    --b2-L                         <int>       [ default: 20               ]\n    --b2-i                         <func>      [ default: S,1,1.25         ]\n    --b2-n-ceil                    <func>      [ default: L,0,0.15         ]\n    --b2-gbar                      <int>       [ default: 4                ]\n\n  Scoring options\n    --b2-mp                        <int>,<int> [ default: 6,2              ]\n    --b2-np                        <int>       [ default: 1                ]\n    --b2-rdg                       <int>,<int> [ default: 5,3              ]\n    --b2-rfg                       <int>,<int> [ default: 5,3              ]\n    --b2-score-min                 <func>      [ default: L,-0.6,-0.6      ]\n\n  Effort options\n    --b2-D                         <int>       [ default: 15               ]\n    --b2-R                         <int>       [ default: 2                ]\n\nFusion related options:\n    --fusion-search\n    --fusion-anchor-length         <int>       [ default: 20               ]\n    --fusion-min-dist              <int>       [ default: 10000000         ]\n    --fusion-read-mismatches       <int>       [ default: 2                ]\n    --fusion-multireads            <int>       [ default: 2                ]\n    --fusion-multipairs            <int>       [ default: 2                ]\n    --fusion-ignore-chromosomes    <list>      [ e.g, <chrM,chrX>          ]\n\n    --fusion-do-not-resolve-conflicts          [this is for test purposes  ]\n\nSAM Header Options (for embedding sequencing run metadata in output):\n    --rg-id                        <string>    (read group ID)\n    --rg-sample                    <string>    (sample ID)\n    --rg-library                   <string>    (library ID)\n    --rg-description               <string>    (descriptive string, no tabs allowed)\n    --rg-platform-unit             <string>    (e.g Illumina lane ID)\n    --rg-center                    <string>    (sequencing center name)\n    --rg-date                      <string>    (ISO 8601 date of the sequencing run)\n    --rg-platform                  <string>    (Sequencing platform descriptor)\n\n    for detailed help see http://tophat.cbcb.umd.edu/manual.html\n"}]}
{"messages": [{"role": "user", "content":   "tophat-2.1.1"}, {"role": "assistant", "content":  "tophat: \nTopHat maps short sequences from spliced transcripts to whole genomes.\n\nUsage:\n    tophat [options] <bowtie_index> <reads1[,reads2,...]> [reads1[,reads2,...]] \\\n                                    [quals1,[quals2,...]] [quals1[,quals2,...]]\n\nOptions:\n    -v/--version\n    -o/--output-dir                <string>    [ default: ./tophat_out         ]\n    --bowtie1                                  [ default: bowtie2              ]\n    -N/--read-mismatches           <int>       [ default: 2                    ]\n    --read-gap-length              <int>       [ default: 2                    ]\n    --read-edit-dist               <int>       [ default: 2                    ]\n    --read-realign-edit-dist       <int>       [ default: \"read-edit-dist\" + 1 ]\n    -a/--min-anchor                <int>       [ default: 8                    ]\n    -m/--splice-mismatches         <0-2>       [ default: 0                    ]\n    -i/--min-intron-length         <int>       [ default: 50                   ]\n    -I/--max-intron-length         <int>       [ default: 500000               ]\n    -g/--max-multihits             <int>       [ default: 20                   ]\n    --suppress-hits\n    -x/--transcriptome-max-hits    <int>       [ default: 60                   ]\n    -M/--prefilter-multihits                   ( for -G/--GTF option, enable\n                                                 an initial bowtie search\n                                                 against the genome )\n    --max-insertion-length         <int>       [ default: 3                    ]\n    --max-deletion-length          <int>       [ default: 3                    ]\n    --solexa-quals\n    --solexa1.3-quals                          (same as phred64-quals)\n    --phred64-quals                            (same as solexa1.3-quals)\n    -Q/--quals\n    --integer-quals\n    -C/--color                                 (Solid - color space)\n    --color-out\n    --library-type                 <string>    (fr-unstranded, fr-firststrand,\n                                                fr-secondstrand)\n    -p/--num-threads               <int>       [ default: 1                   ]\n    -R/--resume                    <out_dir>   ( try to resume execution )\n    -G/--GTF                       <filename>  (GTF/GFF with known transcripts)\n    --transcriptome-index          <bwtidx>    (transcriptome bowtie index)\n    -T/--transcriptome-only                    (map only to the transcriptome)\n    -j/--raw-juncs                 <filename>\n    --insertions                   <filename>\n    --deletions                    <filename>\n    -r/--mate-inner-dist           <int>       [ default: 50                  ]\n    --mate-std-dev                 <int>       [ default: 20                  ]\n    --no-novel-juncs\n    --no-novel-indels\n    --no-gtf-juncs\n    --no-coverage-search\n    --coverage-search\n    --microexon-search\n    --keep-tmp\n    --tmp-dir                      <dirname>   [ default: <output_dir>/tmp ]\n    -z/--zpacker                   <program>   [ default: gzip             ]\n    -X/--unmapped-fifo                         [use mkfifo to compress more temporary\n                                                 files for color space reads]\n\nAdvanced Options:\n    --report-secondary-alignments\n    --no-discordant\n    --no-mixed\n\n    --segment-mismatches           <int>       [ default: 2                ]\n    --segment-length               <int>       [ default: 25               ]\n\n    --bowtie-n                                 [ default: bowtie -v        ]\n    --min-coverage-intron          <int>       [ default: 50               ]\n    --max-coverage-intron          <int>       [ default: 20000            ]\n    --min-segment-intron           <int>       [ default: 50               ]\n    --max-segment-intron           <int>       [ default: 500000           ]\n    --no-sort-bam                              (Output BAM is not coordinate-sorted)\n    --no-convert-bam                           (Do not output bam format.\n                                                Output is <output_dir>/accepted_hits.sam)\n    --keep-fasta-order\n    --allow-partial-mapping\n\nBowtie2 related options:\n  Preset options in --end-to-end mode (local alignment is not used in TopHat2)\n    --b2-very-fast\n    --b2-fast\n    --b2-sensitive\n    --b2-very-sensitive\n\n  Alignment options\n    --b2-N                         <int>       [ default: 0                ]\n    --b2-L                         <int>       [ default: 20               ]\n    --b2-i                         <func>      [ default: S,1,1.25         ]\n    --b2-n-ceil                    <func>      [ default: L,0,0.15         ]\n    --b2-gbar                      <int>       [ default: 4                ]\n\n  Scoring options\n    --b2-mp                        <int>,<int> [ default: 6,2              ]\n    --b2-np                        <int>       [ default: 1                ]\n    --b2-rdg                       <int>,<int> [ default: 5,3              ]\n    --b2-rfg                       <int>,<int> [ default: 5,3              ]\n    --b2-score-min                 <func>      [ default: L,-0.6,-0.6      ]\n\n  Effort options\n    --b2-D                         <int>       [ default: 15               ]\n    --b2-R                         <int>       [ default: 2                ]\n\nFusion related options:\n    --fusion-search\n    --fusion-anchor-length         <int>       [ default: 20               ]\n    --fusion-min-dist              <int>       [ default: 10000000         ]\n    --fusion-read-mismatches       <int>       [ default: 2                ]\n    --fusion-multireads            <int>       [ default: 2                ]\n    --fusion-multipairs            <int>       [ default: 2                ]\n    --fusion-ignore-chromosomes    <list>      [ e.g, <chrM,chrX>          ]\n\n    --fusion-do-not-resolve-conflicts          [this is for test purposes  ]\n\nSAM Header Options (for embedding sequencing run metadata in output):\n    --rg-id                        <string>    (read group ID)\n    --rg-sample                    <string>    (sample ID)\n    --rg-library                   <string>    (library ID)\n    --rg-description               <string>    (descriptive string, no tabs allowed)\n    --rg-platform-unit             <string>    (e.g Illumina lane ID)\n    --rg-center                    <string>    (sequencing center name)\n    --rg-date                      <string>    (ISO 8601 date of the sequencing run)\n    --rg-platform                  <string>    (Sequencing platform descriptor)\n\n    for detailed help see http://ccb.jhu.edu/software/tophat/manual.shtml\n"}]}
{"messages": [{"role": "user", "content":   "trimmomatic-0.33"}, {"role": "assistant", "content":  "/usr/local/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nUsage: \n       PE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-basein <inputBase> | <inputFile1> <inputFile2>] [-baseout <outputBase> | <outputFile1P> <outputFile1U> <outputFile2P> <outputFile2U>] <trimmer1>...\n   or: \n       SE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] <inputFile> <outputFile> <trimmer1>...\n"}]}
{"messages": [{"role": "user", "content":   "trimmomatic-0.36"}, {"role": "assistant", "content":  "/usr/local/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nUsage: \n       PE [-version] [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-quiet] [-validatePairs] [-basein <inputBase> | <inputFile1> <inputFile2>] [-baseout <outputBase> | <outputFile1P> <outputFile1U> <outputFile2P> <outputFile2U>] <trimmer1>...\n   or: \n       SE [-version] [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-quiet] <inputFile> <outputFile> <trimmer1>...\n   or: \n       -version\n"}]}
{"messages": [{"role": "user", "content":   "trimmomatic-0.38"}, {"role": "assistant", "content":  "/usr/local/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nUsage: \n       PE [-version] [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-summary <statsSummaryFile>] [-quiet] [-validatePairs] [-basein <inputBase> | <inputFile1> <inputFile2>] [-baseout <outputBase> | <outputFile1P> <outputFile1U> <outputFile2P> <outputFile2U>] <trimmer1>...\n   or: \n       SE [-version] [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-summary <statsSummaryFile>] [-quiet] <inputFile> <outputFile> <trimmer1>...\n   or: \n       -version\n"}]}
{"messages": [{"role": "user", "content":   "trimmomatic-0.32"}, {"role": "assistant", "content":  "/usr/local/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nUsage: \n       PE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-basein <inputBase> | <inputFile1> <inputFile2>] [-baseout <outputBase> | <outputFile1P> <outputFile1U> <outputFile2P> <outputFile2U>] <trimmer1>...\n   or: \n       SE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] <inputFile> <outputFile> <trimmer1>...\n"}]}
{"messages": [{"role": "user", "content":   "trimmomatic-0.39"}, {"role": "assistant", "content":  "/usr/local/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nUsage: \n       PE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-basein <inputBase> | <inputFile1> <inputFile2>] [-baseout <outputBase> | <outputFile1P> <outputFile1U> <outputFile2P> <outputFile2U>] <trimmer1>...\n   or: \n       SE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] <inputFile> <outputFile> <trimmer1>...\n"}]}
{"messages": [{"role": "user", "content":   "trimmomatic-0.35"}, {"role": "assistant", "content":  "/usr/local/bin/trimmomatic: line 6: warning: setlocale: LC_ALL: cannot change locale (en_US.UTF-8): No such file or directory\nUsage: \n       PE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-quiet] [-validatePairs] [-basein <inputBase> | <inputFile1> <inputFile2>] [-baseout <outputBase> | <outputFile1P> <outputFile1U> <outputFile2P> <outputFile2U>] <trimmer1>...\n   or: \n       SE [-threads <threads>] [-phred33|-phred64] [-trimlog <trimLogFile>] [-quiet] <inputFile> <outputFile> <trimmer1>...\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.23.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-v1.23.0-plusds-1-deb"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.24.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.25.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.26.1"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.29.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.26.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.27.1"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.28.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.27.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.30.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.31.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.32.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.33.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.34.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.35.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.36.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.37.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.38.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.47.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.46.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.45.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.44.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.43.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.41.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.40.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.39.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.48.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.49.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.50.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.50.1"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.51.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.52.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.53.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.54.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.55.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
{"messages": [{"role": "user", "content":   "vg-1.56.0"}, {"role": "assistant", "content":  "usage: vg map [options] -d idxbase -f in1.fq [-f in2.fq] >aln.gam\nAlign reads to a graph.\n\ngraph/index:\n    -d, --base-name BASE          use BASE.xg and BASE.gcsa as the input index pair\n    -x, --xg-name FILE            use this xg index or graph (defaults to <graph>.vg.xg)\n    -g, --gcsa-name FILE          use this GCSA2 index (defaults to <graph>.gcsa)\n    -1, --gbwt-name FILE          use this GBWT haplotype index (defaults to <graph>.gbwt)\nalgorithm:\n    -t, --threads N               number of compute threads to use\n    -k, --min-mem INT             minimum MEM length (if 0 estimate via -e) [0]\n    -e, --mem-chance FLOAT        set {-k} such that this fraction of {-k} length hits will by chance [5e-4]\n    -c, --hit-max N               ignore MEMs who have >N hits in our index (0 for no limit) [2048]\n    -Y, --max-mem INT             ignore mems longer than this length (unset if 0) [0]\n    -r, --reseed-x FLOAT          look for internal seeds inside a seed longer than FLOAT*--min-seed [1.5]\n    -u, --try-up-to INT           attempt to align up to the INT best candidate chains of seeds (1/2 for paired) [128]\n    -l, --try-at-least INT        attempt to align at least the INT best candidate chains of seeds [1]\n    -E, --approx-mq-cap INT       weight MQ by suffix tree based estimate when estimate less than FLOAT [0]\n    --id-mq-weight N              scale mapping quality by the alignment score identity to this power [2]\n    -W, --min-chain INT           discard a chain if seeded bases shorter than INT [0]\n    -C, --drop-chain FLOAT        drop chains shorter than FLOAT fraction of the longest overlapping chain [0.45]\n    -n, --mq-overlap FLOAT        scale MQ by count of alignments with this overlap in the query with the primary [0]\n    -P, --min-ident FLOAT         accept alignment only if the alignment identity is >= FLOAT [0]\n    -H, --max-target-x N          skip cluster subgraphs with length > N*read_length [100]\n    -w, --band-width INT          band width for long read alignment [256]\n    -O, --band-overlap INT        band overlap for long read alignment [{-w}/8]\n    -J, --band-jump INT           the maximum number of bands of insertion we consider in the alignment chain model [128]\n    -B, --band-multi INT          consider this many alignments of each band in banded alignment [16]\n    -Z, --band-min-mq INT         treat bands with less than this MQ as unaligned [0]\n    -I, --fragment STR            fragment length distribution specification STR=m:\u03bc:\u03c3:o:d [5000:0:0:0:1]\n                                  max, mean, stdev, orientation (1=same, 0=flip), direction (1=forward, 0=backward)\n    -U, --fixed-frag-model        don\\'t learn the pair fragment model online, use {-I} without update\n    -p, --print-frag-model        suppress alignment output and print the fragment model on stdout as per {-I} format\n    --frag-calc INT               update the fragment model every INT perfect pairs [10]\n    --fragment-x FLOAT            calculate max fragment size as frag_mean+frag_sd*FLOAT [10]\n    --mate-rescues INT            attempt up to INT mate rescues per pair [64]\n    -S, --unpaired-cost INT       penalty for an unpaired read pair [17]\n    --no-patch-aln                do not patch banded alignments by locally aligning unaligned regions\n    --xdrop-alignment             use X-drop heuristic (much faster for long-read alignment)\n    --max-gap-length              maximum gap length allowed in each contiguous alignment (for X-drop alignment) [40]\nscoring:\n    -q, --match INT               use this match score [1]\n    -z, --mismatch INT            use this mismatch penalty [4]\n    --score-matrix FILE           read a 5x5 integer substitution scoring matrix from a file\n    -o, --gap-open INT            use this gap open penalty [6]\n    -y, --gap-extend INT          use this gap extension penalty [1]\n    -L, --full-l-bonus INT        the full-length alignment bonus [5]\n    --drop-full-l-bonus           remove the full length bonus from the score before sorting and MQ calculation\n    -a, --hap-exp FLOAT           the exponent for haplotype consistency likelihood in alignment score [1]\n    --recombination-penalty FLOAT use this log recombination penalty for GBWT haplotype scoring [20.7]\n    -A, --qual-adjust             perform base quality adjusted alignments (requires base quality input)\npreset:\n    -m, --alignment-model STR     use a preset alignment scoring model, either \"short\" (default) or \"long\" (for ONT/PacBio)\n                                  \"long\" is equivalent to `-u 2 -L 63 -q 1 -z 2 -o 2 -y 1 -w 128 -O 32`\ninput:\n    -s, --sequence STR            align a string to the graph in graph.vg using partial order alignment\n    -V, --seq-name STR            name the sequence using this value (for graph modification with new named paths)\n    -T, --reads FILE              take reads (one per line) from FILE, write alignments to stdout\n    -b, --hts-input FILE          align reads from htslib-compatible FILE (BAM/CRAM/SAM) stdin (-), alignments to stdout\n    -G, --gam-input FILE          realign GAM input\n    -f, --fastq FILE              input fastq or (2-line format) fasta, possibly compressed, two are allowed, one for each mate\n    -F, --fasta FILE              align the sequences in a FASTA file that may have multiple lines per reference sequence\n    -i, --interleaved             fastq or GAM is interleaved paired-ended\n    -N, --sample NAME             for --reads input, add this sample\n    -R, --read-group NAME         for --reads input, add this read group\noutput:\n    -j, --output-json             output JSON rather than an alignment stream (helpful for debugging)\n    --surject-to TYPE             surject the output into the graph\\'s paths, writing TYPE := bam |sam | cram\n    --buffer-size INT             buffer this many alignments together before outputting in GAM [512]\n    -X, --compare                 realign GAM input (-G), writing alignment with \"correct\" field set to overlap with input\n    -v, --refpos-table            for efficient testing output a table of name, chr, pos, mq, score\n    -K, --keep-secondary          produce alignments for secondary input alignments in addition to primary ones\n    -M, --max-multimaps INT       produce up to INT alignments for each read [1]\n    -Q, --mq-max INT              cap the mapping quality at INT [60]\n    --exclude-unaligned           exclude reads with no alignment\n    -D, --debug                   print debugging information about alignment to stderr\n"}]}
