{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import itertools\n",
        "from collections import Counter\n",
        "import json\n",
        "from dotenv import load_dotenv\n",
        "import os\n",
        "import openai\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import requests\n",
        "import azure\n",
        "from azure.identity import DefaultAzureCredential\n",
        "from openai import AzureOpenAI\n",
        "import evaluate\n",
        "import urllib.request\n",
        "import ssl\n",
        "from scipy import stats"
      ],
      "outputs": [],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1730210393699
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Load configuration file"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import yaml\n",
        "from datetime import datetime\n",
        "snapshot_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
        "\n",
        "with open('config.yaml') as f:\n",
        "    d = yaml.load(f, Loader=yaml.FullLoader)\n",
        "    \n",
        "AZURE_SUBSCRIPTION_ID = d['config']['AZURE_SUBSCRIPTION_ID']\n",
        "AZURE_RESOURCE_GROUP = d['config']['AZURE_RESOURCE_GROUP']\n",
        "AZURE_WORKSPACE = d['config']['AZURE_WORKSPACE']\n",
        "AZURE_DATA_NAME = d['config']['AZURE_DATA_NAME']    \n",
        "DATA_DIR = d['config']['DATA_DIR']\n",
        "CLOUD_DIR = d['config']['CLOUD_DIR']\n",
        "HF_MODEL_NAME_OR_PATH = d['config']['HF_MODEL_NAME_OR_PATH']\n",
        "IS_DEBUG = d['config']['IS_DEBUG']\n",
        "USE_LOWPRIORITY_VM = d['config']['USE_LOWPRIORITY_VM']\n",
        "\n",
        "\n",
        "rest_endpoint = d['evaluation']['rest_endpoint']\n",
        "evaluation_deployment_name = d['evaluation']['deployment_name']"
      ],
      "outputs": [],
      "execution_count": 17,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729688536378
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Configure workspace"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from azure.identity import DefaultAzureCredential, InteractiveBrowserCredential\n",
        "from azure.ai.ml import MLClient, Input\n",
        "from azure.ai.ml.dsl import pipeline\n",
        "from azure.ai.ml import load_component\n",
        "from azure.ai.ml import command\n",
        "from azure.ai.ml.entities import Data, Environment, BuildContext\n",
        "from azure.ai.ml.entities import Model\n",
        "from azure.ai.ml import Input\n",
        "from azure.ai.ml import Output\n",
        "from azure.ai.ml.constants import AssetTypes\n",
        "from azure.core.exceptions import ResourceNotFoundError, ResourceExistsError\n",
        "\n",
        "credential = DefaultAzureCredential()\n",
        "ml_client = None\n",
        "try:\n",
        "    ml_client = MLClient.from_config(credential)\n",
        "except Exception as ex:\n",
        "    print(ex)\n",
        "    ml_client = MLClient(credential, AZURE_SUBSCRIPTION_ID, AZURE_RESOURCE_GROUP, AZURE_WORKSPACE)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "Found the config file in: /config.json\n"
        }
      ],
      "execution_count": 18,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729688537959
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Format csv to json"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.read_csv(\"analysis_and_tools_only.csv\")"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729619681587
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#preprocess data\n",
        "\n",
        "input_string = []\n",
        "\n",
        "# Iterate through the rows of the dataframe\n",
        "for index, row in df.iterrows():\n",
        "    # 'content' is the column with the user questions\n",
        "    user_content = row['content']\n",
        "    # 'answer content' is the column with the ground truth answers\n",
        "    assistant_content = row['answer_content']\n",
        "\n",
        "    # Append only user content to the input_data\n",
        "    input_string.append({\"role\": \"user\", \"content\": user_content})\n",
        "\n",
        "# Define the parameters for the model\n",
        "params = {\n",
        "    \"temperature\": 0.1,\n",
        "    \"max_new_tokens\": 4096, #output\n",
        "    \"do_sample\": True,\n",
        "    \"return_full_text\": False\n",
        "}\n",
        "\n",
        "# Combine input data and parameters into the final test data structure\n",
        "test_data = {\n",
        "    \"input_data\": input_string,\n",
        "    \"parameters\": params\n",
        "}\n",
        "\n",
        "# Save the JSON output to a file\n",
        "with open('test_data.json', 'w') as json_file:\n",
        "    json.dump(test_data, json_file, indent=4)\n",
        "\n",
        "print(\"JSON file created successfully.\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "JSON file created successfully.\n"
        }
      ],
      "execution_count": 19,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729688542877
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import json\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "# Replace with your actual OpenAI endpoint URL and deployment details\n",
        "endpoint_url = \"<YOUR_OPENAI_ENDPOINT_URL>\"\n",
        "deployment_name = \"<YOUR_CHAT_MODEL_DEPLOYMENT_NAME>\"  # Make sure this is a chat model like 'gpt-4-turbo'\n",
        "\n",
        "# Authenticate using AAD (Azure Active Directory)\n",
        "credential = DefaultAzureCredential()\n",
        "token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
        "\n",
        "# Define the chat input in the appropriate format for chat completion\n",
        "input_data = {\n",
        "    \"messages\": [\n",
        "        {\"role\": \"user\", \"content\": \"What is samtools.\"}\n",
        "    ],\n",
        "    \"max_tokens\": 4096,\n",
        "    \"temperature\": 0.1\n",
        "    }\n",
        "\n",
        "# Set up the headers with AAD token\n",
        "headers = {\n",
        "    \"Authorization\": f\"Bearer {token}\",\n",
        "    \"Content-Type\": \"application/json\"\n",
        "}\n",
        "\n",
        "# Send a request to the chat completion API\n",
        "response = requests.post(\n",
        "    url=f\"https://olmelnic-test.openai.azure.com/openai/deployments/gpt-4o-2024-05-13/chat/completions?api-version=2023-03-15-preview\",\n",
        "    headers=headers,\n",
        "    json=input_data\n",
        ")\n",
        "\n",
        "# Print the response\n",
        "if response.status_code == 200:\n",
        "    result = response.json()\n",
        "    print(result[\"choices\"][0][\"message\"][\"content\"])\n",
        "else:\n",
        "    print(f\"Failed with status code: {response.status_code}, {response.text}\")\n"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Failed with status code: 429, {\"error\":{\"code\":\"429\",\"message\": \"Rate limit is exceeded. Try again in 8 seconds.\"}}\n"
        }
      ],
      "execution_count": 20,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729688546339
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluate model "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Evaulation real data"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path for saving the real request file\n",
        "test_src_dir = \"./gpt4-inference-test\"\n",
        "os.makedirs(test_src_dir, exist_ok=True)\n",
        "real_data_path = os.path.join(test_src_dir, \"test_data.json\")\n",
        "\n",
        "# Save the real request data to a JSON file\n",
        "with open(real_data_path, \"w\") as f:\n",
        "    json.dump(test_data, f, indent=4)"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729619689407
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define paths and parameters\n",
        "timeout = 1000\n",
        "test_src_dir = \"./gpt4-inference-test\"\n",
        "response_src_dir = \"./gpt4-inference-responses\"\n",
        "batch_size = 1\n",
        "\n",
        "# Ensure directories exist\n",
        "os.makedirs(test_src_dir, exist_ok=True)\n",
        "os.makedirs(response_src_dir, exist_ok=True)\n",
        "\n",
        "def batch_data(data, batch_size):\n",
        "    \"\"\"Split data into batches of given size.\"\"\"\n",
        "    return [data[i:i + batch_size] for i in range(0, len(data), batch_size)]\n",
        "\n",
        "\n",
        "# Create batches\n",
        "batches = batch_data(test_data[\"input_data\"], batch_size)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729619691210
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def allowSelfSignedHttps(allowed):\n",
        "    # bypass the server certificate verification on client side\n",
        "    if allowed and not os.environ.get('PYTHONHTTPSVERIFY', '') and getattr(ssl, '_create_unverified_context', None):\n",
        "        ssl._create_default_https_context = ssl._create_unverified_context\n",
        "\n",
        "allowSelfSignedHttps(True) # this line is needed if you use self-signed certificate in your scoring service.\n"
      ],
      "outputs": [],
      "execution_count": 21,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729688551506
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import urllib.request\n",
        "import time\n",
        "from azure.identity import DefaultAzureCredential\n",
        "\n",
        "def submit_tasks(query, id, max_retries=5):\n",
        "    data = {\n",
        "    \"messages\": query,\n",
        "    \"max_tokens\": 4096,\n",
        "    \"temperature\": 0.1\n",
        "    }\n",
        "\n",
        "\n",
        "    batch_file_path = os.path.join(test_src_dir, f\"batch_{id}.json\")\n",
        "    # Save the batch request data to a JSON file\n",
        "    with open(batch_file_path, \"w\") as f:\n",
        "        json.dump(data, f, indent=4)\n",
        "\n",
        "    body = str.encode(json.dumps(data))\n",
        "    #print(f\"Batch request data saved to: {batch_file_path}\")\n",
        "\n",
        "    url = 'https://olmelnic-test.openai.azure.com/openai/deployments/gpt-4o-2024-05-13/chat/completions?api-version=2023-06-01-preview'\n",
        "    \n",
        "    # Authenticate using AAD (Azure Active Directory)\n",
        "    credential = DefaultAzureCredential()\n",
        "    token = credential.get_token(\"https://cognitiveservices.azure.com/.default\").token\n",
        "\n",
        "    # Set up the headers with AAD token\n",
        "    headers = {\n",
        "        \"Authorization\": f\"Bearer {token}\",\n",
        "        \"Content-Type\": \"application/json\"\n",
        "    }\n",
        "\n",
        "    retries = 0\n",
        "    while retries < max_retries:\n",
        "        response = requests.post(\n",
        "            url=f\"https://olmelnic-test.openai.azure.com/openai/deployments/gpt-4o-2024-05-13/chat/completions?api-version=2023-03-15-preview\",\n",
        "            headers=headers,\n",
        "            json=data\n",
        "        )\n",
        "        # Check if the response is rate limited (status code 429)\n",
        "        if response.status_code == 429:\n",
        "            retry_after = int(response.headers.get(\"Retry-After\", 32))  # Default to 32 seconds if not provided\n",
        "            print(f\"Rate limit exceeded. Retrying in {retry_after} seconds...\")\n",
        "            time.sleep(retry_after)\n",
        "            retries += 1\n",
        "        else:\n",
        "            # If successful or another error occurred, break out of the retry loop\n",
        "            break\n",
        "\n",
        "    # Check if the request was successful after retrying\n",
        "    if response.status_code == 200:\n",
        "        result = response.json()\n",
        "        output = result[\"choices\"][0][\"message\"][\"content\"]\n",
        "    else:\n",
        "        print(f\"Request failed after {retries} retries. Status code: {response.status_code}, Response: {response.text}\")\n",
        " \n",
        "    response_file_path = os.path.join(response_src_dir, f\"response_{id}.json\")\n",
        "    \n",
        "    # Save both input and response data\n",
        "    with open(response_file_path, \"w\") as f:\n",
        "        json.dump({\n",
        "            \"input_data\": query,  # Use query here\n",
        "            \"response_data\": output  # Ensure result is JSON serializable\n",
        "        }, f, indent=4)\n",
        "    \n",
        "    print(f\"Result for batch {id} saved to: {response_file_path}\")\n",
        "\n"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729688565897
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## loop through batches\n",
        "for i, batch in enumerate(batches):\n",
        "    submit_tasks(batch,i)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Batch request data saved to: ./gpt4-inference-test/batch_0.json\nResult for batch 0 saved to: ./gpt4-inference-responses/response_0.json\nBatch request data saved to: ./gpt4-inference-test/batch_1.json\nResult for batch 1 saved to: ./gpt4-inference-responses/response_1.json\nBatch request data saved to: ./gpt4-inference-test/batch_2.json\nRate limit exceeded. Retrying in 47 seconds...\nResult for batch 2 saved to: ./gpt4-inference-responses/response_2.json\nBatch request data saved to: ./gpt4-inference-test/batch_3.json\nRate limit exceeded. Retrying in 1 seconds...\nResult for batch 3 saved to: ./gpt4-inference-responses/response_3.json\nBatch request data saved to: ./gpt4-inference-test/batch_4.json\nRate limit exceeded. Retrying in 45 seconds...\nResult for batch 4 saved to: ./gpt4-inference-responses/response_4.json\nBatch request data saved to: ./gpt4-inference-test/batch_5.json\nRate limit exceeded. Retrying in 2 seconds...\nResult for batch 5 saved to: ./gpt4-inference-responses/response_5.json\nBatch request data saved to: ./gpt4-inference-test/batch_6.json\nRate limit exceeded. Retrying in 47 seconds...\nResult for batch 6 saved to: ./gpt4-inference-responses/response_6.json\nBatch request data saved to: ./gpt4-inference-test/batch_7.json\nResult for batch 7 saved to: ./gpt4-inference-responses/response_7.json\nBatch request data saved to: ./gpt4-inference-test/batch_8.json\nRate limit exceeded. Retrying in 43 seconds...\nResult for batch 8 saved to: ./gpt4-inference-responses/response_8.json\nBatch request data saved to: ./gpt4-inference-test/batch_9.json\nRate limit exceeded. Retrying in 2 seconds...\nResult for batch 9 saved to: ./gpt4-inference-responses/response_9.json\nBatch request data saved to: ./gpt4-inference-test/batch_10.json\nRate limit exceeded. Retrying in 45 seconds...\nResult for batch 10 saved to: ./gpt4-inference-responses/response_10.json\nBatch request data saved to: ./gpt4-inference-test/batch_11.json\nResult for batch 11 saved to: ./gpt4-inference-responses/response_11.json\nBatch request data saved to: ./gpt4-inference-test/batch_12.json\nRate limit exceeded. Retrying in 38 seconds...\nResult for batch 12 saved to: ./gpt4-inference-responses/response_12.json\nBatch request data saved to: ./gpt4-inference-test/batch_13.json\nRate limit exceeded. Retrying in 9 seconds...\nResult for batch 13 saved to: ./gpt4-inference-responses/response_13.json\nBatch request data saved to: ./gpt4-inference-test/batch_14.json\nRate limit exceeded. Retrying in 44 seconds...\nResult for batch 14 saved to: ./gpt4-inference-responses/response_14.json\nBatch request data saved to: ./gpt4-inference-test/batch_15.json\nResult for batch 15 saved to: ./gpt4-inference-responses/response_15.json\nBatch request data saved to: ./gpt4-inference-test/batch_16.json\nRate limit exceeded. Retrying in 39 seconds...\nResult for batch 16 saved to: ./gpt4-inference-responses/response_16.json\nBatch request data saved to: ./gpt4-inference-test/batch_17.json\nRate limit exceeded. Retrying in 9 seconds...\nResult for batch 17 saved to: ./gpt4-inference-responses/response_17.json\nBatch request data saved to: ./gpt4-inference-test/batch_18.json\nRate limit exceeded. Retrying in 36 seconds...\nResult for batch 18 saved to: ./gpt4-inference-responses/response_18.json\nBatch request data saved to: ./gpt4-inference-test/batch_19.json\nRate limit exceeded. Retrying in 5 seconds...\nResult for batch 19 saved to: ./gpt4-inference-responses/response_19.json\nBatch request data saved to: ./gpt4-inference-test/batch_20.json\nRate limit exceeded. Retrying in 40 seconds...\nResult for batch 20 saved to: ./gpt4-inference-responses/response_20.json\nBatch request data saved to: ./gpt4-inference-test/batch_21.json\nRate limit exceeded. Retrying in 6 seconds...\nResult for batch 21 saved to: ./gpt4-inference-responses/response_21.json\nBatch request data saved to: ./gpt4-inference-test/batch_22.json\nRate limit exceeded. Retrying in 41 seconds...\nResult for batch 22 saved to: ./gpt4-inference-responses/response_22.json\nBatch request data saved to: ./gpt4-inference-test/batch_23.json\nRate limit exceeded. Retrying in 5 seconds...\nResult for batch 23 saved to: ./gpt4-inference-responses/response_23.json\nBatch request data saved to: ./gpt4-inference-test/batch_24.json\nRate limit exceeded. Retrying in 37 seconds...\nResult for batch 24 saved to: ./gpt4-inference-responses/response_24.json\nBatch request data saved to: ./gpt4-inference-test/batch_25.json\nRate limit exceeded. Retrying in 4 seconds...\nResult for batch 25 saved to: ./gpt4-inference-responses/response_25.json\nBatch request data saved to: ./gpt4-inference-test/batch_26.json\nRate limit exceeded. Retrying in 40 seconds...\nResult for batch 26 saved to: ./gpt4-inference-responses/response_26.json\nBatch request data saved to: ./gpt4-inference-test/batch_27.json\nRate limit exceeded. Retrying in 11 seconds...\nResult for batch 27 saved to: ./gpt4-inference-responses/response_27.json\nBatch request data saved to: ./gpt4-inference-test/batch_28.json\nRate limit exceeded. Retrying in 41 seconds...\nResult for batch 28 saved to: ./gpt4-inference-responses/response_28.json\nBatch request data saved to: ./gpt4-inference-test/batch_29.json\nRate limit exceeded. Retrying in 3 seconds...\nResult for batch 29 saved to: ./gpt4-inference-responses/response_29.json\nBatch request data saved to: ./gpt4-inference-test/batch_30.json\nRate limit exceeded. Retrying in 39 seconds...\nResult for batch 30 saved to: ./gpt4-inference-responses/response_30.json\nBatch request data saved to: ./gpt4-inference-test/batch_31.json\nRate limit exceeded. Retrying in 7 seconds...\nResult for batch 31 saved to: ./gpt4-inference-responses/response_31.json\nBatch request data saved to: ./gpt4-inference-test/batch_32.json\nRate limit exceeded. Retrying in 42 seconds...\nResult for batch 32 saved to: ./gpt4-inference-responses/response_32.json\nBatch request data saved to: ./gpt4-inference-test/batch_33.json\nRate limit exceeded. Retrying in 3 seconds...\nResult for batch 33 saved to: ./gpt4-inference-responses/response_33.json\nBatch request data saved to: ./gpt4-inference-test/batch_34.json\nRate limit exceeded. Retrying in 41 seconds...\nResult for batch 34 saved to: ./gpt4-inference-responses/response_34.json\nBatch request data saved to: ./gpt4-inference-test/batch_35.json\nRate limit exceeded. Retrying in 4 seconds...\nResult for batch 35 saved to: ./gpt4-inference-responses/response_35.json\nBatch request data saved to: ./gpt4-inference-test/batch_36.json\nRate limit exceeded. Retrying in 39 seconds...\nResult for batch 36 saved to: ./gpt4-inference-responses/response_36.json\nBatch request data saved to: ./gpt4-inference-test/batch_37.json\nResult for batch 37 saved to: ./gpt4-inference-responses/response_37.json\nBatch request data saved to: ./gpt4-inference-test/batch_38.json\nRate limit exceeded. Retrying in 14 seconds...\nResult for batch 38 saved to: ./gpt4-inference-responses/response_38.json\nBatch request data saved to: ./gpt4-inference-test/batch_39.json\nRate limit exceeded. Retrying in 12 seconds...\nResult for batch 39 saved to: ./gpt4-inference-responses/response_39.json\nBatch request data saved to: ./gpt4-inference-test/batch_40.json\nRate limit exceeded. Retrying in 27 seconds...\nResult for batch 40 saved to: ./gpt4-inference-responses/response_40.json\nBatch request data saved to: ./gpt4-inference-test/batch_41.json\nRate limit exceeded. Retrying in 13 seconds...\nResult for batch 41 saved to: ./gpt4-inference-responses/response_41.json\nBatch request data saved to: ./gpt4-inference-test/batch_42.json\nRate limit exceeded. Retrying in 32 seconds...\nResult for batch 42 saved to: ./gpt4-inference-responses/response_42.json\nBatch request data saved to: ./gpt4-inference-test/batch_43.json\nRate limit exceeded. Retrying in 14 seconds...\nResult for batch 43 saved to: ./gpt4-inference-responses/response_43.json\nBatch request data saved to: ./gpt4-inference-test/batch_44.json\nRate limit exceeded. Retrying in 36 seconds...\nResult for batch 44 saved to: ./gpt4-inference-responses/response_44.json\nBatch request data saved to: ./gpt4-inference-test/batch_45.json\nRate limit exceeded. Retrying in 9 seconds...\nResult for batch 45 saved to: ./gpt4-inference-responses/response_45.json\nBatch request data saved to: ./gpt4-inference-test/batch_46.json\nRate limit exceeded. Retrying in 35 seconds...\nResult for batch 46 saved to: ./gpt4-inference-responses/response_46.json\nBatch request data saved to: ./gpt4-inference-test/batch_47.json\nRate limit exceeded. Retrying in 13 seconds...\nResult for batch 47 saved to: ./gpt4-inference-responses/response_47.json\nBatch request data saved to: ./gpt4-inference-test/batch_48.json\nRate limit exceeded. Retrying in 32 seconds...\nResult for batch 48 saved to: ./gpt4-inference-responses/response_48.json\nBatch request data saved to: ./gpt4-inference-test/batch_49.json\nRate limit exceeded. Retrying in 14 seconds...\nResult for batch 49 saved to: ./gpt4-inference-responses/response_49.json\nBatch request data saved to: ./gpt4-inference-test/batch_50.json\nRate limit exceeded. Retrying in 36 seconds...\nResult for batch 50 saved to: ./gpt4-inference-responses/response_50.json\nBatch request data saved to: ./gpt4-inference-test/batch_51.json\nRate limit exceeded. Retrying in 10 seconds...\nResult for batch 51 saved to: ./gpt4-inference-responses/response_51.json\nBatch request data saved to: ./gpt4-inference-test/batch_52.json\nRate limit exceeded. Retrying in 34 seconds...\nResult for batch 52 saved to: ./gpt4-inference-responses/response_52.json\nBatch request data saved to: ./gpt4-inference-test/batch_53.json\nRate limit exceeded. Retrying in 12 seconds...\nResult for batch 53 saved to: ./gpt4-inference-responses/response_53.json\nBatch request data saved to: ./gpt4-inference-test/batch_54.json\nRate limit exceeded. Retrying in 34 seconds...\nResult for batch 54 saved to: ./gpt4-inference-responses/response_54.json\nBatch request data saved to: ./gpt4-inference-test/batch_55.json\nRate limit exceeded. Retrying in 13 seconds...\nResult for batch 55 saved to: ./gpt4-inference-responses/response_55.json\nBatch request data saved to: ./gpt4-inference-test/batch_56.json\nRate limit exceeded. Retrying in 34 seconds...\nResult for batch 56 saved to: ./gpt4-inference-responses/response_56.json\nBatch request data saved to: ./gpt4-inference-test/batch_57.json\nRate limit exceeded. Retrying in 11 seconds...\nResult for batch 57 saved to: ./gpt4-inference-responses/response_57.json\nBatch request data saved to: ./gpt4-inference-test/batch_58.json\nRate limit exceeded. Retrying in 38 seconds...\nResult for batch 58 saved to: ./gpt4-inference-responses/response_58.json\nBatch request data saved to: ./gpt4-inference-test/batch_59.json\nRate limit exceeded. Retrying in 12 seconds...\nResult for batch 59 saved to: ./gpt4-inference-responses/response_59.json\nBatch request data saved to: ./gpt4-inference-test/batch_60.json\nRate limit exceeded. Retrying in 35 seconds...\nResult for batch 60 saved to: ./gpt4-inference-responses/response_60.json\nBatch request data saved to: ./gpt4-inference-test/batch_61.json\nRate limit exceeded. Retrying in 14 seconds...\nResult for batch 61 saved to: ./gpt4-inference-responses/response_61.json\nBatch request data saved to: ./gpt4-inference-test/batch_62.json\nRate limit exceeded. Retrying in 35 seconds...\nResult for batch 62 saved to: ./gpt4-inference-responses/response_62.json\nBatch request data saved to: ./gpt4-inference-test/batch_63.json\nRate limit exceeded. Retrying in 10 seconds...\nResult for batch 63 saved to: ./gpt4-inference-responses/response_63.json\nBatch request data saved to: ./gpt4-inference-test/batch_64.json\nRate limit exceeded. Retrying in 34 seconds...\nResult for batch 64 saved to: ./gpt4-inference-responses/response_64.json\nBatch request data saved to: ./gpt4-inference-test/batch_65.json\nRate limit exceeded. Retrying in 13 seconds...\nResult for batch 65 saved to: ./gpt4-inference-responses/response_65.json\nBatch request data saved to: ./gpt4-inference-test/batch_66.json\nRate limit exceeded. Retrying in 33 seconds...\nResult for batch 66 saved to: ./gpt4-inference-responses/response_66.json\nBatch request data saved to: ./gpt4-inference-test/batch_67.json\nRate limit exceeded. Retrying in 13 seconds...\nResult for batch 67 saved to: ./gpt4-inference-responses/response_67.json\nBatch request data saved to: ./gpt4-inference-test/batch_68.json\nRate limit exceeded. Retrying in 33 seconds...\nResult for batch 68 saved to: ./gpt4-inference-responses/response_68.json\nBatch request data saved to: ./gpt4-inference-test/batch_69.json\nRate limit exceeded. Retrying in 13 seconds...\nResult for batch 69 saved to: ./gpt4-inference-responses/response_69.json\nBatch request data saved to: ./gpt4-inference-test/batch_70.json\nRate limit exceeded. Retrying in 31 seconds...\nResult for batch 70 saved to: ./gpt4-inference-responses/response_70.json\n"
        }
      ],
      "execution_count": 14,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729618301799
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "file_pattern = \"./gpt4-inference-responses/*.json\"\n",
        "\n",
        "questions = []\n",
        "answers = []\n",
        "\n",
        "for file_name in glob.glob(file_pattern): #glob lists all the text files in the current working dir.\n",
        "    with open(file_name, 'r') as file:\n",
        "        data = json.load(file)\n",
        "    \n",
        "\n",
        "    if \"input_data\" in data: #retrieves list of dictionaries\n",
        "        for item in data[\"input_data\"]: #iterates over each dictionary in the list\n",
        "            if \"content\" in item:\n",
        "                questions.append(item[\"content\"])\n",
        "\n",
        "    if \"response_data\" in data:\n",
        "        response = data[\"response_data\"]\n",
        "        answers.append(response)\n",
        "\n",
        "final_df = pd.DataFrame({\"Question\": questions, \"Answer\": answers})\n",
        "\n",
        "# Save DataFrame to a CSV file (optional)\n",
        "final_df.to_csv('combined_questions_answers.csv', index=False)\n",
        "\n",
        "final_df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "                                            Question  \\\n0  Hi Everyone. I was trying to add help section ...   \n1  I am currently using BWA-MEM to map metagenomi...   \n2  Hi all:I recently got quite confused with two ...   \n3  Hello, I am pretty new to bioinformatics and t...   \n4  Samtools can be used to select reads above cer...   \n\n                                              Answer  \n0  The issue you're encountering is due to the pl...  \n1  Yes, you can filter out the unmapped reads to ...  \n2  Hi Vanilla,\\n\\nYour confusion is understandabl...  \n3  Welcome to the world of bioinformatics! It sou...  \n4  To select reads with a mapping quality below a...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Question</th>\n      <th>Answer</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Hi Everyone. I was trying to add help section ...</td>\n      <td>The issue you're encountering is due to the pl...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>I am currently using BWA-MEM to map metagenomi...</td>\n      <td>Yes, you can filter out the unmapped reads to ...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Hi all:I recently got quite confused with two ...</td>\n      <td>Hi Vanilla,\\n\\nYour confusion is understandabl...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Hello, I am pretty new to bioinformatics and t...</td>\n      <td>Welcome to the world of bioinformatics! It sou...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Samtools can be used to select reads above cer...</td>\n      <td>To select reads with a mapping quality below a...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729619703168
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ROUGE"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "########note: rouge captures (number of n-grams in prediction summary (fine-tuned phi-3 model) that match the reference summary (ground-truth)) / number of n-grams in reference summary"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = final_df[\"Answer\"].to_list()\n",
        "references = df[\"answer_content\"].to_list()\n",
        "\n",
        "rouge = evaluate.load('rouge') #https://huggingface.co/spaces/evaluate-metric/rouge\n",
        "\n",
        "\n",
        "results = rouge.compute(predictions=predictions,\n",
        "                         references=references,\n",
        "                        use_aggregator=True)\n",
        "\n",
        "print(\"ROUGE-1:\", round(results[\"rouge1\"], 3))\n",
        "print(\"ROUGE-2:\", round(results[\"rouge2\"], 3))\n",
        "print(\"ROUGE-L:\", round(results[\"rougeL\"], 3))\n",
        "print(\"ROUGE-Lsum:\", round(results[\"rougeLsum\"], 3))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ROUGE-1: 0.122\nROUGE-2: 0.014\nROUGE-L: 0.072\nROUGE-Lsum: 0.091\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729619709807
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Loop 100 times for generating confidence intervals"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "rouge_df = pd.DataFrame(columns=[\"ROUGE-1\", \"ROUGE-2\", \"ROUGE-L\", \"ROUGE-Lsum\"])\n",
        "\n",
        "for j in range(100):\n",
        "\n",
        "    print(f\"Iteration {j}\")\n",
        "\n",
        "    ## loop through batches\n",
        "    for i, batch in enumerate(batches):\n",
        "        submit_tasks(batch,i)\n",
        "\n",
        "    file_pattern = \"./gpt4-inference-responses/*.json\"\n",
        "\n",
        "    questions = []\n",
        "    answers = []\n",
        "\n",
        "    for file_name in glob.glob(file_pattern): #glob lists all the text files in the current working dir.\n",
        "        with open(file_name, 'r') as file:\n",
        "            data = json.load(file)\n",
        "        \n",
        "\n",
        "        if \"input_data\" in data: #retrieves list of dictionaries\n",
        "            for item in data[\"input_data\"]: #iterates over each dictionary in the list\n",
        "                if \"content\" in item:\n",
        "                    questions.append(item[\"content\"])\n",
        "\n",
        "        if \"response_data\" in data:\n",
        "            response = data[\"response_data\"]\n",
        "            answers.append(response)\n",
        "\n",
        "    final_df = pd.DataFrame({\"Question\": questions, \"Answer\": answers})\n",
        "\n",
        "    # Save DataFrame to a CSV file (optional)\n",
        "    final_df.to_csv('combined_questions_answers.csv', index=False)\n",
        "\n",
        "    final_df.head()\n",
        "\n",
        "    predictions = final_df[\"Answer\"].to_list()\n",
        "    references = df[\"answer_content\"].to_list()\n",
        "\n",
        "    rouge = evaluate.load('rouge') #https://huggingface.co/spaces/evaluate-metric/rouge\n",
        "\n",
        "\n",
        "    results = rouge.compute(predictions=predictions,\n",
        "                            references=references,\n",
        "                            use_aggregator=True)\n",
        "    rouge_df.loc[j] = [\n",
        "        round(results[\"rouge1\"], 3),\n",
        "        round(results[\"rouge2\"], 3),\n",
        "        round(results[\"rougeL\"], 3),\n",
        "        round(results[\"rougeLsum\"], 3)\n",
        "    ]"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Iteration 0\nRate limit exceeded. Retrying in 12 seconds...\nResult for batch 0 saved to: ./gpt4-inference-responses/response_0.json\nRate limit exceeded. Retrying in 23 seconds...\nRate limit exceeded. Retrying in 30 seconds...\nRate limit exceeded. Retrying in 30 seconds...\n"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[25], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m## loop through batches\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, batch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batches):\n\u001b[0;32m---> 12\u001b[0m     \u001b[43msubmit_tasks\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m file_pattern \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./gpt4-inference-responses/*.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     16\u001b[0m questions \u001b[38;5;241m=\u001b[39m []\n",
            "Cell \u001b[0;32mIn[23], line 46\u001b[0m, in \u001b[0;36msubmit_tasks\u001b[0;34m(query, id, max_retries)\u001b[0m\n\u001b[1;32m     44\u001b[0m     retry_after \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(response\u001b[38;5;241m.\u001b[39mheaders\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRetry-After\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m32\u001b[39m))  \u001b[38;5;66;03m# Default to 32 seconds if not provided\u001b[39;00m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRate limit exceeded. Retrying in \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mretry_after\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m seconds...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 46\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretry_after\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m     retries \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;66;03m# If successful or another error occurred, break out of the retry loop\u001b[39;00m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "execution_count": 25,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1729785546427
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Confidence Interval\n",
        "def confidence_interval(data, confidence=0.95):\n",
        "    mean = np.mean(data)\n",
        "    sem = stats.sem(data)  # Standard Error of Mean\n",
        "    interval = sem * stats.t.ppf((1 + confidence) / 2., len(data) - 1)\n",
        "    return mean, interval"
      ],
      "outputs": [],
      "execution_count": 32,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730210351680
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_df.to_csv('./rouge_results.csv', index=False)\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_df = pd.read_csv('./rouge_results.csv')"
      ],
      "outputs": [],
      "execution_count": 28,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730210285790
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import pandas as pd\n",
        "\n",
        "rouge1_mean, rouge1_ci = confidence_interval(rouge_df['ROUGE-1'])\n",
        "rouge2_mean, rouge2_ci = confidence_interval(rouge_df['ROUGE-2'])\n",
        "rougeL_mean, rougeL_ci = confidence_interval(rouge_df['ROUGE-L'])\n",
        "rougeLsum_mean, rougeLsum_ci = confidence_interval(rouge_df['ROUGE-Lsum'])\n",
        "\n",
        "print(f\"ROUGE-1: {rouge1_mean:.3f} ± {rouge1_ci:.3f}\")\n",
        "print(f\"ROUGE-2: {rouge2_mean:.3f} ± {rouge2_ci:.3f}\")\n",
        "print(f\"ROUGE-L: {rougeL_mean:.3f} ± {rougeL_ci:.3f}\")\n",
        "print(f\"ROUGE-Lsum: {rougeLsum_mean:.3f} ± {rougeLsum_ci:.3f}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "ROUGE-1: 0.122 ± 0.000\nROUGE-2: 0.014 ± 0.000\nROUGE-L: 0.070 ± 0.000\nROUGE-Lsum: 0.089 ± 0.000\n"
        }
      ],
      "execution_count": 35,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1730210401683
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python310-sdkv2",
      "language": "python",
      "display_name": "Python 3.10 - SDK v2"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.14",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "python310-sdkv2"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}